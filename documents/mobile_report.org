:SETUP:    
#+TITLE: High-Frequency Consumption Panel: Preliminary Report
#+AUTHOR: Reajul Chowdhury, Elliott Collins, Ethan Ligon, Munshi Sulaiman
#+DATE: 2017-02-09
#+OPTIONS: texht:t toc:nil
#+LATEX_CLASS_OPTIONS: [12pt,article]
#+LATEX_HEADER:       \newcommand{\T}{\top}
#+LATEX_HEADER:       \newcommand{\E}{\ensuremath{\mbox{E}}}
#+LATEX_HEADER:       \newcommand{\R}{\ensuremath{\mathbb{R}}}
#+LATEX_HEADER:       \newcommand{\one}{\ensuremath{\mathbbm{1}}}
#+LATEX_HEADER:       \newcommand{\Eq}[1]{(\ref{eq:#1})}
#+LATEX_HEADER:       \renewcommand{\vec}[1]{\boldsymbol{#1}}
#+LaTeX_HEADER:       \usepackage{biblatex}
#+LaTeX_HEADER:       \bibliography{prospectus}
#+LaTeX_HEADER:       \usepackage[style=authordate]{biblatex}
#+LATEX_HEADER_EXTRA: \usepackage{bbm}
#+LATEX_HEADER_EXTRA: \usepackage{dcolumn}\newcolumntype{d}[1]{D{.}{.}{#1}}
#+LATEX_HEADER_EXTRA: \newtheorem{proposition}{Proposition} \newcommand{\Prop}[1]{Proposition \ref{prop:#1}}
#+LATEX_HEADER_EXTRA: \newtheorem{theorem}{Theorem} \newcommand{\Thm}[1]{Theorem \ref{thm:#1}}
#+LATEX_HEADER_EXTRA: \newtheorem{remark}{Remark} \newcommand{\Rem}[1]{Remark \ref{rem:#1}}
#+LATEX_HEADER_EXTRA: \newtheorem{condition}{Condition} \newcommand{\Cond}[1]{Condition \ref{cond:#1}}
#+LATEX_HEADER_EXTRA: \newtheorem{lemma}{Lemma} \newcommand{\Lem}[1]{Lemma \ref{prop:#1}}
#+LATEX_HEADER_EXTRA: \newcommand{\Fig}[1]{Figure \ref{fig:#1}} \newcommand{\Tab}[1]{Table \ref{tab:#1}}
:END:      

#+begin_abstract

#+end_abstract
\newpage

* Introduction
 

* The Data

** Empirical Strategy 

We estimate a single model using interactions between time effects and group
assignment, as well as baseline values of the outcome variable where available. 

\begin{equation*}
Y_{it} =\sum_{t=2014}^{2015}\delta_{t}+\beta_{t}^{Cash}I_{t}*Cash_{it}+\beta_{t}^{TUP}I_{t}*TUP_{it}+\gamma Y_{i,2013}+\epsilon_{i}
\end{equation*}

where $\delta_{t}$ are time fixed effects and $I_{t}$ is an indicator if the year
/t/, and $Y_{it}$ is an outcome of interest for household /i/ in year /t/. The
interaction of /Cash/ and /2015/ is the endline treatment effect of the cash
treatment. We take the interactions of TUP assignment with 2014 and 2015 indicators
as the treatment effects at 3-6 and 15-18 months respectively. The analagous
interactions with the Cash group offer a second baseline and a 12-month treatment
effect, respectively. Since those transfers happened after the midline survey, its
interaction with /2014/ acts as a placebo; there is no /ex ante/ reason to expect
that they were different from the rest of the control group at that point.
Given the slight difference in timing, we report a t-test of the hypothesis
\beta_{TUP,t}-\beta_{Cash,2015}=0 for both $t \in {2014,2015}$. Since the difference
in timing is much smaller, we consider \beta_{TUP,2015}-\beta_{Cash,2015}=0 to be the
central hypothesis of interest.
 
* Results
** Sample Size


|-----------+------+------+-----+-------|
| Month     | Cash | Gift | TUP | Total |
|-----------+------+------+-----+-------|
| 1November |   94 |  245 | 188 |   527 |
| 2December |  101 |  297 | 213 |   611 |
| 3January  |   95 |  276 | 206 |   577 |
| 4February |   43 |  105 |  73 |   221 |
|-----------+------+------+-----+-------|

** Consumption

The results for several important consumption measures are presented in Table
\ref{tab:consumption}. Importantly, we do not know about prices for each good in this
time, though we can say that inflation was as high as 100% between 2014 and 2015.
Nonetheless, we take the sum of all consumption and expenditure questions together as
a measure of welfare. [fn:: Details on this issue are discussed further in Beegle
(2012).]

The main result is that TUP participants had higher consumption consumption in 2014,
a few months into the primary monitoring phase after the asset transfers. Similarly,
the Cash group has higher consumption in 2015, measured just over a year after
disbursal. Food transfers had ceased weeks before the 2014 survey was conducted, and
the assets had been transferred 6-8 months prior. The TUP group sees no notable
difference from control in that period. The short-term consumption effects of either
program are economically significant, representing a 23% and 14% increase in average
total consumption for TUP and Cash, respectively.

These results are consistent with a story in which either sort of transfer has a
short-term consumption effect. Importantly, we do not reject the null hypothesis that
the two effects are equal to one another. In either group, the increase in total
consumption appears to be driven mainly by increased food consumption, with smaller
effects on non-food consumption goods and durables. As such, there is no evidence
that the share of food consumed falls, as might be predicted by Engel's law.

\newpage

#+name: consumption_aggregate_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<consumption_analysis>>
return tab
#+end_src

#+name: tab:consumption
#+caption: Average treatment effects by Group-Year, controlling for baseline levels.
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
|                                  | Tot           | logTot        | Food          | FoodShr       | Non-durable   | Durable       |
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
| CTL mean                         | $39.80^{*}$   | $ 3.52^{***}$ | $27.46^{*}$   | $ 0.70^{***}$ | $ 9.73$       | $ 3.07$       |
|                                  | $(22.18)$     | $( 0.61)$     | $(15.54)$     | $( 0.18)$     | $(10.38)$     | $( 5.48)$     |
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
| TUP*2014                         | $ 9.34^{***}$ | $ 0.23^{***}$ | $ 6.12^{***}$ | $-0.01$       | $ 1.94^{*}$   | $ 1.28^{**}$  |
|                                  | $( 2.26)$     | $( 0.06)$     | $( 1.57)$     | $( 0.02)$     | $( 1.02)$     | $( 0.50)$     |
| TUP*2015                         | $ 1.69$       | $ 0.04$       | $ 0.72$       | $-0.01$       | $ 1.13$       | $ 0.09$       |
|                                  | $( 2.15)$     | $( 0.05)$     | $( 1.50)$     | $( 0.01)$     | $( 0.96)$     | $( 0.47)$     |
| CSH*2014                         | $-1.03$       | $-0.02$       | $-0.97$       | $ 0.01$       | $ 0.96$       | $-0.38$       |
|                                  | $( 2.80)$     | $( 0.07)$     | $( 1.95)$     | $( 0.02)$     | $( 1.28)$     | $( 0.62)$     |
| CSH*2015                         | $ 5.66^{**}$  | $ 0.14^{**}$  | $ 3.50^{*}$   | $-0.01$       | $ 2.17^{*}$   | $ 0.06$       |
|                                  | $( 2.75)$     | $( 0.07)$     | $( 1.91)$     | $( 0.02)$     | $( 1.24)$     | $( 0.61)$     |
| Bsln_NAN                         | $ 6.83^{***}$ | $ 0.31^{***}$ | $ 6.13^{***}$ | $ 0.09^{***}$ | $-0.74$       | $ 0.80^{*}$   |
|                                  | $( 2.47)$     | $( 0.09)$     | $( 1.68)$     | $( 0.03)$     | $( 0.87)$     | $( 0.43)$     |
| 2014                             | $35.09^{***}$ | $ 3.25^{***}$ | $26.03^{***}$ | $ 0.69^{***}$ | $ 8.32^{***}$ | $ 2.12^{***}$ |
|                                  | $( 1.89)$     | $( 0.08)$     | $( 1.30)$     | $( 0.03)$     | $( 0.80)$     | $( 0.36)$     |
| 2015                             | $35.93^{***}$ | $ 3.29^{***}$ | $24.62^{***}$ | $ 0.64^{***}$ | $10.14^{***}$ | $ 2.74^{***}$ |
|                                  | $( 1.77)$     | $( 0.08)$     | $( 1.22)$     | $( 0.03)$     | $( 0.74)$     | $( 0.33)$     |
| Bsln2013                         | $ 0.10^{***}$ | $ 0.06^{***}$ | $ 0.07^{**}$  | $ 0.07^{**}$  | $-0.11$       | $ 0.05$       |
|                                  | $( 0.03)$     | $( 0.02)$     | $( 0.03)$     | $( 0.03)$     | $( 0.15)$     | $( 0.03)$     |
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
| F-stat                           | $ 4.83$       | $ 4.77$       | $ 5.79$       | $ 6.30$       | $ 2.23$       | $ 2.12$       |
| N                                | $1305.00$     | $1305.00$     | $1295.00$     | $1295.00$     | $1296.00$     | $1260.00$     |
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $ 3.68$       | $ 0.09$       | $ 2.61$       | $-0.01$       | $-0.23$       | $ 1.22$       |
|                                  | $( 3.51)$     | $( 0.09)$     | $( 2.44)$     | $( 0.02)$     | $( 1.59)$     | $( 0.78)$     |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $-3.97$       | $-0.10$       | $-2.78$       | $-0.00$       | $-1.04$       | $ 0.03$       |
|                                  | $( 2.85)$     | $( 0.07)$     | $( 1.98)$     | $( 0.02)$     | $( 1.30)$     | $( 0.64)$     |
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|

** Assets
   
We turn now to asset holdings for the households. Controlling for baseline asset holdings
where possible, we estimate treatment effects for total value of assets owned, total
value of potentially "productive" assets, as well as land and financial assets. 

*** Total Asset Holdings

Perhaps interestingly, the cash group does not appear to have seen an increase in the
value of assets measured, with negative and imprecise point estimates. The most
important result is that the TUP group has significantly more asset wealth than the
cash or control groups in the short term and two years after receipt of transfers.
The TUP group has a change of 536 SSP on average (43% increase over controls, p<.01).
So-called "Productive" assets include anything that could plausibly be used in
productive activity. [fn:: For now, we include in this list: small and large
livestock, farm equipment, mobiles, carts, sewing equipment, sheds, and shop
premises.] Here we see the TUP group has 320 SSP (95%) more in this area over the
control group, with a similar magnitude at midline.

Importantly, this is not due to a preciptous increase in assets reported over this
time. Note also that the effect on total assets is higher in absolute value than the
effect on productive asset value, suggesting that the increased wealth cannot be
explained purely by households holding onto asset transfers for the length of the
program's monitoring phase. Instead, the TUP group is the only one for whom total
measured asset holdings did not fall on average over these two years, which saw
hyperinflation and a significant aggregate economic downturn.

#+CAPTION: Measured asset wealth by group-year
#+NAME: fig:AssetTotal
[[../figures/AssetTotal_groupyear.png]] 

#+name: asset_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<asset_analysis>>
return tab
#+end_src

#+name: tab:assets
#+caption: Average treatment effects by group-year on total value (in SSP) of all assets measured and of productive assets measured
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+-----------------+----------------|
|                                  | Total           | Productive     |
|----------------------------------+-----------------+----------------|
| CTL mean                         | $1225.61$       | $337.60$       |
|                                  | $(1502.46)$     | $(605.57)$     |
|----------------------------------+-----------------+----------------|
| TUP*2014                         | $535.79^{***}$  | $361.80^{***}$ |
|                                  | $(154.02)$      | $(74.19)$      |
| TUP*2015                         | $624.79^{***}$  | $320.74^{***}$ |
|                                  | $(146.01)$      | $(68.68)$      |
| CSH*2014                         | $-125.86$       | $18.50$        |
|                                  | $(191.31)$      | $(95.80)$      |
| CSH*2015                         | $-49.99$        | $-5.00$        |
|                                  | $(187.32)$      | $(88.40)$      |
| Bsln2013                         | $ 0.08^{***}$   | $ 0.00$        |
|                                  | $( 0.02)$       | $( 0.01)$      |
| 2014                             | $1259.75^{***}$ | $465.53^{***}$ |
|                                  | $(112.68)$      | $(55.96)$      |
| 2015                             | $1124.61^{***}$ | $392.97^{***}$ |
|                                  | $(103.46)$      | $(50.21)$      |
| Bsln_NAN                         | $21.30$         | $-131.14^{**}$ |
|                                  | $(146.51)$      | $(51.35)$      |
|----------------------------------+-----------------+----------------|
| N                                | $1305.00$       | $1247.00$      |
| F-stat                           | $ 8.53$         | $10.19$        |
|----------------------------------+-----------------+----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $585.78^{**}$   | $366.79^{***}$ |
|                                  | $(239.76)$      | $(114.58)$     |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $674.78^{***}$  | $325.74^{***}$ |
|                                  | $(194.72)$      | $(92.26)$      |
|----------------------------------+-----------------+----------------|


*** Savings

Both treatment arms had significant impact on the average level of cash savings
within households. The TUP households are strongly encouraged to pay into a savings
account maintained by BRAC each time they meet. Anecdotally, this has discouraged
some women from attending the meetings, but it results in TUP participants being 44%
(20 pp) more likely to report having any savings at all. It's
worth noting though that since the TUP households also regard their savings behavior
as much more transparent to BRAC (and have received pressure to save from them) than
the other groups, these households may simply be more likely to reveal that they are
saving when asked. Among those who have savings, TUP households report having roughly
43% (81 SSP) more in value.

Cash households appear no more likely than the control households to report having
cash savings (around 45% in each group), but households that report saving report
having 47% (91.4 SSP) more in value. This is significantly less than was given to
these households, but combined with the short-term consumption results, goes some
distance in explaining the lack of effect on physical asset wealth. 

It is common in this community (and most in the region) to store non-perishable food
like maize, cassava, or millet as a form of savings. This would seem particularly
reasonable in a high-inflation context, where the price of grain had doubled in the
previous year. At least as many households report saving in food (53%) as in cash
(46%), with an average market value of 106 SSP. However, we find no evidence that
either treatment group increased food savings. [fn:: Note that food savings was not
measured at baseline, so these controls are omitted.]

Neither do we find evidence that either treatment increased the size or likelihood of
giving or receiving interhousehold transfers, either in cash or in kind. These
results are omitted since only 35 and 60 households reported giving and recieving
transfers respectively, with no difference in group means.

#+name: savings_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<savings_analysis>>
return Table
#+end_src


#+name: tab:Nonzero
#+caption: Average treatment effects by group-year on percentage of households reporting any savings or land access 
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+---------------|---------------+---------------+---------------|
| % > 0                            | Savings       | Food Sav      | LandCult      | LandOwn       |
|----------------------------------+---------------|---------------+---------------+---------------|
| CTL mean                         | $ 0.45$       | $ 0.82$       | $ 0.82$       | $ 0.90$       |
|----------------------------------+---------------|---------------+---------------+---------------|
| CSH*2014                         | $-0.06$       | $ 0.00$       | $-0.04$       | $-0.01$       |
|                                  | $( 0.06)$     | $( 0.04)$     | $( 0.04)$     | $( 0.04)$     |
| CSH*2015                         | $ 0.03$       | $ 0.02$       | $ 0.05$       | $ 0.02$       |
|                                  | $( 0.05)$     | $( 0.04)$     | $( 0.04)$     | $( 0.04)$     |
| TUP*2014                         | $ 0.22^{***}$ | $-0.02$       | $-0.03$       | $-0.00$       |
|                                  | $( 0.04)$     | $( 0.03)$     | $( 0.03)$     | $( 0.03)$     |
| TUP*2015                         | $ 0.21^{***}$ | $-0.03$       | $ 0.01$       | $-0.01$       |
|                                  | $( 0.04)$     | $( 0.03)$     | $( 0.03)$     | $( 0.03)$     |
| 2014                             | $ 0.43^{***}$ | $ 1.00^{***}$ | $ 0.83^{***}$ | $ 0.82^{***}$ |
|                                  | $( 0.04)$     | $( 0.02)$     | $( 0.06)$     | $( 0.05)$     |
| 2015                             | $ 0.39^{***}$ | $ 0.82^{***}$ | $ 0.77^{***}$ | $ 0.84^{***}$ |
|                                  | $( 0.04)$     | $( 0.02)$     | $( 0.05)$     | $( 0.05)$     |
| Bsln2013                         | $ 0.05$       |               | $ 0.05$       | $ 0.07$       |
|                                  | $( 0.04)$     |               | $( 0.05)$     | $( 0.04)$     |
| Bsln_NAN                         | $ 0.08^{*}$   |               | $ 0.05$       | $ 0.05$       |
|                                  | $( 0.04)$     |               | $( 0.06)$     | $( 0.05)$     |
|----------------------------------+---------------|---------------+---------------+---------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $ 0.19$       | $-0.04$       | $-0.07$       | $-0.02$       |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $ 0.18$       | $-0.05$       | $-0.03$       | $-0.03$       |
|----------------------------------+---------------|---------------+---------------+---------------|
| F-stat                           | $ 8.83$       | $15.60$       | $ 0.79$       | $ 0.76$       |
| N                                | $1259.00$     | $870.00$      | $1231.00$     | $1251.00$     |
|----------------------------------+---------------|---------------+---------------+---------------|


#+name: tab:Savings
#+caption: Average treatment effects by group-year on total value (in SSP) of all cash and food savings and area (in fedan) of land being cultiviated by the household (including rented or temporary-use) and owned by the household.
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+----------------|----------------+----------------+----------------|
| Amt.                             | Savings        | Food Sav       | LandCult       | LandOwn        |
|----------------------------------+----------------|----------------+----------------+----------------|
| CTL mean                         | $191.19$       | $114.78$       | $61.88$        | $46.00$        |
|----------------------------------+----------------|----------------+----------------+----------------|
| CSH*2014                         | $28.74$        | $ 0.22$        | $10.18$        | $10.50$        |
|                                  | $(42.93)$      | $(15.38)$      | $(15.07)$      | $(12.57)$      |
| CSH*2015                         | $91.40^{**}$   | $-14.34$       | $-39.18^{***}$ | $-32.37^{***}$ |
|                                  | $(40.89)$      | $(14.98)$      | $(14.90)$      | $(11.95)$      |
| TUP*2014                         | $-27.09$       | $17.16$        | $-4.76$        | $-3.02$        |
|                                  | $(29.76)$      | $(12.33)$      | $(11.94)$      | $(10.04)$      |
| TUP*2015                         | $81.33^{***}$  | $ 1.13$        | $-17.38$       | $-12.56$       |
|                                  | $(29.32)$      | $(12.26)$      | $(11.65)$      | $( 9.41)$      |
| 2014                             | $106.72^{***}$ | $62.03^{***}$  | $11.37$        | $17.31^{**}$   |
|                                  | $(24.85)$      | $( 8.36)$      | $( 9.94)$      | $( 8.56)$      |
| 2015                             | $163.04^{***}$ | $114.78^{***}$ | $61.52^{***}$  | $51.89^{***}$  |
|                                  | $(24.13)$      | $( 7.60)$      | $( 9.54)$      | $( 7.88)$      |
| Bsln2013                         | $ 0.05^{**}$   |                | $ 0.94$        | $-2.43$        |
|                                  | $( 0.02)$      |                | $( 3.07)$      | $( 1.95)$      |
| Bsln_NAN                         | $40.07^{*}$    |                | $-1.60$        | $-6.02$        |
|                                  | $(21.24)$      |                | $( 9.92)$      | $( 8.29)$      |
|----------------------------------+----------------|----------------+----------------+----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $-118.49$      | $31.50$        | $34.42$        | $29.35$        |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $-10.07$       | $15.47$        | $21.79$        | $19.80$        |
|----------------------------------+----------------|----------------+----------------+----------------|
| F-stat                           | $ 7.41$        | $ 7.14$        | $ 4.91$        | $ 3.72$        |
| N                                | $671.00$       | $777.00$       | $1042.00$      | $1114.00$      |
|----------------------------------+----------------|----------------+----------------+----------------|


*** Land Holdings

We also examine land ownership and cultivation in each year. We find no evidence that
either group is more or less likely to report owning or cultivating land, though this
may be in part because land ownership and cultivation is already very common.
However, members of the cash group who are involved in agriculture are found to be
cultivating significantly less land after the fact, which reports cultivating 65%
less and owning 70% less land than the control group. This raises the interesting
question of whether the cash group was likely to switch occupations from farming to
non-farm self-employment.

** COMMENT Other Outcomes

* Concluding Remarks
  
* COMMENT Extra Analysis

* COMMENT Code appendix
  
** Cleanup

#+name: get_mobile_data
#+BEGIN_SRC python :dir ../analysis :noweb no-export :results silent :tangle mobile_analysis.py
<<Imports>>
DataDir = "../../data/"
File = DataDir+"Mobile/remote_survey_Nov2015_April2016.dta"
LocationsFile = DataDir+"csv/Locations.csv"

set_thresholds = False
use_bsln =  "b" #~ "m" "e"
M = stata.read_stata(File)
M["Sell"]  = M.filter(regex="^S6_[abcd]_2").sum(1)
M["Buy"]   = M.filter(regex="^S7S7_[abcd]_2").sum(1)
M["iSell"] = (M["Sell"]>0).apply(int)
M["iBuy"]  = (M["Buy"]>0).apply(int)
VARNAMES = {"Sell":"Sell","Buy":"Buy","iSell":"iSell","iBuy":"iBuy","introDate_Int":"date", "introId_Number":"HH", "introRes_name":"Name", "introEnu_name":"Enumerator",
        "S3S3_a":"hh_size", "S3S3_b":"children", "S3S3_c":"num_meals", "S4S4_a":"vegetables",
        "S4S4_b":"sugar", "S4S4_c":"fish", "S4S4_d":"nuts", "S4S4_e":"beans",
        "S5S5_a":"fuel", "S5S5_b":"medicine", "S5S5_c":"airtime", "S5S5_d":"cosmetics",
        "S5S5_e":"soap", "SubmissionDate_year":"year", "SubmissionDate_month":"month", "SubmissionDate_day":"day",
        "SubmissionDate_hour":"hour", "SubmissionDate_minute":"minute", "SubmissionDate_second":"second", "month":"monthname"}

Datename = {"November 2015":"1November", "December 2015":"2December",
"January 2016":"3January", "February 2016":"4February", "March 2016":"5March", "April 2016":"6April"}

HH_vars = ["hh_size", "children"]
ITEMS = ["vegetables", "sugar", "fish", "nuts", "beans", "fuel", "medicine", "airtime", "cosmetics", "soap"]
Food, Durables = ITEMS[:5], ITEMS[5:]

M = M.rename(columns=VARNAMES)[list(VARNAMES.values())]
M['t'] = M['monthname'].replace(Datename)
M = M.set_index(["t","HH"],drop=False)
#~ Eliminate duplicates
M = M.groupby(level=["t","HH"]).last()

if not set_thresholds: #~ thresholds set manually by looking for outliers graphically.
    thresholds={'soap': 750, 'airtime': 600,
                'fuel': 510, 'fish': 150,
                'nuts': 150, 'medicine': 5000,
                'sugar': 500, 'cosmetics': 1100,
                'beans': 200, 'vegetables': 400,
                'hh_size': 25, 'children': 17}
else: #~ Make graphs and set thresholds manually
    M["Total"] = M[ITEMS].sum(1)
    thresholds={}
    for item in ITEMS:
        find_outliers(item,M,"Total")
        plt.show()
        thresholds[item]=int(Input(item+": "))
    for item in HH_vars:
        find_outliers(item,M)
        plt.show()
        thresholds[item]=int(Input(item+": "))
for item,thresh in Items(thresholds): #~ Topcode
    M.loc[M[item]>thresh,item]=thresh

M[Food] /= 3.
M[Durables] /= 30.

C = pd.read_pickle(DataDir+"Mobile/C.df")
M = M.drop(['t',"HH"],1).reset_index()
M['t']=2016
M = M.set_index(["t","HH"])
C.index.names=['t',"HH"]
Goods = C.filter(regex="^c_").columns.tolist()
C.rename(columns={i:i[2:] for i in Goods}, inplace=True)
C = pd.concat([C,M])

C["Mtot"] = C[ITEMS].sum(1)
C["logMtot"] = C["Mtot"].apply(np.log)
C["Ftot"] = C[Food].sum(1)
C["Dtot"] = C[Durables].sum(1)
C.ix[2016]["Tot"] = C.loc[2016,:]["Mtot"]
C.ix[2016]["Food"] = C.loc[2016,:]["Ftot"]
C.to_pickle(DataDir+"Mobile/C_withMobile.df")

#~ Check sum correlations

fig,ax = plt.subplots(3,2)
for i,var in enumerate([("Tot","Mtot"), ("Food","Ftot"),("Month","Dtot")]):
    C.plot(  kind='scatter',x=var[0],y=var[1],ax=ax[i,0])
    C["lnx"]=C[var[0]].apply(np.log)
    C["lny"]=C[var[1]].apply(np.log)
    C.plot(kind='scatter',x='lnx',y='lny',ax=ax[i,1])
    ax[i,0].set_title("{} vs. {}".format(*var))

cc = C.loc[2013:2015,ITEMS+['Tot','Mtot']].copy()
cc = sm.add_constant(cc)
X  = ["const"]+ITEMS[:]
for item in ITEMS:
    cc[item]=cc[item].fillna(0)
    cc['nan'+item]=(cc[item]<=0).apply(int)
    X.append('nan'+item)

M1 = sm.OLS(cc.loc[:,'Tot'],cc.loc[:,X], missing='drop').fit()
cc['Tot_']=M1.fittedvalues
Vs = ["Tot","Mtot","Tot_"]
print(cc[Vs].corr())
#~fig,ax = plt.subplots(2)
#~for i in range(1,3): cc.plot(kind='scatter', x='Tot',y='r'+str(i),ax = ax[i-1])
#~plt.savefig("../figures/mobile_fitting.png")

C['const'] = 1.
for item in ITEMS:
    C[item]=C[item].fillna(0)
    C['nan'+item]=(C[item]<=0).apply(int)

C["Tot_"] = C[X].dot(M1.params)
C.groupby(level=0)["Tot","Tot_","Mtot"].mean()
C.groupby(level=0)["Tot","Tot_","Mtot"].median()

for item in ITEMS: C.unstack(0)['fish'].fillna(0).plot(kind='scatter',x=2015,y=2016)

if use_bsln:
    D = stata.read_stata(DataDir+"TUP_full.dta")
    D['t'] = "0June2013"
    D = D.rename(columns={"idno":"HH", "child_total_b":"children","hh_size_b":"hh_size"})
    dd = D.filter(regex="^c_.*_{}$".format(use_bsln)).rename(columns=lambda x: x[2:-2])
    #~ D = D.filter(regex="^c_.*_[bme]$").rename(columns=lambda x: x[2:-2])
    D = dd.join(D[['t',"HH","children","hh_size"]]).set_index(['t','HH'])
    ITEMS = [i for i in ITEMS if i in D]
    D=D[ITEMS+["children","hh_size"]]
    M = pd.concat([D,M])
    

#~ Merge Locations & Group
L = pd.read_csv(LocationsFile).rename(columns={"RespID":"HH"}).set_index("HH")
M = M.join(L,how="inner")
M.to_pickle(DataDir+"mobile_full.df")
Ntable = M.groupby(["t","Group"]).count()["hh_size"].unstack(1)
Ntable["Total"] = M.groupby("t").count()["hh_size"]
#~ Graph
Ntable["Cash,Gift,TUP".split(',')].T.plot(kind='bar')
plt.tight_layout()
plt.savefig("../figures/MobileSample.png")
plt.clf()
#~ Org Table
TAB = df_to_orgtbl(Ntable)
#+end_src

|-----------+------+------+-----+-------|
| t         | Cash | Gift | TUP | Total |
|-----------+------+------+-----+-------|
| 1November |   94 |  245 | 188 |   527 |
| 2December |  101 |  297 | 213 |   611 |
| 3January  |   95 |  276 | 206 |   577 |
| 4February |   43 |  105 |  73 |   221 |
|-----------+------+------+-----+-------|

** Consumption
   
|                                         | Food          | FoodShr       | Month         | Tot           | Year          | logTot        |
|-----------------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
| TUP*2016                                | $-1.18^{**}$  | $ 0.00^{***}$ | $-0.00$       | $-1.22^{*}$   | $-0.00$       | $-0.00^{***}$ |
|                                         | $( 0.53)$     | $( 0.00)$     | $( 0.00)$     | $( 0.69)$     | $( 0.00)$     | $( 0.00)$     |
| CSH*2016                                | $ 0.74$       | $ 0.00$       | $ 0.00$       | $ 1.74^{**}$  | $ 0.00$       | $ 0.00$       |
|                                         | $( 0.66)$     | $( 0.00)$     | $( 0.00)$     | $( 0.86)$     | $( 0.00)$     | $( 0.00)$     |
| Bsln2013                                | $-0.04^{***}$ | $ 0.07^{**}$  | $-0.11$       | $-0.01$       | $ 0.05$       | $ 0.06^{***}$ |
|                                         | $( 0.01)$     | $( 0.03)$     | $( 0.15)$     | $( 0.01)$     | $( 0.03)$     | $( 0.02)$     |
| 2014                                    | $29.50^{***}$ | $ 0.69^{***}$ | $ 8.32^{***}$ | $38.86^{***}$ | $ 2.12^{***}$ | $ 3.25^{***}$ |
|                                         | $( 1.69)$     | $( 0.03)$     | $( 0.80)$     | $( 2.19)$     | $( 0.36)$     | $( 0.08)$     |
| 2015                                    | $28.42^{***}$ | $ 0.64^{***}$ | $10.14^{***}$ | $39.83^{***}$ | $ 2.74^{***}$ | $ 3.29^{***}$ |
|                                         | $( 1.49)$     | $( 0.03)$     | $( 0.74)$     | $( 1.92)$     | $( 0.33)$     | $( 0.08)$     |
| 2016                                    | $28.74^{***}$ | $-0.00^{***}$ | $-0.00^{**}$  | $43.76^{***}$ | $-0.00$       | $-0.00^{***}$ |
|                                         | $( 0.49)$     | $( 0.00)$     | $( 0.00)$     | $( 0.65)$     | $( 0.00)$     | $( 0.00)$     |
| TUP*2014                                | $ 5.30^{**}$  | $-0.01$       | $ 1.94^{*}$   | $ 8.79^{***}$ | $ 1.28^{**}$  | $ 0.23^{***}$ |
|                                         | $( 2.44)$     | $( 0.02)$     | $( 1.02)$     | $( 3.15)$     | $( 0.50)$     | $( 0.06)$     |
| TUP*2015                                | $-0.46$       | $-0.01$       | $ 1.13$       | $ 0.99$       | $ 0.09$       | $ 0.04$       |
|                                         | $( 2.29)$     | $( 0.01)$     | $( 0.96)$     | $( 2.94)$     | $( 0.47)$     | $( 0.05)$     |
| CSH*2014                                | $-1.63$       | $ 0.01$       | $ 0.96$       | $-1.42$       | $-0.38$       | $-0.02$       |
|                                         | $( 3.05)$     | $( 0.02)$     | $( 1.28)$     | $( 3.92)$     | $( 0.62)$     | $( 0.07)$     |
| CSH*2015                                | $ 2.40$       | $-0.01$       | $ 2.17^{*}$   | $ 5.00$       | $ 0.06$       | $ 0.14^{**}$  |
|                                         | $( 2.95)$     | $( 0.02)$     | $( 1.24)$     | $( 3.80)$     | $( 0.61)$     | $( 0.07)$     |
| Bsln_NAN                                | $-0.84$       | $ 0.09^{***}$ | $-0.74$       | $ 1.13$       | $ 0.80^{*}$   | $ 0.31^{***}$ |
|                                         | $( 0.79)$     | $( 0.03)$     | $( 0.87)$     | $( 1.04)$     | $( 0.43)$     | $( 0.09)$     |
| F-stat                                  | $ 3.28$       | $ 6.30$       | $ 2.23$       | $ 3.20$       | $ 2.12$       | $ 4.77$       |
|                                         |               |               |               |               |               |               |
| N                                       | $14723.00$    | $1295.00$     | $1296.00$     | $14733.00$    | $1260.00$     | $1305.00$     |
|                                         |               |               |               |               |               |               |
| CTL mean                                | $27.46^{*}$   | $ 0.70^{***}$ | $ 9.73$       | $39.80^{*}$   | $ 3.07$       | $ 3.52^{***}$ |
|                                         | $(15.54)$     | $( 0.18)$     | $(10.38)$     | $(22.18)$     | $( 5.48)$     | $( 0.61)$     |
| $\beta^{TUP}_{2014}-\beta^{CSH}$        | $ 2.89$       | $-0.01$       | $-0.23$       | $ 3.80$       | $ 1.22$       | $ 0.09$       |
|                                         | $( 3.82)$     | $( 0.02)$     | $( 1.59)$     | $( 4.93)$     | $( 0.78)$     | $( 0.09)$     |
| $\beta^{TUP}_{2015}-\beta^{CSH}$        | $-2.86$       | $-0.00$       | $-1.04$       | $-4.01$       | $ 0.03$       | $-0.10$       |
|                                         | $( 3.12)$     | $( 0.02)$     | $( 1.30)$     | $( 4.01)$     | $( 0.64)$     | $( 0.07)$     |
| $\beta^{TUP}_{2016}-\beta^{CSH}_{2016}$ | $-1.92^{***}$ | $ 0.00^{***}$ | $-0.00$       | $-2.96^{***}$ | $-0.00$       | $-0.00^{***}$ |
|                                         | $( 0.68)$     | $( 0.00)$     | $( 0.00)$     | $( 0.88)$     | $( 0.00)$     | $( 0.00)$     |





 |                                         | Food          | FoodShr       | Month         | Tot           | Year          | logTot        |
 |-----------------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
 | Bsln2013                                | $ 0.03$       | $ 0.07^{**}$  | $-0.11$       | $ 0.06^{**}$  | $ 0.05$       | $ 0.06^{***}$ |
 |                                         | $( 0.02)$     | $( 0.03)$     | $( 0.15)$     | $( 0.03)$     | $( 0.03)$     | $( 0.02)$     |
 | 2014                                    | $27.24^{***}$ | $ 0.69^{***}$ | $ 8.32^{***}$ | $36.33^{***}$ | $ 2.12^{***}$ | $ 3.25^{***}$ |
 |                                         | $( 1.16)$     | $( 0.03)$     | $( 0.80)$     | $( 1.65)$     | $( 0.36)$     | $( 0.08)$     |
 | 2015                                    | $25.97^{***}$ | $ 0.64^{***}$ | $10.14^{***}$ | $37.26^{***}$ | $ 2.74^{***}$ | $ 3.29^{***}$ |
 |                                         | $( 1.06)$     | $( 0.03)$     | $( 0.74)$     | $( 1.52)$     | $( 0.33)$     | $( 0.08)$     |
 | 2016                                    | $24.65^{***}$ | $-0.00^{***}$ | $-0.00^{**}$  | $39.15^{***}$ | $-0.00$       | $-0.00^{***}$ |
 |                                         | $( 1.07)$     | $( 0.00)$     | $( 0.00)$     | $( 1.53)$     | $( 0.00)$     | $( 0.00)$     |
 | TUP*2014                                | $ 5.79^{***}$ | $-0.01$       | $ 1.94^{*}$   | $ 9.08^{***}$ | $ 1.28^{**}$  | $ 0.23^{***}$ |
 |                                         | $( 1.48)$     | $( 0.02)$     | $( 1.02)$     | $( 2.10)$     | $( 0.50)$     | $( 0.06)$     |
 | TUP*2015                                | $ 0.23$       | $-0.01$       | $ 1.13$       | $ 1.33$       | $ 0.09$       | $ 0.04$       |
 |                                         | $( 1.40)$     | $( 0.01)$     | $( 0.96)$     | $( 1.98)$     | $( 0.47)$     | $( 0.05)$     |
 | CSH*2014                                | $-1.25$       | $ 0.01$       | $ 0.96$       | $-1.25$       | $-0.38$       | $-0.02$       |
 |                                         | $( 1.85)$     | $( 0.02)$     | $( 1.28)$     | $( 2.61)$     | $( 0.62)$     | $( 0.07)$     |
 | CSH*2015                                | $ 3.04^{*}$   | $-0.01$       | $ 2.17^{*}$   | $ 5.31^{**}$  | $ 0.06$       | $ 0.14^{**}$  |
 |                                         | $( 1.80)$     | $( 0.02)$     | $( 1.24)$     | $( 2.55)$     | $( 0.61)$     | $( 0.07)$     |
 | TUP*2016                                | $ 0.28$       | $ 0.00^{***}$ | $-0.00$       | $ 0.30$       | $-0.00$       | $-0.00^{***}$ |
 |                                         | $( 1.41)$     | $( 0.00)$     | $( 0.00)$     | $( 2.00)$     | $( 0.00)$     | $( 0.00)$     |
 | CSH*2016                                | $ 1.25$       | $ 0.00$       | $ 0.00$       | $ 1.47$       | $ 0.00$       | $ 0.00$       |
 |                                         | $( 1.80)$     | $( 0.00)$     | $( 0.00)$     | $( 2.57)$     | $( 0.00)$     | $( 0.00)$     |
 | Bsln_NAN                                | $ 3.43^{***}$ | $ 0.09^{***}$ | $-0.74$       | $ 4.48^{**}$  | $ 0.80^{*}$   | $ 0.31^{***}$ |
 |                                         | $( 1.27)$     | $( 0.03)$     | $( 0.87)$     | $( 1.83)$     | $( 0.43)$     | $( 0.09)$     |
 | F-stat                                  | $ 5.02$       | $ 6.30$       | $ 2.23$       | $ 3.50$       | $ 2.12$       | $ 4.77$       |
 |                                         |               |               |               |               |               |               |
 | N                                       | $1980.00$     | $1295.00$     | $1296.00$     | $1990.00$     | $1260.00$     | $1305.00$     |
 |                                         |               |               |               |               |               |               |
 | CTL mean                                | $27.46^{*}$   | $ 0.70^{***}$ | $ 9.73$       | $39.80^{*}$   | $ 3.07$       | $ 3.52^{***}$ |
 |                                         | $(15.54)$     | $( 0.18)$     | $(10.38)$     | $(22.18)$     | $( 5.48)$     | $( 0.61)$     |
 | $\beta^{TUP}_{2014}-\beta^{CSH}$        | $ 2.74$       | $-0.01$       | $-0.23$       | $ 3.78$       | $ 1.22$       | $ 0.09$       |
 |                                         | $( 2.31)$     | $( 0.02)$     | $( 1.59)$     | $( 3.27)$     | $( 0.78)$     | $( 0.09)$     |
 | $\beta^{TUP}_{2015}-\beta^{CSH}$        | $-2.81$       | $-0.00$       | $-1.04$       | $-3.98$       | $ 0.03$       | $-0.10$       |
 |                                         | $( 1.88)$     | $( 0.02)$     | $( 1.30)$     | $( 2.66)$     | $( 0.64)$     | $( 0.07)$     |
 | $\beta^{TUP}_{2016}-\beta^{CSH}_{2016}$ | $-0.97$       | $ 0.00^{***}$ | $-0.00$       | $-1.16$       | $-0.00$       | $-0.00^{***}$ |
 |                                         | $( 1.88)$     | $( 0.00)$     | $( 0.00)$     | $( 2.67)$     | $( 0.00)$     | $( 0.00)$     |

  

 #+name: neediness_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
 <<get_mobile_data>>
 <<regression_tools>>

 y,z,market = M[ITEMS], M[HH_vars], M["Location"]
 b,ce,d,se,V = nd.estimate_bdce_with_missing_values(y,z,market=market,return_se=True,return_v=True,time_index=0)

 print("Getting Loglambdas")
 bphi,logL=nd.get_loglambdas(ce,TEST="warn")
 logL /=logL.std()
 M["lambdas"]=logL
 fig, ax = plt.subplots(4, 1)
 for i,mnth in enumerate(Datename.values()):
     M.loc[mnth].groupby("Group")['lambdas'].plot(kind='kde',ax=ax[i])
     ax[i].set_xlim(-3,3)
     if not i: ax[i].legend()
 plt.savefig("loglambdas_by_group.png")
 plt.clf()

 #~ Follow TUP regressions
 M["Total"] = M[ITEMS].sum(1)
 bL = M.loc['0June2013','lambdas']
 bL.name="baseline"
 M = M.join(bL)
 M = M.ix[Datename.values()]
 M = M.join(pd.get_dummies(M["Group"]))
 models = {}
 models["lambdas"] = smf.ols("lambdas ~ -1 + C(t) + Cash + TUP + baseline",M).fit()
 models["Total"] = smf.ols("Total ~ -1 + C(t) + Cash + TUP + baseline",M).fit()
 models["Sell_amt"] = smf.ols("Sell ~ -1 + C(t) + Cash + TUP",M).fit()
 models["Buy_amt"] = smf.ols("Buy  ~ -1 + C(t) + Cash + TUP",M).fit()
 models["Sell_any"] = smf.ols("iSell ~ -1 + C(t) + Cash + TUP",M).fit()
 models["Buy_any"] = smf.ols("iBuy  ~ -1 + C(t) + Cash + TUP",M).fit()

 regTAB = reg_table(models, orgtbl=True)
 return regTAB

 #+END_SRC

 #+RESULTS: neediness_analysis
 #+end_example

 
** Assets
   
#+name: asset_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle Endline_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
from matplotlib import pyplot as plt
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

D = full_data(balance=[])

Outcomes = ["Total", "Productive"]
Aval2013 = asset_vars(D,year=2013)[0][Outcomes]
Aval2014 = asset_vars(D,year=2014)[0][Outcomes]
Aval2015 = asset_vars(D,year=2015)[0][Outcomes]

#$\approx$ Creates Year dummies and baseline values as `var'2013
for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
    Aval["Year"]=Year
    for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
Vals = Vals.join(D[["TUP","CSH"]])
Vals["CTL"] = (Vals["TUP"]+Vals["CSH"] ==0).apply(int)

for group in ("TUP", "CSH"):
    for year in ("2013", "2014", "2015"):
        Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

#~ Make graph of distribution
stringify = lambda var: Vals[var].apply(lambda x: var if x else "")
Vals["Group"] = stringify("TUP")+stringify("CSH")+stringify("CTL")


amean = Vals.groupby(["Year","Group"]).mean()[["Total","Productive"]]
aN = Vals.groupby(["Year","Group"]).count()[["Total","Productive"]]
astd = Vals.groupby(["Year","Group"]).std()[["Total","Productive"]]
ase = astd/np.sqrt(aN)
asset_pctchange = (amean/amean.ix[2013]).unstack("Year") - 1

for var in ("Total","Productive"):
   fig,ax = plt.subplots(1,2)
   for i,yr in enumerate((2014,2015)):
       Vals.ix[yr].dropna(subset=[[var,"TUP","CSH","CTL"]]).groupby("Group")[var].plot(kind="kde",ax=ax[i])
       ax[i].set_title("{} Asset Value in {}".format(var,yr))
       ax[i].legend()
       #~ ax[i].set_aspect(1)
       ax[i].set_xlim(left=0)
   plt.savefig("../figures/Asset{}_kde.png".format(var))
   plt.clf()
   amean.unstack("Group")["Total"].plot(kind="bar",yerr=ase.unstack("Group")["Total"].values)
   plt.tight_layout()
   plt.xticks(rotation=45)
   plt.savefig("../figures/Asset{}_groupyear.png".format(var))
   plt.clf()

Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

#$\approx$ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
Vals=Vals.loc[2014:2015]
regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

CTL = Vals["TUP"]+Vals["CSH"] ==0
CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

for var in Outcomes:
    ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


results = results.append(diff)
SE = SE.append(diff_se)

tab = df_to_orgtbl(results, sedf=SE)
#+end_src

#+RESULTS: asset_analysis
: None

#+name: assets_disaggreate_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
import numpy as np
import pandas as pd
from TUP import full_data, regressions, asset_vars, reg_table, df_to_orgtbl

D = full_data(balance=[])
D["cons"] = 1.
Count = D.filter(regex="^asset_n_").rename(columns=lambda col: col[8:])
Vals = D.filter(regex="^asset_val_").rename(columns=lambda col: col[10:])
#~ 1 if coded as >0, 0 if zero, keep NaN's missing, since those are only from missing surveys.
Nonzero = Count.applymap(lambda i: float(i>0) if not np.isnan(i) else np.nan)
#~ Restrict both df's to nonzero responses.
Count = Count.replace(0,np.nan)
Vals  =  Vals.replace(0,np.nan)
too_many_zeros = 30
many_zeros = [item for item in Vals if Vals[item].notnull().sum()<too_many_zeros]
for df in (Nonzero, Count, Vals): df.drop(many_zeros,1, inplace=True)

#~ Merge in Control Vars
controls = ["cons","TUP","Cash"]
Nonzero  = Nonzero.join(D[controls],how="left")
Count    =   Count.join(D[controls],how="left")
Vals     =    Vals.join(D[controls],how="left")

Items = [item[:-2] for item in Vals if item.endswith("_e")]
CTL = Vals[controls].sum(axis=1)==1 #~ i.e. only constant ==1, TUP & Cash ==0
Zeros_ctl_mean = Nonzero.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Count_ctl_mean =   Count.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Vals_ctl_mean  =    Vals.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])

ZeroTable       = reg_table(regressions(Nonzero,"_e", outcomes=Items, controls = ["cons",'Cash','TUP'], Baseline="_b"), orgtbl=True, Transpose=True)
Count, Count_se = reg_table(regressions(Count,"_e",   outcomes=Items, controls = ["cons",'Cash','TUP'], Baseline="_b"), resultdf=True, Transpose=True)
Vals, Vals_se   = reg_table(regressions(Vals,"_e",    outcomes=Items, controls = ["cons",'Cash','TUP'], Baseline="_b"), resultdf=True, Transpose=True)

#~ Make full table of Standard errors-- MAKE SURE YOU HAVE THE SUFFIXES RIGHT.
SE = Count_se[["TUP","Cash"]].join(Vals_se[["TUP","Cash"]], rsuffix=" (SSP)", lsuffix=" (# own)")

#~ Make full table of point estimates and control-group means
FullTable = pd.DataFrame({"# own (CTL)":Count_ctl_mean, "Value (CTL)":Vals_ctl_mean}).join(Count[["TUP","Cash"]])
FullTable = FullTable.join(Vals[["TUP","Cash","N"]], rsuffix=" (SSP)", lsuffix=" (# own)")
FullTable = df_to_orgtbl(FullTable, sedf=SE)
AllTables = FullTable+"\n\n"+ZeroTable
return AllTables
#+end_src


** Extras

#+name: Imports
#+BEGIN_SRC python :noweb no-export :results silent 
import sys
import numpy as np
import pandas as pd
from pandas.io import stata
import statsmodels.api as sm
sys.path.append("../../../vesdemand/Empirics")
import neediness as nd
from oct2py import octave
octave.addpath('../utils/IncPACK')
from matplotlib import pyplot as plt
import statsmodels.formula.api as smf

try: Input = raw_input
except NameError: Input = input
try: Items = dict.iteritems
except AttributeError: Items = dict.items

def find_outliers(Var, data, run=False, ax=False):
    """
    ax is a pair of axis objects into which will be put a histogram and a scatter plot
    """
    keep = [Var]+bool(run)*[run]
    data = data[keep].dropna(subset=[Var]).copy()
    Series=data[Var]
    try: run=data[run]
    except KeyError:
        run=pd.Series(range(len(Series)))
        run.name="sorted"
        Series=Series.sort_values()

    if not ax: fig, ax = plt.subplots(1,2)
    ax[0].set_title(Series.name)
    ax[0].hist(Series.values,bins=int(len(Series)/25))
    ax[1].scatter(run, Series)
    try: title="vs. "+run.name
    except: title=""
    ax[1].set_title(title)
    try: return fig,ax
    except NameError: return ax

def topcode(Series, **kwargs):
    """
    Need to implement two-sided censoring as well.
    WARNING: if Top<0, all zeros will be changed to Top
    """

    percent    = kwargs.setdefault("percent",99)
    stdev      = kwargs.setdefault("stdev",False)
    drop       = kwargs.setdefault("drop",False)
    drop_zeros = kwargs.setdefault("drop_zeros",True)
    twoway     = kwargs.setdefault("twoway",False)

    if drop_zeros: S = Series.replace(0,np.nan).dropna()
    else: S = Series.dropna()
    N_OBS = S.notnull().sum()
    if N_OBS<10: return S

    if percent: Top = np.percentile(S, percent)
    if stdev:   
        Top =  S.dropna().mean()
        Top += stdev*S.dropna().std()
    try: assert((not drop_zeros) or Top>0)
    except AssertionError: raise ValueError("Top < 0 but zeros excluded")
    if drop: replace_with = np.nan
    else:    replace_with = Top
    Series[Series>Top]=replace_with

    if not twoway: return Series
    else:
        kwargs['twoway'] = False
        return -1*topcode(-1*Series, **kwargs)
def sqr(List, width=4):
    """
    Returns a rectangular string representation of List
    For easy reading in interactive mode
    if List is a pd.DataFrame, returns sqr of columns
    """
    try: List=List.columns.tolist()
    except AttributeError: pass
    lines = []
    for i in range(int(len(List)/width)+1):
        sublist= List[width*i:width*(i+1)]
        lines.append(", ".join(map(str,sublist)))
    square = "\n".join(lines)
    print(square)

def small(df,n=4,m=None):
    """
    Returns top n rows and m columns of a df or Series for viewing
    like pd.head(), but gives a subset of columns
    if n or m are negative, gives bottom rows and last columns
    """
    if m is None: m=n
    if (len(df.shape)==1):
        if n>0: return df.iloc[:n]
        else: return df.iloc[n:]
    if (n>0) and (m>0): return df.iloc[:n,:m]
    else: return df.iloc[n:,m:]

def df_to_orgtbl(df,tdf=None,sedf=None,float_fmt='%5.3f'):
    """
    Print pd.DataFrame in format which forms an org-table.
    Note that headers for code block should include ':results table raw'.
    """
    if len(df.shape)==1: # We have a series?
       df=pd.DataFrame(df)

    if (tdf is None) and (sedf is None):
        return '|'+df.to_csv(sep='|',float_format=float_fmt,line_terminator='|\n|')
    elif not (tdf is None) and (sedf is None):
        s = '| |'+'|  '.join(df.columns)+' |\n|-\n'
        for i in df.index:
            s+='| %s ' % i
            for j in df.columns:
                try:
                    stars=(np.abs(tdf.loc[i,j])>1.65) + 0.
                    stars+=(np.abs(tdf.loc[i,j])>1.96) + 0.
                    stars+=(np.abs(tdf.loc[i,j])>2.577) + 0.
                    if stars>0:
                        stars='^{'+'*'*stars + '}'
                    else: stars=''
                except KeyError: stars=''
                if np.isnan(df.loc[i,j]): entry='| $ $ '
                else: entry='| $'+float_fmt+stars+'$ '
                s+=entry % df.loc[i,j]
            s+='|\n'
        return s

    elif not sedf is None: # Print standard errors on alternate rows
        s = '| |'+'|  '.join(df.columns)+' |\n|-\n'
        for i in df.index:
            s+='| %s ' % i
            for j in df.columns: # Point estimates
                try:
                    stars = (np.abs(df.loc[i,j]/sedf.loc[i,j])>1.65) + 0.
                    stars+= (np.abs(df.loc[i,j]/sedf.loc[i,j])>1.96) + 0.
                    stars+= (np.abs(df.loc[i,j]/sedf.loc[i,j])>2.577) + 0.
                    if stars>0:
                        stars='^{'+'*'*stars + '}'
                    else: stars=''
                except KeyError: stars=''
                if np.isnan(df.loc[i,j]): entry='| $ $ '
                else: entry='| $'+float_fmt+stars+'$ '
                s+=entry % df.loc[i,j]
            s+='|\n|'
            for j in df.columns: # Now standard errors
                s+=' '
                try:
                    if not np.isnan(sedf.loc[i,j]):
                        se='$(' + float_fmt % sedf.loc[i,j] + ')$' 
                        entry='| '+se+' '
                    else: entry='| '
                except KeyError: entry='| '
                s+=entry 
            s+='|\n'
        return s

#+end_src

#+name: regression_tools
#+BEGIN_SRC python :noweb no-export :results silent 

def fe_reg(y,X,group, data=None, alert=True):
    """
    y is an outcome vector
    X is a matrix of controls
      UNLESS data is not None, where y and X are a string and list of strings

    `group' is a variable name in X to be used for the fixed-effects
    if data is not None and `group' will be added to X without warning or error if not included


    NOTES:

    There is *NO* note as to which FE's were used

    Demean-ing proceedure described in
    http://www.stata.com/support/faqs/statistics/intercept-in-fixed-effects-model/

    The standard errors *still* aren't quite the same, but closer
    but point estimates appear to be correct
    Even if data is missing
    """
    if data is not None:
        if group not in X: X.append(group)
        y, X = data[y], data[X]
    try: y.name #~ Check that y is a Series
    except AttributeError: y=pd.Series(y.iloc[:,0])
    if not y.name: Outcome="Y" #~ If y doesn't have a name, call it "Y"
    else: Outcome=y.name
    #~ Adding Outcome to X for the demeaning and everythign
    X[Outcome] = y
    X = X.dropna()
    VARS = [var for var in X.columns if var is not group]

    #~ def demean(df):
    #~     M = df.mean()
    #~     for var in df:
    #~         if var in M: df[var] = df[var]-M[var]
    #~     return df

    GrandMean = X[VARS].mean()
    if alert: print("De-meaninging")
    demean = lambda grp: grp-grp.mean()
    X = X.groupby(group)[VARS].apply(demean)
    #~ X = pd.concat([demean(grp_data.drop(group,1)) for grp, grp_data in byGroup])
    for var in VARS:
        if var in GrandMean: X[var] = X[var]+GrandMean[var]
    X['const'] = 1.
    y, X = X[Outcome], X.drop(Outcome, 1)
    if alert: print("Running Regression")
    model = sm.OLS(y,X,missing='drop')
    return model

def regressions(DF,Year="", **kwargs):
    """ Run a set of regressions and return a dict of {Outcome: sm.OLS (or RLM) model} for each model
     DF:
         The full dataset with outcomes and control variables.
     Year:
         A suffix on each outcome variable, specifying which round of data is being used. (Default to "")
     Baseline:
         A suffix on each variable to be used as a baseline covariate, specifying which round of data is being used.
         If the outcome variable doesn't have a corresponding column with that suffix, passes without error.
         (Default to 2013)
     Controls:
         A list or tuple of variables to be used as covariates in each regression.
     Outcomes:
         The list of outcomes (also the names of the models)
     rhs_extra:
         A dictionary of covariates to be added to the regression for specific outcomes.
     Baseline_na:
         If True, code missing values of baseline variable as zero and include a "Bsln_NAN" indicator in outcomes.
     Robust:
         If True, use statsmodel's RLM class instead of OLS (defaults to Huber-T se's)
     Return:
         dict {outcome var:model} for each outcome in outcomes.
    """
    #~ Kwargs
    Baseline    = kwargs.setdefault("Baseline",  2013)
    controls    = kwargs.setdefault("controls",  ["cons",'Cash','TUP'])
    rhs_extra   = kwargs.setdefault("rhs_extra", {})
    outcomes    = kwargs.setdefault("outcomes",  [])
    baseline_na = kwargs.setdefault("baseline_na", True)
    robust      = kwargs.setdefault("robust",    False)
    

    if robust: regress=sm.RLM
    else: regress=sm.OLS
    if not type(Year)==str: Year=str(Year)
    if not type(Baseline)==str: Baseline=str(Baseline)
    models_ols = {}

    for outcome in outcomes: #~ Run regressions and store models in a dictionary
        Yt = [outcome+Year]
        if outcome+Baseline in DF: #~ Present in DataFrame
            if DF[outcome+Baseline].isnull().sum(): Yt.append(outcome+Baseline)
        if outcome in rhs_extra:
            if not type(rhs_extra[outcome]) in (list,tuple): rhs_extra[outcome] = [rhs_extra[outcome]]
            for x in rhs_extra[outcome]:
                try: assert(x in DF)
                except AssertionError: raise KeyError("Extra Covariate for outcome {} not found in data".format(x,outcome))
            Yt += list(rhs_extra[outcome])
        df = DF[Yt+controls].rename(columns={outcome+Baseline:"Bsln"+Baseline})
        if "Bsln"+Baseline in df and baseline_na:
            df["Bsln_NAN"] = df["Bsln"+Baseline].isnull().apply(int)
            df["Bsln"+Baseline].fillna(0,inplace=True)
        df = df.dropna()
        #~ Full-sample OLS
        models_ols[outcome] = regress(df[outcome+Year], df.drop(outcome+Year,1)).fit()
        del df
    return models_ols
    #~ TODO: SPLIT models and results into two functions.

def reg_table(models,**kwargs):
    """ Take a list or dict of sm.RegressionResults objects and create a nice table.
     Summary: (Default)
       If True, return a summary_col object (from sm.iolib.summary2), which allows for as_text and as_latex
     Orgtbl:
       If True, return an orgtable (uses df_to_orgtbl) for the OLS model params.
     Resultdf:
       Returns the coefficient and SE df's for modification and subsequent entry into df_to_orgtbl.
       Useful for adding other columns/rows, like control-group means
     table_info:
       A list of model statistics that can be included at the bottom (like with stata's esttab)
       Allows for "N", "R2", "R2-adj", "F-stat"
       Defaults to just "N"
     Transpose:
       Places outcomes on left with regressors on top.
    """
    try: Items = dict.iteritems
    except AttributeError: Items = dict.items

    summary    = kwargs.setdefault("summary",   True)
    orgtbl     = kwargs.setdefault("orgtbl",    False)
    resultdf   = kwargs.setdefault("resultdf",  False)
    table_info = kwargs.setdefault("table_info", "N")
    Transpose  = kwargs.setdefault("Transpose", False)
    summary    = not any((orgtbl, resultdf)) #~ Summary by default
 
    #~ Construct the Summary table, using either table or df_to_orgtbl
    if table_info:
        if type(table_info) not in (list,tuple): table_info=[table_info]
        info_dict = {"N": lambda model: model.nobs,
                     "R2": lambda model: model.rsquared,
                     "R2-adj": lambda model: model.rsquared_adj,
                     "F-stat": lambda model: model.fvalue}
        info_dict = dict([(x,info_dict[x]) for x in table_info])

    if summary:
        from statsmodels.iolib import summary2
        Summary = summary2.summary_col(models.values(), stars=True, float_format='%.3f',info_dict=info_dict)
        #~ This mangles much of the pretty left to the Summary2 object and returns a pd.DF w/o se's
        if Transpose: Summary = Summary.tables[0].T.drop("",1)

    else:
        # Extras = lambda model: pd.Series({"N":model.nobs})
        # results = pd.DataFrame({Var:model.params.append(Extras(model)) for Var,model in models.iteritems()})
        results = pd.DataFrame({Var:model.params for Var,model in Items(models)})
        SEs     = pd.DataFrame({Var:model.bse    for Var,model in Items(models)})
        if table_info:
            extras = pd.DataFrame({Var: pd.Series({name:stat(model) for name,stat in Items(info_dict)}) for Var,model in Items(models)})
            results = results.append(extras)
        if Transpose: results, SEs = results.T, SEs.T

        if orgtbl: Summary = df_to_orgtbl(results,sedf=SEs)
        else:
            assert(resultdf)
            Summary = results, SEs

    return Summary


#+end_src

#+name: residuals_by_group
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
def residuals_by_group(models, groups, outcomes=[], kind="kde", figure_dir="../figures/", seriesname="Treat", blanks_to = "Control"):
    """
     Takes a set of statsmodels regression results and,
     for each outcome, produces a plot comparing the
     distribution of residuals by group.

     models:
         A dictionary of the form {variable name: sm.RegressionResults}. Empty defaults to all available.
     groups:
         A list, pd.Series, or pd.DataFrame with variables
         (A later version could contain an arbitrary set of categorical and give groups for every combination.)
     outcomes:
         A list specifying which variables in models to make plots for.
     kind:
         kde (or density) or histogram (or hist)
         Density plots are on a single axis. Histograms are stacked by group.
     figure_dir:
         The directory into which the figures get saved. If doesn't exist, throws error (future version might make that directory on the fly.)
     Seriesname:
         If a series or list is passed without a name, defaults to `seriesname'
     blanks_to:
         Observations with no treatment status from "groups" gets renamed to `blanks_to'
    """

    #~ Make outcomes a list. If empty, defaults to all variables in models
    if type(outcomes)==str: outcomes=[outcomes]
    if not outcomes: outcomes = sorted(models.keys())

    #~ Make data frame and make "Group" categorical
    df = pd.DataFrame(groups).rename(columns={0:seriesname})
    for var in df: df[var] = df[var].applymap(lambda x: var if x else "")
    df["Group"] = df.sum(axis=1).replace("",blanks_to)

    #~ Make residuals
    for var in outcomes:
        #~ Make column of residual values
        resid_var = "resid_{}".format(var)
        df[resid_var] = models[var].resid
        #~ Groupby object
        groups = df.dropna(subset=[resid_var]).groupby("Group")[resid_var]

        #~ Plot density by group
        if kind in ("kde", "density"):
            fig, ax = plt.subplots()
            groups.plot(kind=kind, ax=ax, legend=True)
            fig.savefig(figure_dir+resid_var+".png")

        #~ Plot histograms by group
        elif kind in ("hist", "histogram"):
            i=0
            fig, ax = plt.subplots(len(set(df["Group"])),1,sharex=True)
            for group, data in grps[var]:
                ax[i].hist(data.values, bins=20)
                ax[i].set_title(group)
                i+=1
            i=0
            fig.savefig(figure_dir+resid_var+".png")
        print(resid_var+".png created.")


#+end_src
   
