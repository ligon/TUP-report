:SETUP:
#+TITLE: TUP 2015 Endline Report
#+AUTHOR: Elliott Collins 
#+DATE: November 1, 2015
#+OPTIONS: texht:t toc:nil
#+LATEX_CLASS_OPTIONS: [12pt,article]
#+LATEX_HEADER:       \newcommand{\T}{\top}
#+LATEX_HEADER:       \newcommand{\E}{\ensuremath{\mbox{E}}}
#+LATEX_HEADER:       \newcommand{\R}{\ensuremath{\mathbb{R}}}
#+LATEX_HEADER:       \newcommand{\one}{\ensuremath{\mathbbm{1}}}
#+LATEX_HEADER:       \newcommand{\Eq}[1]{(\ref{eq:#1})}
#+LATEX_HEADER:       \renewcommand{\vec}[1]{\boldsymbol{#1}}
#+LATEX_HEADER_EXTRA: \usepackage{bbm}
#+LATEX_HEADER_EXTRA: \usepackage{dcolumn}\newcolumntype{d}[1]{D{.}{.}{#1}}
#+LATEX_HEADER_EXTRA: \newtheorem{proposition}{Proposition} \newcommand{\Prop}[1]{Proposition \ref{prop:#1}}
#+LATEX_HEADER_EXTRA: \newtheorem{theorem}{Theorem} \newcommand{\Thm}[1]{Theorem \ref{thm:#1}}
#+LATEX_HEADER_EXTRA: \newtheorem{remark}{Remark} \newcommand{\Rem}[1]{Remark \ref{rem:#1}}
#+LATEX_HEADER_EXTRA: \newtheorem{condition}{Condition} \newcommand{\Cond}[1]{Condition \ref{cond:#1}}
#+LATEX_HEADER_EXTRA: \newtheorem{lemma}{Lemma} \newcommand{\Lem}[1]{Lemma \ref{prop:#1}}
#+LATEX_HEADER_EXTRA: \newcommand{\Fig}[1]{Figure \ref{fig:#1}} \newcommand{\Tab}[1]{Table \ref{tab:#1}}
#+LATEX_HEADER_EXTRA: \usepackage{stringstrings}\renewcommand{\cite}[1]{\caselower[q]{#1}\citet{\thestring}}
:END:

* Introduction
 
*Preliminary Results of BRAC-South Sudan's TUP pilot in Yei*

A census was conducted in 2013 in the area around BRAC's Yei County offices to
identify eligible women for participation in the Transfers to the Ultra-Poor
(TUP) program. A baseline survey was conducted in 2013 which successfully
interviewed 649 women, of whom 250 were selected to participate in the
program. Another 125 were selected to receive cash grants of $350-$410US,
which was identified as the marginal cost of adding a new participant
to the program. Half of each group was randomly selected to receive additional
"top-up" transfers with market value around 20% of the original transfers. This
report presents estimates of the impact these treatments had on key household
variables.[fn:: Only consumption and asset holdings have been looked at so far. Other
important ones will be financial assets, income, occupation, food security.]

In response to the outbreak of violence in late 2013 and subsequent closing of the
offices in Yei, a midline survey was conducted in 2014 to ensure that we could
separate pre- and post-conflict changes in outcomes. For lack of a valid comparison group, we will
not speak with any authority about the effect of the conflict on economic conditions
in Yei, though we will report estimates of treatment effects on the severity or
likelihood of having been effected exposure to the conflict. Some of the original asset transfers were done before the
office closure, which may bias estimates of the difference between programs if rates
of return changed in the few intervening months. 

Finally, an endline survey was conducted in mid-2015 to estimate the effect of program
participation on households' financial situation and overall welfare. 

* Empirical Strategy 

In order to examine the parameters of interest in a single equation, we use
interactions between time fixed effects and group assignment, as well as baseline
values of the outcome variable where available. We take the terms including TUP
assignment as the treatment effects at midline and endline. The interaction of /Cash/
and /2015/ is the endline treatment effect of the cash treatment. Since those
transfers happened after the midline survey, its interaction with /2014/ acts as a
placebo. Since the timing is mismatched between treatments, we report a t-test of the
hypothesis \beta_{TUP,t}-\beta_{Cash,2015}=0 for both years. We prefer the comparison
between endline results, as the difference in timing is smaller in 2015.

\begin{equation*}
Y_{it} =\sum_{t=2014}^{2015}\delta_{t}+\beta_{t}^{Cash}I_{t}*Cash_{it}+\beta_{t}^{TUP}I_{t}*TUP_{it}+\gamma Y_{i,2013}+\epsilon_{i}
\end{equation*}
 
- $\delta_{t}$: Time FE's,  $I_{2014}= 1 if t==2014$
- Estimate four treatment effects, $(2014,2015)\times(TUP,Cash)$
- $\beta^{CSH}_{2014}$ is a placebo test.
- Test $\beta^{CSH}_{2015} = \beta^{TUP}_{2014}$ and $\beta^{CSH}_{2015} = \beta^{TUP}_{2015}$
 
* Results
** Randomization Check

A crucial assumption is that the treatment and control groups were selected
appropriately. We check this by presenting summary statistics by group on a
range of factors related to consumption, asset holdings, and household
characteristics. For the puroposes of this analysis, we pool the control and
cash groups and present balance on observables in Table \ref{tab:summary_statistics}.

#+BEGIN_SRC python :dir ../analysis :results value table :exports none
import check_balance
return check_balance.tables
#+END_SRC

#+name: tab:summary_statistics
#+caption: Means of some analysis variables at baseline.  Asterisks in the column labeled "Diff." are an indication of a significant difference between the means reported in the "CTL" and "TUP" columns.
#+attr_latex: :environment longtable :align lrrrrr
|                  | $N$ |     CTL |     TUP |   Diff. |  $p$ |
|------------------+-----+---------+---------+---------+------|
| Goods            |     |         |         |         |      |
|------------------+-----+---------+---------+---------+------|
| Fuel             | 456 |   23.44 |   24.53 |    1.09 | 0.75 |
| Oil              | 613 |    4.48 |    3.74 |   -0.74 | 0.34 |
| Cereals          | 605 |   28.55 |   24.93 |   -3.63 | 0.19 |
| Soap             | 536 |   15.22 |   15.02 |   -0.20 | 0.94 |
| Sugar            | 604 |    6.03 |    5.64 |   -0.39 | 0.77 |
| Cosmetics        | 468 |   20.05 |   21.76 |    1.71 | 0.59 |
| Egg              | 276 |    3.50 |    3.10 |   -0.40 | 0.44 |
| Transport        | 193 |    5.72 |    5.32 |   -0.41 | 0.79 |
| Beans            | 192 |    2.37 |    3.07 |    0.70 | 0.22 |
| Meat             | 378 |   13.23 |   11.12 |   -2.12 | 0.11 |
| Fish             | 474 |    7.48 |    7.07 |   -0.41 | 0.53 |
| Salt             | 617 |    1.40 |    1.28 |   -0.12 | 0.37 |
| Vegetables       | 471 |    4.66 |    4.23 |   -0.43 | 0.38 |
| Fruit            | 272 |    2.17 |    2.09 |   -0.08 | 0.82 |
|------------------+-----+---------+---------+---------+------|
| Asset Values     |     |         |         |         |      |
|------------------+-----+---------+---------+---------+------|
| Shop             |  44 |   85.46 |   79.41 |   -6.04 | 0.89 |
| Sewing           |  28 |    8.56 |    4.96 |   -3.60 | 0.42 |
| Poultry          | 161 |   39.68 |   39.04 |   -0.64 | 0.94 |
| Homestead        | 274 | 4432.11 | 4738.73 |  306.62 | 0.77 |
| Tv               |  42 |   36.28 |   45.94 |    9.66 | 0.54 |
| Radio            | 260 |   53.39 |   52.48 |   -0.91 | 0.94 |
| Motorcycle       |  93 |  450.07 |  534.69 |   84.62 | 0.48 |
| Bicycle          | 171 |  105.58 |   96.52 |    -9.1 | 0.65 |
| Bed              | 521 |  251.30 |  249.26 |   -2.04 | 0.94 |
| Chairs & tables  | 531 |  207.89 |  177.42 |  -30.48 | 0.31 |
| Mobile           | 414 |   96.25 |  110.16 |   13.91 | 0.14 |
| Mosquito Nets... | 423 |   19.24 |   19.83 |    0.59 | 0.77 |
| ...Some treated  | 181 |    8.18 |    9.04 |    0.85 | 0.56 |
| Shed             |   9 |    1.85 |    0.02 | -1.83** | 0.03 |
| Cows             |  35 |  222.79 |  112.70 | -110.09 | 0.19 |
| Carts            |  17 |    2.31 |    3.48 |    1.17 | 0.45 |
| Small animals    | 123 |  198.90 |  150.53 |  -48.37 | 0.36 |
| Fan              |  16 |    3.56 |    1.84 |   -1.71 | 0.28 |
|------------------+-----+---------+---------+---------+------|
| Other Variables  |     |         |         |         |      |
|------------------+-----+---------+---------+---------+------|
| # Children       | 623 |    3.90 |    3.77 |   -0.12 | 0.47 |
| Daily Exp        | 649 |   33.12 |   30.56 |   -2.56 | 0.15 |
| # Houses         | 543 |    2.87 |    2.86 |   -0.01 | 0.97 |
| Cash Savings     | 431 |  173.76 |  185.79 |   12.04 | 0.71 |
| Asset total      | 603 | 1787.27 | 1712.26 |  -75.01 | 0.73 |
| Land (Fedan)     | 542 |    2.16 |    1.86 |   -0.30 | 0.17 |
| In Business?     | 265 |    0.40 |    0.44 |    0.03 | 0.42 |
| Cereals          | 605 |   28.55 |   24.93 |   -3.63 | 0.19 |
| HH size          | 648 |    7.32 |    7.06 |   -0.27 | 0.18 |
| Cosmetics        | 468 |   20.05 |   21.76 |    1.71 | 0.59 |
|------------------+-----+---------+---------+---------+------|


Generally speaking, this is at best suggestive evidence the comparability of
these groups, but it suggests that our stratified randomization was not too far
from creating comparable groups.

** Consumption

The first measure of welfare to consider is household consumption, defined as the market
value of goods or services used by the household. A sizable basket of goods were
included in the survey module. These are separated into three categories: Food items
with a 3-day recall window, non-durables with a /Month/ recall window, and durables
and large expenditures with a /year/ recall window. Consumption, as both the total amount
and the composition of household spending, is perhaps the most appropriate
measure of the welfare or poverty of a household in our survey. 

Importantly, we know only a little about the changes in prices over this
period, except to say that inflatino was as high as 100% between 2014 and 2015, making
available price indices uninformative. Nonetheless, we take the sum of all
consumption and expenditure questions together as a measure of welfare. [fn:: Details
on this issue are discussed further in Beegle (2012).] 

The results for several important consumption measures are presented in Table
\ref{tab:consumption}. We can compare estimates in this most recent survey to those from
the midline survey, which we take to be "short-term" effects of the TUP program,
since they had recently received a large transfer. 

The main finding is that consumption increased in 2014 for the TUP group and in 2015
for the Cash group. Food transfers had recently ceased when the 2014 survey was
conducted, and the assets had been transferred 6-8 months prior. The results for the
cash group in 2015 are from more than a year after the transfers were given out, and
the TUP group sees no notable difference from control in that period.

This is arguably consistent with a story in which either sort of transfer has a
short-term affect on consumption that does not persist. It is also consistent with a
story in which cash is consumed (rather than saved or invested) in a high-inflation
environment while the transfer of productive assets mostly predicts wealth instead of
consumption.

In either case, the increase in total consumption appears to be driven by increased
food consumption, and there is no evidence that the share of food consumed falls, as
might be predicted by Engel's law.  This all translates to an economically (though
not statistically) significant difference between the TUP and Cash point estimates.

 #+name: consumption_aggregate_results
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
 <<consumption_analysis>>
 return tab
 #+end_src

#+name: tab:consumption
#+caption: Average treatment effects by Group (TUP, Cash) and Year (2014, 2015), controlling for baseline levels.
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
|                                  | Tot           | logTot        | Food          | FoodShr       | Month         | Year          |
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
| CTL mean                         | $39.80^{*}$   | $ 3.52^{***}$ | $27.46^{*}$   | $ 0.70^{***}$ | $ 9.73$       | $ 3.07$       |
|                                  | $(22.18)$     | $( 0.61)$     | $(15.54)$     | $( 0.18)$     | $(10.38)$     | $( 5.48)$     |
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
| TUP*2014                         | $ 6.64^{***}$ | $ 0.21^{***}$ | $ 3.88^{***}$ | $-0.02$       | $ 1.94^{*}$   | $ 1.28^{**}$  |
|                                  | $( 1.99)$     | $( 0.05)$     | $( 1.28)$     | $( 0.02)$     | $( 1.02)$     | $( 0.50)$     |
| TUP*2015                         | $ 1.50$       | $ 0.04$       | $ 0.47$       | $-0.01$       | $ 1.13$       | $ 0.09$       |
|                                  | $( 1.89)$     | $( 0.05)$     | $( 1.21)$     | $( 0.01)$     | $( 0.96)$     | $( 0.47)$     |
| CSH*2014                         | $-0.04$       | $-0.03$       | $-0.08$       | $ 0.01$       | $ 0.96$       | $-0.38$       |
|                                  | $( 2.47)$     | $( 0.07)$     | $( 1.58)$     | $( 0.02)$     | $( 1.28)$     | $( 0.62)$     |
| CSH*2015                         | $ 5.48^{**}$  | $ 0.14^{**}$  | $ 3.27^{**}$  | $-0.01$       | $ 2.17^{*}$   | $ 0.06$       |
|                                  | $( 2.42)$     | $( 0.07)$     | $( 1.54)$     | $( 0.02)$     | $( 1.24)$     | $( 0.61)$     |
| Bsln2013                         | $ 0.07^{**}$  | $ 0.06^{***}$ | $ 0.05^{**}$  | $ 0.08^{**}$  | $-0.11$       | $ 0.05$       |
|                                  | $( 0.03)$     | $( 0.02)$     | $( 0.02)$     | $( 0.03)$     | $( 0.15)$     | $( 0.03)$     |
| 2014                             | $26.17^{***}$ | $ 2.97^{***}$ | $17.01^{***}$ | $ 0.62^{***}$ | $ 8.32^{***}$ | $ 2.12^{***}$ |
|                                  | $( 1.66)$     | $( 0.08)$     | $( 1.05)$     | $( 0.03)$     | $( 0.80)$     | $( 0.36)$     |
| 2015                             | $36.81^{***}$ | $ 3.29^{***}$ | $25.33^{***}$ | $ 0.63^{***}$ | $10.14^{***}$ | $ 2.74^{***}$ |
|                                  | $( 1.55)$     | $( 0.08)$     | $( 0.99)$     | $( 0.03)$     | $( 0.74)$     | $( 0.33)$     |
| Bsln_NAN                         | $ 5.43^{**}$  | $ 0.31^{***}$ | $ 4.74^{***}$ | $ 0.09^{***}$ | $-0.74$       | $ 0.80^{*}$   |
|                                  | $( 2.17)$     | $( 0.09)$     | $( 1.36)$     | $( 0.03)$     | $( 0.87)$     | $( 0.43)$     |
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
| F-stat                           | $12.56$       | $15.13$       | $17.85$       | $ 2.25$       | $ 2.23$       | $ 2.12$       |
| N                                | $1305.00$     | $1305.00$     | $1286.00$     | $1286.00$     | $1296.00$     | $1260.00$     |
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $ 1.16$       | $ 0.07$       | $ 0.61$       | $-0.01$       | $-0.23$       | $ 1.22$       |
|                                  | $( 3.09)$     | $( 0.09)$     | $( 1.97)$     | $( 0.02)$     | $( 1.59)$     | $( 0.78)$     |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $-3.98$       | $-0.10$       | $-2.80^{*}$   | $-0.00$       | $-1.04$       | $ 0.03$       |
|                                  | $( 2.51)$     | $( 0.07)$     | $( 1.60)$     | $( 0.02)$     | $( 1.30)$     | $( 0.64)$     |
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|

** Food Security

#+name: foodsecure_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<foodsecure_analysis>>
return tab
#+end_src

|----------------------------------+--------------+-----------+-------------+-------------+-------------+-------------|
| Food Security                    | Z-Score      | Wholeday  | Hungry      | Nofood      | Fewmeals    | Portions    |
|----------------------------------+--------------+-----------+-------------+-------------+-------------+-------------|
| CTL mean                         | $ 0.01$      | $ 0.02$   | $ 0.05$     | $-0.02$     | $ 0.05$     | $-0.02$     |
|                                  | $( 0.98)$    | $( 0.97)$ | $( 1.01)$   | $( 0.98)$   | $( 0.99)$   | $( 0.96)$   |
|----------------------------------+--------------+-----------+-------------+-------------+-------------+-------------|
| Bsln2013                         | $ 0.07^{**}$ | $ 0.03$   | $ 0.05^{*}$ | $ 0.02$     | $ 0.05^{*}$ | $-0.00$     |
|                                  | $( 0.03)$    | $( 0.03)$ | $( 0.03)$   | $( 0.03)$   | $( 0.03)$   | $( 0.03)$   |
| 2014                             | $-0.06$      | $-0.06$   | $-0.08$     | $-0.06$     | $-0.02$     | $ 0.02$     |
|                                  | $( 0.06)$    | $( 0.06)$ | $( 0.06)$   | $( 0.06)$   | $( 0.06)$   | $( 0.06)$   |
| 2015                             | $-0.03$      | $-0.00$   | $ 0.03$     | $-0.02$     | $ 0.02$     | $-0.06$     |
|                                  | $( 0.06)$    | $( 0.06)$ | $( 0.06)$   | $( 0.06)$   | $( 0.06)$   | $( 0.06)$   |
| TUP*2014                         | $ 0.11$      | $ 0.09$   | $ 0.15$     | $ 0.15^{*}$ | $ 0.00$     | $-0.08$     |
|                                  | $( 0.09)$    | $( 0.09)$ | $( 0.09)$   | $( 0.09)$   | $( 0.09)$   | $( 0.09)$   |
| TUP*2015                         | $ 0.01$      | $-0.01$   | $-0.08$     | $ 0.07$     | $-0.13$     | $ 0.07$     |
|                                  | $( 0.09)$    | $( 0.09)$ | $( 0.09)$   | $( 0.09)$   | $( 0.09)$   | $( 0.09)$   |
| CSH*2014                         | $ 0.02$      | $ 0.05$   | $ 0.07$     | $ 0.04$     | $ 0.01$     | $-0.02$     |
|                                  | $( 0.11)$    | $( 0.11)$ | $( 0.11)$   | $( 0.11)$   | $( 0.11)$   | $( 0.11)$   |
| CSH*2015                         | $-0.01$      | $-0.05$   | $-0.11$     | $-0.01$     | $ 0.01$     | $ 0.07$     |
|                                  | $( 0.11)$    | $( 0.11)$ | $( 0.11)$   | $( 0.11)$   | $( 0.11)$   | $( 0.11)$   |
| Bsln_NAN                         | $ 0.16^{*}$  | $ 0.09$   | $ 0.09$     | $-0.01$     | $ 0.15^{*}$ | $ 0.16^{*}$ |
|                                  | $( 0.09)$    | $( 0.08)$ | $( 0.09)$   | $( 0.09)$   | $( 0.09)$   | $( 0.09)$   |
|----------------------------------+--------------+-----------+-------------+-------------+-------------+-------------|
| F-stat                           | $ 1.37$      | $ 0.43$   | $ 1.28$     | $ 0.69$     | $ 1.31$     | $ 0.69$     |
| N                                | $1299.00$    | $1282.00$ | $1297.00$   | $1293.00$   | $1297.00$   | $1292.00$   |
|----------------------------------+--------------+-----------+-------------+-------------+-------------+-------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $ 0.12$      | $ 0.14$   | $ 0.26^{*}$ | $ 0.17$     | $-0.00$     | $-0.15$     |
|                                  | $( 0.14)$    | $( 0.14)$ | $( 0.14)$   | $( 0.14)$   | $( 0.14)$   | $( 0.14)$   |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $ 0.02$      | $ 0.04$   | $ 0.03$     | $ 0.09$     | $-0.13$     | $ 0.00$     |
|                                  | $( 0.12)$    | $( 0.12)$ | $( 0.12)$   | $( 0.12)$   | $( 0.12)$   | $( 0.12)$   |
|----------------------------------+--------------+-----------+-------------+-------------+-------------+-------------|

Observed changes in total consumption don't translate into improved food security.
In each year, we ask how often the respondent has had experiences indicative of food
insecurity. Included are (from left to right) going a whole day without eating, going to sleep hungry,
being without any food in the house, eating fewer meals than normal at mealtimes, and
limiting portions. We report standardized measures of each, as well as a standardized
composite z-score using all of these questions. While TUP participants by and large
have slightly higher scores on average, there is little evidence of a significant treatment
effect at endline.

** Assets
   
We turn now to asset holdings for the households. Controlling for baseline asset holdings
where possible, we estimate treatment effects for total value of assets owned, total
value of potentially "productive" assets, and the quantity and value owned for each of the 26
items in the survey. 

*** Total Asset Holdings

Perhaps interestingly, the cash group does not appear to have seen an increase in the
value of assets measured at all, with small and imprecise point estimates. The most
important result is that the TUP group has significantly more asset wealth than the
cash or control groups two years after receipt of transfers. The TUP group has a
sizeable change of 662 SSP on average (53% increase over controls, p<.01). Total
measured asset holdings in the TUP group have fallen 60 SSP on average, but 127 SSP
in the control group. 40% of TUP households report higher asset wealth than in the
midline survey, 12 percentage points higher than among control HH's (p<.01). This
actually represents a larger gain than seen in 2014, where the TUP program resulted
in a 38% increase in total asset holdings.

So-called "Productive" assets include anything that could plausibly be used in
productive activity. [fn:: For now, we include in this list: cows, goats and small
animals, chickens, ducks, ploughs, sheds, shop premises, pangas, axes, mobiles,
carts, and sewing equipment.] Here we see the TUP group has 336 SSP more in this area
over the control group, or twice as much. The total value of the treatment effect was
roughly the same at midline, while the percentage change over baseline was smaller at
74%. Note also that the effect on total value is higher than the effect on productive
asset value, suggesting that the increased wealth cannot be explained purely by
households holding onto asset transfers for the length of the program's monitoring
phase.

More interesting tests remain. [fn:: TODO: Separate effects by sub-groups (asset types & high/low asset levels)] [fn:: TODO: Add food and cash savings to the asset tables?]

*** COMMENT Asset Portfolio Composition

These results raise some worthwhile follow-up questions about the composition of
asset holdings across groups. First, \Tab{asset_disaggregate} presents treatment
effects on for the number and value of individual asset categories, while
\Tab{asset_nonzero} presents the treatment effects on probability of owning any of
each asset. We find unsurprisingly that the TUP group own significantly more ducks.
They are more likely to own small animals (primarily goats) and sheds (for housing
animals), though unlike with ducks, they do not hold larger or more valuble herds
than other goat rearing households. These results may account for the higher
productive portfolios, but the lack of clear or precise estimates outside of these
few categories is also arguably illuminating. The increase in non-productive asset
holdings is not clearly driven by a particular kind of purchase.

The cash group saw no notable aggregate change in wealth relative to baseline, but
also owned measurably more small animals (12 pp) and chickens (9 pp). However, they
also report owning small animals of notably (40%) lower value overall. We also see a
13% (11pp) increase in the percentage of households owning mobile phones.

*** Savings

Both treatment arms had significant impact on the average level of cash savings
within households. The TUP households are strongly encouraged to pay into a savings
account maintained by BRAC each time they meet. Annecdotally, this has discouraged
some women from attending the meetings, but it results in TUP participants being 44% (20
pp) more likely to report having any savings at all. Some skepticism might be
warranted in interpreting this result. Since the TUP households also regard their
savings behavior as much more transparent to BRAC (and have received pressure to save
from them) than the other groups, these households may simply be more likely to
reveal that they are saving when asked. Among those who have
savings, TUP households report having roughly 40% (72 SSP) more in value.

Cash households appear no more likely than the control households to report having
cash savings (around 45% in each group), but households that report saving report
having 56% (102 SSP) more in value. This is significantly less than was given to
these households of course, but combined with the short-term consumption results,
goes some distance in explaining the lack of effect on physical asset wealth.
This result is based on 50 cash households reporting cash savings, but is nonetheless
statistically significant.

It is common in this community (and most in the region) to store non-perishable food like
maize, cassava, or millet as a form of savings. This would seem particularly
reasonable in this high-inflation context, where the price of grain had doubled in
the previous year. At least as many households report saving
like this (53%) as have cash savings (46%), with an average market value of 106 SSP.
However, we find no evidence that either treatment group increased food savings.

*** Transfers & Loans

*Lending*

Neither do we find evidence that either treatment increased the size or likelihood of
giving or receiving interhousehold transfers, either in cash or in kind.

*** Land Holdings
We also find no evidence that either group is more likely to report owning or
cultivating land, though this may be because land ownership and cultivation is
already very common. Interestingly, both treatment groups report owning less land
overall than the control households. This difference is large and statistically
significant (p<.1) for the cash group, which reports cultivating 55% less and owning
38% less land than the control group. This raises the interesting question of whether
the cash group was likely to switch occupations from farming to non-farm
self-employment. [fn:: TODO: Write the occupational choice section...]



*** Tables
#+name: asset_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<asset_analysis>>
return tab
#+end_src

|                                  | Productive     | Total           |
|----------------------------------+----------------+-----------------|
| Bsln2013                         | $ 0.00$        | $ 0.08^{***}$   |
|                                  | $( 0.01)$      | $( 0.02)$       |
| 2014                             | $465.53^{***}$ | $1259.75^{***}$ |
|                                  | $(55.96)$      | $(112.68)$      |
| 2015                             | $392.97^{***}$ | $1124.61^{***}$ |
|                                  | $(50.21)$      | $(103.46)$      |
| TUP*2014                         | $361.80^{***}$ | $535.79^{***}$  |
|                                  | $(74.19)$      | $(154.02)$      |
| TUP*2015                         | $320.74^{***}$ | $624.79^{***}$  |
|                                  | $(68.68)$      | $(146.01)$      |
| CSH*2014                         | $18.50$        | $-125.86$       |
|                                  | $(95.80)$      | $(191.31)$      |
| CSH*2015                         | $-5.00$        | $-49.99$        |
|                                  | $(88.40)$      | $(187.32)$      |
| Bsln_NAN                         | $-131.14^{**}$ | $21.30$         |
|                                  | $(51.35)$      | $(146.51)$      |
| F-stat                           | $10.19$        | $ 8.53$         |
|                                  |                |                 |
| N                                | $1247.00$      | $1305.00$       |
|                                  |                |                 |
| CTL mean                         | $337.60$       | $1225.61$       |
|                                  | $(605.57)$     | $(1502.46)$     |
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $366.79^{***}$ | $585.78^{**}$   |
|                                  | $(114.58)$     | $(239.76)$      |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $325.74^{***}$ | $674.78^{***}$  |
|                                  | $(92.26)$      | $(194.72)$      |

#+name: savings_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<savings_analysis>>
return Table
#+end_src

|----------------------------------+----------------+----------------+----------------+----------------+----------------|
| % > 0                            | Savings        | Food Sav       | Give Trans     | LandCult       | LandOwn        |
|----------------------------------+----------------+----------------+----------------+----------------+----------------|
| CTL mean                         | $ 0.45$        | $ 0.82$        | $ 1.00$        | $ 0.82$        | $ 0.90$        |
|----------------------------------+----------------+----------------+----------------+----------------+----------------|
| CSH*2014                         | $-0.06$        | $ 0.00$        | $-0.00$        | $-0.04$        | $-0.01$        |
|                                  | $( 0.06)$      | $( 0.04)$      | $( 0.03)$      | $( 0.04)$      | $( 0.04)$      |
| CSH*2015                         | $ 0.03$        | $ 0.02$        | $-0.00$        | $ 0.05$        | $ 0.02$        |
|                                  | $( 0.05)$      | $( 0.04)$      | $( 0.04)$      | $( 0.04)$      | $( 0.04)$      |
| TUP*2014                         | $ 0.22^{***}$  | $-0.02$        | $-0.03$        | $-0.03$        | $-0.00$        |
|                                  | $( 0.04)$      | $( 0.03)$      | $( 0.03)$      | $( 0.03)$      | $( 0.03)$      |
| TUP*2015                         | $ 0.21^{***}$  | $-0.03$        | $-0.04$        | $ 0.01$        | $-0.01$        |
|                                  | $( 0.04)$      | $( 0.03)$      | $( 0.03)$      | $( 0.03)$      | $( 0.03)$      |
| 2014                             | $ 0.43^{***}$  | $ 1.00^{***}$  | $ 0.50^{***}$  | $ 0.83^{***}$  | $ 0.82^{***}$  |
|                                  | $( 0.04)$      | $( 0.02)$      | $( 0.02)$      | $( 0.06)$      | $( 0.05)$      |
| 2015                             | $ 0.39^{***}$  | $ 0.82^{***}$  | $ 0.50^{***}$  | $ 0.77^{***}$  | $ 0.84^{***}$  |
|                                  | $( 0.04)$      | $( 0.02)$      | $( 0.02)$      | $( 0.05)$      | $( 0.05)$      |
| Bsln2013                         | $ 0.05$        | $ $            | $ 0.51^{***}$  | $ 0.05$        | $ 0.07$        |
|                                  | $( 0.04)$      |                | $( 0.02)$      | $( 0.05)$      | $( 0.04)$      |
| Bsln_NAN                         | $ 0.08^{*}$    | $ $            | $ 0.49^{***}$  | $ 0.05$        | $ 0.05$        |
|                                  | $( 0.04)$      |                | $( 0.01)$      | $( 0.06)$      | $( 0.05)$      |
|----------------------------------+----------------+----------------+----------------+----------------+----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $ 0.19$        | $-0.04$        | $-0.02$        | $-0.07$        | $-0.02$        |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $ 0.18$        | $-0.05$        | $-0.03$        | $-0.03$        | $-0.03$        |
|----------------------------------+----------------+----------------+----------------+----------------+----------------|
| F-stat                           | $ 8.83$        | $15.60$        | $ 0.60$        | $ 0.79$        | $ 0.76$        |
| N                                | $1259.00$      | $870.00$       | $161.00$       | $1231.00$      | $1251.00$      |
|----------------------------------+----------------+----------------+----------------+----------------+----------------|

|----------------------------------+----------------+----------------+----------------+----------------+----------------|
| Amt.                             | Savings        | Food Sav       | Give Trans     | LandCult       | LandOwn        |
|----------------------------------+----------------+----------------+----------------+----------------+----------------|
| CTL mean                         | $191.19$       | $114.78$       | $138.40$       | $61.88$        | $46.00$        |
|----------------------------------+----------------+----------------+----------------+----------------+----------------|
| CSH*2014                         | $28.74$        | $ 0.22$        | $-61.19$       | $10.18$        | $10.50$        |
|                                  | $(42.93)$      | $(15.38)$      | $(57.24)$      | $(15.07)$      | $(12.57)$      |
| CSH*2015                         | $91.40^{**}$   | $-14.34$       | $17.37$        | $-39.18^{***}$ | $-32.37^{***}$ |
|                                  | $(40.89)$      | $(14.98)$      | $(72.41)$      | $(14.90)$      | $(11.95)$      |
| TUP*2014                         | $-27.09$       | $17.16$        | $32.65$        | $-4.76$        | $-3.02$        |
|                                  | $(29.76)$      | $(12.33)$      | $(43.79)$      | $(11.94)$      | $(10.04)$      |
| TUP*2015                         | $81.33^{***}$  | $ 1.13$        | $-41.12$       | $-17.38$       | $-12.56$       |
|                                  | $(29.32)$      | $(12.26)$      | $(50.57)$      | $(11.65)$      | $( 9.41)$      |
| 2014                             | $106.72^{***}$ | $62.03^{***}$  | $86.25^{*}$    | $11.37$        | $17.31^{**}$   |
|                                  | $(24.85)$      | $( 8.36)$      | $(49.01)$      | $( 9.94)$      | $( 8.56)$      |
| 2015                             | $163.04^{***}$ | $114.78^{***}$ | $128.32^{***}$ | $61.52^{***}$  | $51.89^{***}$  |
|                                  | $(24.13)$      | $( 7.60)$      | $(48.03)$      | $( 9.54)$      | $( 7.88)$      |
| Bsln2013                         | $ 0.05^{**}$   | $ $            | $ 0.02$        | $ 0.94$        | $-2.43$        |
|                                  | $( 0.02)$      |                | $( 0.09)$      | $( 3.07)$      | $( 1.95)$      |
| Bsln_NAN                         | $40.07^{*}$    | $ $            | $12.38$        | $-1.60$        | $-6.02$        |
|                                  | $(21.24)$      |                | $(41.51)$      | $( 9.92)$      | $( 8.29)$      |
| N                                | $671.00$       | $777.00$       | $159.00$       | $1042.00$      | $1114.00$      |
|----------------------------------+----------------+----------------+----------------+----------------+----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $-118.49$      | $31.50$        | $15.28$        | $34.42$        | $29.35$        |
|                                  |                |                |                |                |                |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $-10.07$       | $15.47$        | $-58.49$       | $21.79$        | $19.80$        |
|----------------------------------+----------------+----------------+----------------+----------------+----------------|

Transfers received contains outliers and needs fixing. There were no statistically
significant treatment effects.


** Income
#+name: income_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<income_analysis>>
return tab
#+end_src


#+Caption: Distribution of total observed income by group
#+NAME: fig:IncomeDistribution
[[../analysis/figures/IncomeDistribution.png]] 

Note that topcoding has a large effect on the distribution here. The control group in
2015 has a measured income of roughly 4325 SSP per year, or roughly $540 US (assuming
an exchange rate of around 8). The TUP group sees a 327 SSP ($41 US) increase in
annual average income, but with a fairly skewed distribution and high standard
errors). The related figure shows that total income is not particularly different
among groups. Perhaps the main lesson is that the TUP group has measurably more
reported livestock-related income, and less farm income, indicating a shift away from
farming. The cash group may exhibit some substitution away from farm and livestock,
but sees no notable change in income overall. 

** COMMENT Occupation
    
*** Occupation Code

Endline:
    household_roster.dta S1_6
    Check relationship code: S1_2

Midline:     

Baseline:
    income activities.dta (or income.dta, post income_cleanup.do)
    Occupation code: S3_2
    Check relationship code by merging on id and line no. from household roster.dta. 

*** TODO Should this (and other outcomes) be in logs?

    

** Other Outcomes
   
*** Confidence & Autonomy
*** Vulnerability

** Exposure to Conflict
#+name: conflict_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<conflict_analysis>>
return Table
#+end_src

|----------+---------------+---------------+---------------+---------------+---------------+---------------|
|          | Affected      | NoInvest      | Migrated      | NoMeans       | ProtectLives  | Worried       |
|----------+---------------+---------------+---------------+---------------+---------------+---------------|
| CTL mean | $ 0.53^{***}$ | $ 0.14^{***}$ | $ 0.34^{***}$ | $ 0.32^{***}$ | $ 0.41^{***}$ | $ 0.93^{***}$ |
|          | $( 0.03)$     | $( 0.02)$     | $( 0.02)$     | $( 0.02)$     | $( 0.03)$     | $( 0.01)$     |
|----------+---------------+---------------+---------------+---------------+---------------+---------------|
| TUP      | $-0.12^{***}$ | $-0.04$       | $ 0.03$       | $-0.05$       | $ 0.00$       | $-0.02$       |
|          | $( 0.04)$     | $( 0.03)$     | $( 0.04)$     | $( 0.04)$     | $( 0.05)$     | $( 0.02)$     |
| Bsln_NaN | $ 0.04$       | $ 0.13^{***}$ | $-0.05$       | $ 0.11^{*}$   | $-0.20^{**}$  | $-0.03$       |
|          | $( 0.07)$     | $( 0.05)$     | $( 0.07)$     | $( 0.07)$     | $( 0.09)$     | $( 0.04)$     |
|----------+---------------+---------------+---------------+---------------+---------------+---------------|
| F-stat   | $ 4.77$       | $ 5.42$       | $ 0.74$       | $ 2.67$       | $ 2.52$       | $ 0.45$       |
| N        | $601.00$      | $655.00$      | $655.00$      | $655.00$      | $585.00$      | $603.00$      |
|----------+---------------+---------------+---------------+---------------+---------------+---------------|

Here we see averages on responses regarding households' experiences of the outbreak
of violence in late 2013 and early 2014. Outcomes of interest are whether individuals
say they were "worried", "directly affected" by the violence, unable to invest in a farm or
business as a result, migrated as a cautionary measure, or did something else to
protect the lives of family members, and finally whether they took no cautionary
measures because they did not have the means (i.e. "NoMeans"). We find negative
associations with treatment for having been affected, having delayed investment, and
having no means to migrate or seek protection, though it is only precise for the
general question of having been "affected".
    
* Concluding Remarks

* COMMENT Extra Analysis

** Good-level analysis

Next, \ref{tab:consumption_full} sets aside these aggregated measures to look more
carefully at potential changes in the composition of consumption in each group. Given
the large number of zeros, we use a linear model to consider first the 
frequency of non-zero consumption of each good among treatment and control
households, then look at levels of consumption among households with non-zero
consumption. \Tab{consumption_full} presents point estimates.

A few changes in the composition of consumption are interesting. TUP households appear to consume 17% less
sorghum (often considered an inferior good in Yei) and more on rice, which is
considered a higher-quality staple. While almost everyone reports some health
spending over the past month, both treatment groups spent more, though only
statistically significant in the cash group, which saw a 50% increase over the
control group. The cash group was also 30% (14 pp) more likely to have spent money
on funerals, though they did not spend more on average.

#+name: consumption_disaggreate_results
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
<<consumption_analysis>>
from matplotlib import pyplot as plt
#~ Only keep disaggregate items
Goods = C.filter(regex="^c_").rename(columns=lambda col: col[2:] if col.startswith("c_") else col)
#~ 1 if coded as >0, 0 if zero, keep NaN's missing, since those are only from missing surveys.
Nonzero = Goods.applymap(lambda i: float(i>0) if not np.isnan(i) else np.nan)
#~ Restrict Goods df to positive responses.
Goods = Goods.replace(0,np.nan)
too_many_zeros = 30
many_zeros = [item for item in Goods if Goods[item].notnull().sum()<too_many_zeros]
Nonzero = Nonzero.drop(many_zeros,1)
Goods = Goods.drop(many_zeros,1)

#~ Merge in Control Vars
controls = ["cons","TUP","CSH"]
Goods = Goods.join(C[controls],how="left")
Nonzero = Nonzero.join(C[controls],how="left")
Items = [item[:-2] for item in Goods if item.endswith("_e")]
CTL = Goods[controls].sum(axis=1)==1 #~ i.e. only constant ==1, TUP & Cash ==0
Goods_ctl_mean =   Goods.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Zeros_ctl_mean = Nonzero.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])

Zero, Zero_se = reg_table(regressions(Nonzero,"_e", outcomes=Items, Baseline="_b"), resultdf=True, Transpose=True)
Good, Good_se = reg_table(regressions(Goods,"_e", outcomes=Items, Baseline="_b"  ), resultdf=True, Transpose=True)
#~ Make full table of Standard errors
SE = Zero_se[["TUP","CSH"]].join(Good_se[["TUP","CSH"]], lsuffix=" (%>0)", rsuffix=" (Amt.)")

#~ Make full table of point estimates and control-group means
FullTable = pd.DataFrame({"Mean (CTL)":Goods_ctl_mean, "% >0 (CTL)":Zeros_ctl_mean}).join(Zero[["TUP","CSH"]])
FullTable = FullTable.join(Good[["TUP","CSH","N"]], lsuffix=" (%>0)", rsuffix=" (Amt.)")

#~ Make % change graph
fig, ax = plt.subplots(2,1, figsize=(6,9))
for i, group in enumerate(("TUP","CSH")):
    pct_change = FullTable[group+" (Amt.)"]/FullTable["Mean (CTL)"]
    se_change  = SE[group+" (Amt.)"]/FullTable["Mean (CTL)"]
    pct_change.sort()
    pct_change.plot(kind="bar", yerr=SE[group+" (Amt.)"], ax=ax[i])
    ax[i].set_title(group, fontsize=6)
fig.savefig("figures/Consumption.png")

FullTable = df_to_orgtbl(FullTable, sedf=SE)
return FullTable
#+end_src

#+name: tab:consumption_full
#+caption: Control group means and estimated treatment effects for percent consuming any and total amounts consumed.
#+attr_latex: :environment longtable :align lrrrrrrr
 #+RESULTS: consumption_disaggreate_results

** Disaggregate Asset Results 

#+name: asset_disaggreate_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<assets_disaggreate_analysis>>
return tab
#+end_src

#+name: tab:asset_disaggregate
#+caption: Control means and treatment effects for Assets owned by >40 households
#+RESULTS: asset_disaggreate_results
|--------------+---------+---------------+--------------+-----------+----------------+----------------+----------|
|              | # own   |               |              | Value     |                |                |          |
|--------------+---------+---------------+--------------+-----------+----------------+----------------+----------|
| Item         | CTL     | TUP           | Cash         | CTL       | TUP            | Cash           | N        |
|--------------+---------+---------------+--------------+-----------+----------------+----------------+----------|
| Pangas       | $ 1.06$ | $ 0.01$       | $ 0.13^{**}$ | $11.86$   | $ 1.66^{**}$   | $ 0.04$        | $410.00$ |
| Chickens     | $ 3.79$ | $ 0.70$       | $-0.32$      | $154.35$  | $23.31$        | $ 0.80$        | $162.00$ |
| Mobile       | $ 1.88$ | $-0.09$       | $ 0.08$      | $113.96$  | $ 2.62$        | $ 1.70$        | $569.00$ |
| Radio        | $ 1.62$ | $ 0.84$       | $-0.40$      | $57.25$   | $ 4.98$        | $ 5.10$        | $333.00$ |
| Shed         | $ 1.22$ | $-0.14$       | $-0.22$      | $48.81$   | $37.57$        | $ 6.81$        | $53.00$  |
| Stoves       | $ 1.44$ | $ 0.34$       | $ 0.31$      | $20.32$   | $18.19^{**}$   | $ 8.31$        | $84.00$  |
| Potspans     | $ 4.46$ | $-0.06$       | $-0.23$      | $102.73$  | $15.90$        | $-15.40$       | $582.00$ |
| Ducks        | $ 5.72$ | $ 4.26^{***}$ | $-0.16$      | $230.93$  | $109.99^{***}$ | $-19.34$       | $223.00$ |
| Motorcycle   | $ 1.51$ | $-0.48$       | $ 0.12$      | $2288.48$ | $300.46$       | $-196.32$      | $66.00$  |
| Chairtables  | $ 5.02$ | $ 0.25$       | $ 0.39$      | $167.62$  | $19.00$        | $-24.73$       | $638.00$ |
| Net          | $ 3.07$ | $ 0.03$       | $-0.08$      | $24.49$   | $ 0.66$        | $-3.81$        | $382.00$ |
| Axes         | $ 1.02$ | $ 0.03$       | $-0.02$      | $17.74$   | $ 0.02$        | $-3.94^{**}$   | $218.00$ |
| Smallanimals | $ 3.39$ | $ 0.29$       | $-0.90$      | $767.26$  | $-151.35$      | $-311.05^{**}$ | $155.00$ |
| Charcoal     | $ 2.20$ | $-0.26$       | $-0.83$      | $35.81$   | $-1.43$        | $-4.65$        | $176.00$ |
| Bicycle      | $ 6.34$ | $-5.46$       | $-5.52$      | $272.90$  | $-31.50$       | $-42.67$       | $135.00$ |
| Bed          | $ 3.17$ | $-0.23$       | $-0.40$      | $300.64$  | $19.32$        | $-57.78^{*}$   | $628.00$ |
| Tv           | $ 1.48$ | $-0.36$       | $-0.26$      | $380.45$  | $121.95$       | $348.23^{**}$  | $45.00$  |
|--------------+---------+---------------+--------------+-----------+----------------+----------------+----------|


* COMMENT Code appendix
  
** Food Security

 #+name: foodsecure_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
 D = full_data(balance=[])

 codes = {"1-2 times a week": 3,
         "3-6 times a week": 2,
         "Everyday": 1,
         "everyday": 1,
         "Less than once a week": 4,
         "less than once a week": 4,
         "Never": 5,
         "never": 5}

 recode = lambda x: codes.setdefault(x,x)

 Aval2013 = D.filter(regex="^fs_.*_b").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Aval2014 = D.filter(regex="^fs_.*_m").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Aval2015 = D.filter(regex="^fs_.*_e").rename(columns=lambda x: x[3:-2]).applymap(recode)

 index_vars = "worried,portions,fewmeals,nofood,hungry,wholeday".split(",")
 Outcomes = index_vars+["z-score"]
 #~ Creates Year dummies, z-scores and baseline values as `var'2013
 for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
     Aval["Year"]=Year
     for var in index_vars:
        Aval[index_vars] = (Aval[index_vars]-Aval[index_vars].mean())/Aval[index_vars].std()
     FS_sum = Aval[index_vars].sum(axis=1)
     Aval["z-score"] = (FS_sum-FS_sum.mean())/FS_sum.std()
     for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
    
 Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
 Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
 Vals = Vals.join(D[["TUP","CSH"]])

 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

 Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 Vals=Vals.loc[2014:2015]
 regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = Vals["TUP"]+Vals["CSH"] ==0
 CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+end_src

** Consumption

 #+name: consumption_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py

 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 from TUP import full_data, consumption_data, regressions, reg_table, df_to_orgtbl

 food = ['c_cereals', 'c_maize', 'c_sorghum', 'c_millet', 'c_potato', 'c_sweetpotato', 'c_rice', 'c_bread', 'c_beans', 'c_oil', 'c_salt', 'c_sugar', 'c_meat', 'c_livestock', 'c_poultry', 'c_fish', 'c_egg', 'c_nuts', 'c_milk', 'c_vegetables', 'c_fruit', 'c_tea', 'c_spices', 'c_alcohol', 'c_otherfood']
 month = ['c_fuel', 'c_medicine', 'c_airtime', 'c_cosmetics', 'c_soap', 'c_transport', 'c_entertainment', 'c_childcare', 'c_tobacco', 'c_batteries', 'c_church', 'c_othermonth']    
 year = ['c_clothesfootwear', 'c_womensclothes', 'c_childrensclothes', 'c_shoes', 'c_homeimprovement', 'c_utensils', 'c_furniture', 'c_textiles', 'c_ceremonies', 'c_funerals', 'c_charities', 'c_dowry', 'c_other']    
 normalize = {3:food, 30:month, 360:year}

 D = full_data(normalize=normalize,balance=[])

 C, HH, T = consumption_data(D, how="long")
 C = C.join(T, how="left")
 Outcomes = ["Tot", "FoodShr", "Food",  "logTot", "Month", "Year"]

 #$\approx$ Make aggregate variables
 for Year,suffix in ( ("2013","_b"), ("2014","_m"), ("2015","_e") ):
     C["Food"]   = C[[item for item in food  if item in C]].sum(axis=1).replace(0,np.nan)
     C["Month"]  = C[[item for item in month if item in C]].sum(axis=1).replace(0,np.nan)
     C["Year"]   = C[[item for item in year  if item in C]].sum(axis=1).replace(0,np.nan)
     C["Tot"]    = C[["Food","Month","Year"]].sum(axis=1).replace(0,np.nan)
     C["FoodShr"]= C["Food"]/C["Tot"] #$\approx$ FoodShare variable
     C["logTot"] = C["Tot"].apply(np.log)
     try: #~ THIS NEEDS TO BE FIXED TO CALL TUP.py the way neediness.org does...
         lmdas = pd.read_pickle("../data/modified/ss-loglambdas_b{}.df".format(suffix))
         C["$\log\lambda$"+Year] = lmdas["lambda"]
     except IOError:
         if not Year =="2013": print("ss-loglambdas_b{} doesn't exist. Run goods_analysis_2015.py".format(suffix))


 #$\approx$ Make Baseline variable
 for var in Outcomes: 
     Bl = C.loc[2013,var]
     C = C.join(Bl,rsuffix="2013", how="left")


 C["Y"]=np.nan
 for yr in (2013, 2014, 2015): C.loc[yr,"Y"]=str(int(yr))

 C = C.join(pd.get_dummies(C["Y"]), how="left")
 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         interaction = C[group]*C[year]
         if interaction.sum()>0: C["{}*{}".format(group,year)] = interaction

 Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']
 C = C.loc[2014:2015]
 #$\approx$ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 regs = regressions(C, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)
 #$\approx$ regs = {var: sm.OLS(C[var], C[Controls], missing='drop').fit() for var in Outcomes}

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = C["TUP"]+C["CSH"] ==0
 CTLmean = {var: C[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: C[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+END_SRC
 
** Assets

 #+name: asset_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle Endline_analysis.py
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 from matplotlib import pyplot as plt
 from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
 D = full_data(balance=[])

 Outcomes = ["Total", "Productive"]
 Aval2013 = asset_vars(D,year=2013)[0][Outcomes]
 Aval2014 = asset_vars(D,year=2014)[0][Outcomes]
 Aval2015 = asset_vars(D,year=2015)[0][Outcomes]

 #$\approx$ Creates Year dummies and baseline values as `var'2013
 for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
     Aval["Year"]=Year
     for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
 Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
 Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
 Vals = Vals.join(D[["TUP","CSH"]])
 Vals["CTL"] = (Vals["TUP"]+Vals["CSH"] ==0).apply(int)

 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

 #~ Make graph of distribution
 stringify = lambda var: Vals[var].apply(lambda x: var if x else "")
 Vals["Group"] = stringify("TUP")+stringify("CSH")+stringify("CTL")
 for var in ("Total","Productive"):
    fig,ax = plt.subplots(1,2)
    for i,yr in enumerate((2014,2015)):
        Vals.ix[yr].dropna(subset=[[var,"TUP","CSH","CTL"]]).groupby("Group")[var].plot(kind="kde",ax=ax[i])
        ax[i].set_title("{} Asset Value in {}".format(var,yr))
        ax[i].legend()
        #~ ax[i].set_aspect(1)
        ax[i].set_xlim(left=0)
    plt.savefig("figures/Asset{}_kde.png".format(var))
    plt.clf()

 Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

 #$\approx$ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 Vals=Vals.loc[2014:2015]
 regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = Vals["TUP"]+Vals["CSH"] ==0
 CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+end_src

#+name: assets_disaggreate_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
import numpy as np
import pandas as pd
from TUP import full_data, regressions, asset_vars, reg_table, df_to_orgtbl

D = full_data(balance=[])
D["cons"] = 1.
Count = D.filter(regex="^asset_n_").rename(columns=lambda col: col[8:])
Vals = D.filter(regex="^asset_val_").rename(columns=lambda col: col[10:])
#~ 1 if coded as >0, 0 if zero, keep NaN's missing, since those are only from missing surveys.
Nonzero = Count.applymap(lambda i: float(i>0) if not np.isnan(i) else np.nan)
 #~ Restrict both df's to nonzero responses.
Count = Count.replace(0,np.nan)
Vals  =  Vals.replace(0,np.nan)
too_many_zeros = 30
many_zeros = [item for item in Vals if Vals[item].notnull().sum()<too_many_zeros]
for df in (Nonzero, Count, Vals): df.drop(many_zeros,1, inplace=True)

#~ Merge in Control Vars
controls = ["cons","TUP","Cash"]
Nonzero  = Nonzero.join(D[controls],how="left")
Count    =   Count.join(D[controls],how="left")
Vals     =    Vals.join(D[controls],how="left")

Items = [item[:-2] for item in Vals if item.endswith("_e")]
CTL = Vals[controls].sum(axis=1)==1 #~ i.e. only constant ==1, TUP & Cash ==0
Zeros_ctl_mean = Nonzero.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Count_ctl_mean =   Count.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Vals_ctl_mean  =    Vals.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])

ZeroTable       = reg_table(regressions(Nonzero,"_e", outcomes=Items, controls = ["cons",'Cash','TUP'], Baseline="_b"), orgtbl=True, Transpose=True)
Count, Count_se = reg_table(regressions(Count,"_e",   outcomes=Items, controls = ["cons",'Cash','TUP'], Baseline="_b"), resultdf=True, Transpose=True)
Vals, Vals_se   = reg_table(regressions(Vals,"_e",    outcomes=Items, controls = ["cons",'Cash','TUP'], Baseline="_b"), resultdf=True, Transpose=True)

#~ Make full table of Standard errors-- MAKE SURE YOU HAVE THE SUFFIXES RIGHT.
SE = Count_se[["TUP","Cash"]].join(Vals_se[["TUP","Cash"]], rsuffix=" (SSP)", lsuffix=" (# own)")

#~ Make full table of point estimates and control-group means
FullTable = pd.DataFrame({"# own (CTL)":Count_ctl_mean, "Value (CTL)":Vals_ctl_mean}).join(Count[["TUP","Cash"]])
FullTable = FullTable.join(Vals[["TUP","Cash","N"]], rsuffix=" (SSP)", lsuffix=" (# own)")
FullTable = df_to_orgtbl(FullTable, sedf=SE)
AllTables = FullTable+"\n\n"+ZeroTable
return AllTables
#+end_src

** Savings

#+name: savings_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl

# Top-Code or censor outliers?
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

D = full_data(balance=[])

Sav = pd.DataFrame(index=D.index) #~ Set up empty DataFrame to fill

years = [("_b",2013), ("_m",2014), ("_e", 2015)]
for suff,year in years: #~ Make Aggregate savings and land holding variables
    Sav["Savings{}".format(year)]  = D.filter(regex="^savings_(home|bank|BRAC|NGOs|other){}".format(suff)).sum(1)
    Sav["Food Sav{}".format(year)] = D.filter(regex="^savings_(maize|sorghum|otherfood)_val{}".format(suff)).sum(1)

    Sav["LandCult{}".format(year)] = D.filter(regex="^land_(owncult|rentcult|communitycult){}".format(suff)).sum(1)
    Sav["LandOwn{}".format(year)]  = D.filter(regex="^land_own.*{}".format(suff)).sum(1)

    Sav["Get Trans{}".format(year)]  = D.filter(regex="^transfers_get.*{}".format(suff)).sum(1)
    Sav["Give Trans{}".format(year)] = D.filter(regex="^transfers_give.*{}".format(suff)).sum(1)

Outcomes = ["Savings","Food Sav","LandCult","LandOwn", "Get Trans", "Give Trans"] #~ Loans give/received omitted

by_year = []
for yr in ("2013","2014","2015"): #~ Provide Baseline Values & put in long format
    S_Year = Sav.filter(like=yr).rename(columns=lambda x:x[:-4])
    for var in Outcomes: 
        if var+"2013" in Sav: S_Year[var+"2013"] = Sav[var+"2013"]
    #~ Note that adding "Year" has to come after topcode, which switches strings to Nan...
    S_Year["Year"] = yr
    by_year.append(S_Year)

#~ In long format with Year,HH index
Sav = pd.concat(by_year).reset_index().set_index(["Year", "HH"], drop=False)
#~ Make Year Dummies for fixed effects


Sav = Sav.join(pd.get_dummies(Sav["Year"]).rename(columns=lambda col: str(int(col))), how="left")
Sav = Sav.drop(["HH","Year"],1)
Sav = Sav.join(D[["TUP","CSH"]])

for group in ("TUP", "CSH"): #~ Make Treatment-by-year interactions
    for year in ("2013", "2014", "2015"):
        Sav["{}*{}".format(group,year)] = Sav[group]*Sav[year]

def isPositive(i):
    #~ Returns 1 if number is positive, 0 if number<=0, nan if already nan, and self if string.
    #~ Note that it's safe to run dummy variables through.
    try:
        if np.isnan(i): return i
        else: return float(i>0)
    except TypeError: return i

too_many_null = 30
Nonzero = Sav.applymap(isPositive)

#~ Naturally, only do this after Creating Nonzero dataframe.
for var in Outcomes: #~ Set zeros to missing And topcode among non-zero values
    for outcome in (var,var+"2013"):
       if outcome in Sav:
           Sav[outcome] = Sav[outcome].replace(0,np.nan)
           Sav[outcome] = Sav.groupby(level="Year")[outcome].apply(topcode) #~ (Untested)

many_null = [item for item in Sav if Sav[item].notnull().sum()<too_many_null]
Sav = Sav.drop(many_null,1).copy()

Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

Sav = Sav.loc["2014":"2015"]
Nonzero = Nonzero.loc["2014":"2015"]
Sav_regs = regressions(Sav,     outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)
Zer_regs = regressions(Nonzero, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

Sav_results, Sav_SE  = reg_table(Sav_regs,  resultdf=True,table_info=["N","F-stat"])
Zer_results, Zer_SE  = reg_table(Zer_regs,  resultdf=True,table_info=["N","F-stat"])

CTL = Sav["TUP"]+Sav["CSH"] ==0
#~ Get control group means and standard deviations
Sav_CTLmean = {var: Sav[CTL].loc["2015",var].mean() for var in Outcomes}
Zer_CTLmean = {var: Nonzero[CTL].loc["2015",var].mean() for var in Outcomes}
Sav_CTLsd = {var: Sav[CTL].loc["2015",var].std() for var in Outcomes}
Zer_CTLsd = {var: Nonzero[CTL].loc["2015",var].std() for var in Outcomes}
Sav_diff, Sav_diff_se = pd.DataFrame(Sav_CTLmean,index=["CTL mean"]), pd.DataFrame(Sav_CTLsd,index=["CTL mean"])
Zer_diff, Zer_diff_se = pd.DataFrame(Zer_CTLmean,index=["CTL mean"]), pd.DataFrame(Zer_CTLsd,index=["CTL mean"])

for var in Outcomes:
    #~ Savings regressions first
    ttest1= Sav_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Sav_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Sav_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Sav_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Sav_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Sav_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

    #~ Nonzero regressions second
    ttest1= Zer_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Zer_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Zer_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Zer_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Zer_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Zer_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


Land = ["LandCult","LandOwn"] 
Savings = ["Savings","Food Sav", "Get Trans", "Give Trans"] 

Save_results = Sav_results.append(Sav_diff)
Zero_results = Zer_results.append(Zer_diff)
Save_SE = Sav_SE.append(Sav_diff_se)
Zero_SE = Zer_SE.append(Zer_diff_se)

#~ Land_results = Sav_results[Land]
#~ zLan_results = Zer_results[Land]
#~ Land_SE = Sav_SE[Land]
#~ zLan_SE = Zer_SE[Land]
#~ 
#~ Sav_results = Sav_results[Savings]
#~ Zer_results = Zer_results[Savings]
#~ Sav_SE =           Sav_SE[Savings]
#~ Zer_SE =           Zer_SE[Savings]

Save_tab = df_to_orgtbl(Save_results, sedf=Sav_SE)
Zero_tab = df_to_orgtbl(Zero_results, sedf=Zer_SE)

Table = Zero_tab +"\n"+ Save_tab
#+end_src

** Income

#+name: income_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
from pandas.io import stata
import statsmodels.api as sm
from matplotlib import pyplot as plt
from TUP import full_data, regressions, reg_table, df_to_orgtbl
"""
Note that topcoding has a large effect on the distribution here, and we see only a small (presumably non-random) portion of actual income for each household.
"""

# Top-Code or censor outliers?
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

D = full_data(balance=[])
keep = D.index

I_file = '../data/Endline/sections_8_17.dta'
I = stata.read_stata(I_file).rename(columns={"id":"HH"}).set_index("HH", drop=True).ix[keep]

#~Getting non-agriculture income data is easy
I = I.filter(regex="^s16")
Imonths    = I.filter(regex="s16_\dc").rename(columns=lambda x: x[:-1])
Ipermonth  = I.filter(regex="s16_\dd").rename(columns=lambda x: x[:-1])
Income_12m = Imonths.mul(Ipermonth).sum(axis=1)
Iyear      = I.filter(regex="s16_\de").rename(columns=lambda x: x[:-1]).sum(axis=1)

A_file = "../data/Endline/Agriculture_cleaned.dta"
A = stata.read_stata(A_file).rename(columns={"id":"HH"}).set_index("HH",drop=False).ix[keep]
unit_prices = A.groupby(["harvest_type", "harvest_price_unit"])["harvest_price"].median()
prices = unit_prices.loc[zip(A["harvest_type"],A["harvest_price_unit"])]
A["price"]=list(prices)

A["harvest_unit_match"] = A["harvest_price_unit"] == A["harvest_unit"]
A["price"] = A["harvest_unit_match"]*A["harvest_price"] + (1-A["harvest_unit_match"])*A["price"]

A["income_farm_year"] = A["harvest_size"]*A["price"]
Ayear = A.groupby("HH")["income_farm_year"].sum()

unit_prices = A.groupby(["livestock_type", "livestock_price_unit"])["livestock_price"].median()
prices = unit_prices.loc[zip(A["livestock_type"],A["livestock_price_unit"])]
A["price"]=list(prices)
A["livestock_unit_match"] = A["livestock_price_unit"] == A["livestock_unit"]
A["price"] = A["livestock_unit_match"]*A["livestock_price"] + (1-A["livestock_unit_match"])*A["price"]

A["income_livestock_year"] = A["livestock_size"]*A["price"]
Lyear = A.groupby("HH")["income_livestock_year"].sum()

Outcomes = ["Total", "Non-Farm", "Farm",  "Livestock"]
Controls = ["cons", "TUP","CSH"]
Vals = pd.DataFrame({"Non-Farm": Income_12m, "Farm":Ayear, "Livestock":Lyear})
Vals = Vals.apply(topcode)

Vals["Total"] = Vals.sum(axis=1)
Vals["cons"] = 1.

Vals = Vals.join(D[["TUP","CSH"]])
Vals["CTL"] = (Vals["TUP"]+Vals["CSH"] ==0).apply(int)

#~ Make graph of distribution
stringify = lambda var: Vals[var].apply(lambda x: var if x else "")
Vals["Group"] = stringify("TUP")+stringify("CSH")+stringify("CTL")
Vals.dropna(subset=[["Total","TUP","CSH","CTL"]]).groupby("Group")["Total"].plot(kind="kde")
plt.title("Total Income Distribution by Group")
plt.savefig("figures/IncomeDistribution.png")

regs = {var: sm.OLS(Vals[var], Vals[Controls], missing="drop").fit() for var in Outcomes}
results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

CTL = Vals["CTL"] 
CTLmean = {var: Vals.ix[CTL][var].mean() for var in Outcomes}
CTLsd = {var: Vals[CTL].std() for var in Outcomes}
diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

for var in Outcomes:
    ttest1= regs[var].t_test("TUP - CSH = 0").summary_frame()

    diff.loc[   r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest1["coef"][0]
    diff_se.loc[r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest1["std err"][0]

results = results.append(diff)
SE = SE.append(diff_se)

tab = df_to_orgtbl(results, sedf=SE)

#+end_src


** Conflict Exposure
#+name: conflict_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl

# Top-Code or censor outliers?
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

#~ Read in data
D = full_data(balance=[])
D = D[D.merge_midline != 1]
C = D.filter(like="conflict").rename(columns = lambda x: x[:-2]) #~ Set up empty DataFrame to fill

#~ Make Outcome variables
#~ NOTE: Not looking at whether they protected assets as only 50 #~ said they did, and all said "Migrated to stay with other family"
C["Bsln_NaN"] = D["merge_midline"] == 2
C["Worried"]  = C["conflict_worried"]
C["Affected"] = C["conflict_affected"]
protect_lives_codes = lambda x: {"Nothing": 0, "Migrate to stay with friend/family": 1, "Migrated and found new accommodation": 1,
                                 "Looked for protection with Govt. Military": 2, "Looked for protection with NGO": 2}.get(x)
C["ProtectLives"] = C["conflict_protectlives"].apply(protect_lives_codes)
C["Migrated"] = (C["ProtectLives"]==1) + \
                (C["conflict_affected1"]=="Needed to elocate or migrate")
C["NoMeans"] = C["conflict_whynotprotect"]=="Didn't have the means"
C["NoInvest"]= C.filter(like="affected").applymap(lambda x: x=="Could not plant crop or invest in business").sum(axis=1)
C = C.drop([var for var in C if var.startswith("conflict_")], 1)
Outcomes = ["ProtectLives", "Worried", "Affected", "Migrated", "NoMeans", "NoInvest"]

#~ Bring in Treatment variables
C["TUP"] = D["TUP"]
C["cons"] = 1.
C = C.applymap(lambda x: int(x) if not np.isnan(x) else x)
Controls = ["cons", "TUP"] #~, "Bsln_NaN"]

C_regs = regressions(C, outcomes=Outcomes, controls=Controls, Baseline=False, baseline_na=False)
C_results, C_SE  = reg_table(C_regs,  resultdf=True,table_info=["N","F-stat"])

#~ Get control group means and standard deviations and add to regression table
CTLmean = {var: C.query("TUP==0")[var].mean() for var in Outcomes}
CTLsdv  = {var: C.query("TUP==0")[var].std()  for var in Outcomes}
CTLmean, CTLsdv = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsdv,index=["CTL mean"])
C_results = C_results.append(CTLmean)
C_SE      = C_SE.append(CTLsdv)

Table = df_to_orgtbl(C_results, sedf=C_SE)

#+end_src



** Confidence & Autonomy

#+name: autonomy_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl

D = full_data(balance=[])
E = D.filter(regex="^(access|decide|conf|gone)")
years = {"_b":2013, "_m":2014, "_e": 2015}
E = E.rename(columns = lambda x: x[:-2]+years[x[-2:]])

print "THIS IS UNFINISHED..."

by_year = []
for yr in ("2013","2014","2015"): #~ Provide Baseline Values & put in long format
    E_Year = E.filter(like=yr).rename(columns=lambda x:x[:-4])
    for var in Outcomes: 
        if var+"2013" in Sav: S_Year[var+"2013"] = Sav[var+"2013"]
    #~ Note that adding "Year" has to come after topcode, which switches strings to Nan...
    S_Year["Year"] = yr
    by_year.append(S_Year)

Sav = pd.DataFrame(index=D.index) #~ Set up empty DataFrame to fill

for suff,year in years: #~ Make Aggregate savings and land holding variables
    E["Savings{}".format(year)]  = D.filter(regex="^savings_(home|bank|BRAC|NGOs|other){}".format(suff)).sum(1)
    Sav["Food Sav{}".format(year)] = D.filter(regex="^savings_(maize|sorghum|otherfood)_val{}".format(suff)).sum(1)

    Sav["LandCult{}".format(year)] = D.filter(regex="^land_(owncult|rentcult|communitycult){}".format(suff)).sum(1)
    Sav["LandOwn{}".format(year)]  = D.filter(regex="^land_own.*{}".format(suff)).sum(1)

    Sav["Get Trans{}".format(year)]  = D.filter(regex="^transfers_get.*{}".format(suff)).sum(1)
    Sav["Give Trans{}".format(year)] = D.filter(regex="^transfers_give.*{}".format(suff)).sum(1)



#~ In long format with Year,HH index
Sav = pd.concat(by_year).reset_index().set_index(["Year", "HH"], drop=False)
#~ Make Year Dummies for fixed effects


Sav = Sav.join(pd.get_dummies(Sav["Year"]).rename(columns=lambda col: str(int(col))), how="left")
Sav = Sav.drop(["HH","Year"],1)
Sav = Sav.join(D[["TUP","CSH"]])

for group in ("TUP", "CSH"): #~ Make Treatment-by-year interactions
    for year in ("2013", "2014", "2015"):
        Sav["{}*{}".format(group,year)] = Sav[group]*Sav[year]

def isPositive(i):
    #~ Returns 1 if number is positive, 0 if number<=0, nan if already nan, and self if string.
    #~ Note that it's safe to run dummy variables through.
    try:
        if np.isnan(i): return i
        else: return float(i>0)
    except TypeError: return i

too_many_null = 30
Nonzero = Sav.applymap(isPositive)

#~ Naturally, only do this after Creating Nonzero dataframe.
for var in Outcomes: #~ Set zeros to missing And topcode among non-zero values
    for outcome in (var,var+"2013"):
       if outcome in Sav:
           Sav[outcome] = Sav[outcome].replace(0,np.nan)
           Sav[outcome] = Sav.groupby(level="Year")[outcome].apply(topcode) #~ (Untested)

many_null = [item for item in Sav if Sav[item].notnull().sum()<too_many_null]
Sav = Sav.drop(many_null,1).copy()

Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

Sav = Sav.loc["2014":"2015"]
Nonzero = Nonzero.loc["2014":"2015"]
Sav_regs = regressions(Sav,     outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)
Zer_regs = regressions(Nonzero, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

Sav_results, Sav_SE  = reg_table(Sav_regs,  resultdf=True,table_info=["N","F-stat"])
Zer_results, Zer_SE  = reg_table(Zer_regs,  resultdf=True,table_info=["N","F-stat"])

CTL = Sav["TUP"]+Sav["CSH"] ==0
#~ Get control group means and standard deviations
Sav_CTLmean = {var: Sav[CTL].loc["2015",var].mean() for var in Outcomes}
Zer_CTLmean = {var: Nonzero[CTL].loc["2015",var].mean() for var in Outcomes}
Sav_CTLsd = {var: Sav[CTL].loc["2015",var].std() for var in Outcomes}
Zer_CTLsd = {var: Nonzero[CTL].loc["2015",var].std() for var in Outcomes}
Sav_diff, Sav_diff_se = pd.DataFrame(Sav_CTLmean,index=["CTL mean"]), pd.DataFrame(Sav_CTLsd,index=["CTL mean"])
Zer_diff, Zer_diff_se = pd.DataFrame(Zer_CTLmean,index=["CTL mean"]), pd.DataFrame(Zer_CTLsd,index=["CTL mean"])

for var in Outcomes:
    #~ Savings regressions first
    ttest1= Sav_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Sav_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Sav_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Sav_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Sav_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Sav_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

    #~ Nonzero regressions second
    ttest1= Zer_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Zer_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Zer_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Zer_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Zer_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Zer_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


Land = ["LandCult","LandOwn"] 
Savings = ["Savings","Food Sav", "Get Trans", "Give Trans"] 

Save_results = Sav_results.append(Sav_diff)
Zero_results = Zer_results.append(Zer_diff)
Save_SE = Sav_SE.append(Sav_diff_se)
Zero_SE = Zer_SE.append(Zer_diff_se)

#~ Land_results = Sav_results[Land]
#~ zLan_results = Zer_results[Land]
#~ Land_SE = Sav_SE[Land]
#~ zLan_SE = Zer_SE[Land]
#~ 
#~ Sav_results = Sav_results[Savings]
#~ Zer_results = Zer_results[Savings]
#~ Sav_SE =           Sav_SE[Savings]
#~ Zer_SE =           Zer_SE[Savings]

Save_tab = df_to_orgtbl(Save_results, sedf=Sav_SE)
Zero_tab = df_to_orgtbl(Zero_results, sedf=Zer_SE)

Table = Zero_tab +"\n"+ Save_tab
#+end_src





** Extras
#+name: get_loglambdas
#+BEGIN_SRC python :noweb no-export :results silent

"""
This code does not yet work in this file. Come back to it...
"""
  df = TUP.process_data(C, HH, T, year = year) #~ Process_data() builds consumption data if not given as an argument
  df['Constant']=1
  df["CTL"] = 1-df["TUP"] #~ Code the cash group as controls since they're not in the midline analysis

  explist=[s[2:-2] for s in df.columns[[s.startswith('c_') and s.endswith(year[1]) for s in df.columns]]]
  df = df.rename(columns= lambda x: x[:-2] if x.endswith(year[1]) else x)

  bothdf=[]
  xvars=['hh_size_b','child_total_b','Loc']
  for x in explist:
      if 'c_'+x+r'_b' not in df:
          #~ When you take out the baseline controls in favor of repeated cross-sections, this is where to start...
          print(x+" has no baseline data or had too few non-zero responses at baseline. Skipping.")
          continue
      ydf=pd.DataFrame(df[['c_'+x]].rename(columns={'c_'+x:x.capitalize()}).stack())
      rdict=dict(zip(xvars+['c_'+x+r'_b'],["%s_%s" % (s,x.capitalize()) for s in xvars]+['Baseline_%s' % x.capitalize()]))
      xdf=pd.DataFrame(df[xvars+['c_'+x+r'_b']])
      xdf.index=pd.MultiIndex.from_tuples([(i,x.capitalize()) for i in xdf.index])
      locations=pd.get_dummies(xdf['Loc'],prefix='Loc_%s' % x.capitalize())
      del xdf['Loc']
      xdf.rename(columns=rdict,inplace=True)
      xdf=xdf.join(locations)
      xdf.replace(to_replace=np.NaN,value=0,inplace=True)

      # Add row to restrict location dummies to sum to one
      ydf=pd.concat([ydf,pd.DataFrame([0],index=[(0,x.capitalize())])])
      xdf=pd.concat([xdf,pd.DataFrame([s.startswith('Loc_')+0. for s in xdf.columns],index=xdf.columns,columns=[(0,x.capitalize())]).T]) 

      xdf[0]=ydf
      xdf.dropna(how='any',inplace=True)
      bothdf.append(xdf)

  #~ Are this fillna() call and the xdf.replace call above a problem? It seems necessary for the block-diagonal ols function
  #~ we're using, but aren't we coding zeros as missing and calculating residuals for only those positive consumption? Wouldn't
  #~ replacing them to zero insert some non-zero residual for households that never consume a given good?
  #~ And isn't this the motivation behind svd_missing?
  mydf=pd.concat(bothdf).fillna(value=0)

  X=mydf.iloc[:,1:]

  y=mydf[[0]]

  x=np.exp(y.unstack().iloc[1:,:]) # Expenditures (in levels)
  xshares=x.divide(x.sum(axis=1),axis=0).fillna(value=0).mean() # Expenditure shares (taking missing as zero)
  xshares.index=xshares.index.droplevel(0)

  b,se=ols(X,y)

  ## betahat=b[['Constant_%s' % s.capitalize() for s in explist]]
  ## betahat.rename(columns=dict(zip(betahat.columns,[s.capitalize() for s in explist])),inplace=True)

  e=y-X.dot(b.T)

  e.rename(columns={0:'Resid'},inplace=True)
  e.index.names=['HH','Good']

  testdf=pd.merge(df[['TUP','CTL']].reset_index(),e.reset_index(),how='outer',on=['HH'])
  testdf.set_index(['HH','Good'],inplace=True)

  TUP=testdf['TUP'].mul(testdf['Resid']).dropna().unstack()
  CTL=testdf['CTL'].mul(testdf['Resid']).dropna().unstack()

  e=(e-e.mean()).unstack()

  # Test of significant differences between treatment and control:
  # Weighting matrix:
  A=np.matrix((TUP-CTL).cov().as_matrix()).I
  g=np.matrix((TUP-CTL).mean())
  J=e.shape[0]*g*A*g.T # Chi2 statistic

  p=1-chi2.cdf(J,e.shape[1])

  chi2test="Chi2 test: %f (%f)" % (J,p)

  N=pd.Series([d.shape[0]-1 for d in bothdf],index=[d.index.levels[1][0] for d in bothdf])

  resultdf=pd.DataFrame({'TUP':TUP.mean(),'CTL':CTL.mean(),'$N$':N})
  sedf=pd.DataFrame({'TUP':TUP.std()/np.sqrt(resultdf['$N$']),'CTL':CTL.std()/np.sqrt(resultdf['$N$'])})
  resultdf['Diff.']=resultdf['TUP']-resultdf['CTL']
  sedf['Diff.']=np.sqrt((sedf['TUP']**2) + (sedf['CTL']**2))

  # Use svd (with missing data) to construct beta & log lambda

  myb,myl = get_loglambdas(e,TEST=True)

  myb.index=myb.index.droplevel(0)

  # Normalize log lambdas
  l=myl/myl.std()
#+END_SRC


#+name: residuals_by_group
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
def residuals_by_group(models, groups, outcomes=[], kind="kde", figure_dir="./figures/", seriesname="Treat", blanks_to = "Control"):
    """
     Takes a set of statsmodels regression results and,
     for each outcome, produces a plot comparing the
     distribution of residuals by group.

     models:
         A dictionary of the form {variable name: sm.RegressionResults}. Empty defaults to all available.
     groups:
         A list, pd.Series, or pd.DataFrame with variables
         (A later version could contain an arbitrary set of categorical and give groups for every combination.)
     outcomes:
         A list specifying which variables in models to make plots for.
     kind:
         kde (or density) or histogram (or hist)
         Density plots are on a single axis. Histograms are stacked by group.
     figure_dir:
         The directory into which the figures get saved. If doesn't exist, throws error (future version might make that directory on the fly.)
     Seriesname:
         If a series or list is passed without a name, defaults to `seriesname'
     blanks_to:
         Observations with no treatment status from "groups" gets renamed to `blanks_to'
    """

    #~ Make outcomes a list. If empty, defaults to all variables in models
    if type(outcomes)==str: outcomes=[outcomes]
    if not outcomes: outcomes = sorted(models.keys())

    #~ Make data frame and make "Group" categorical
    df = pd.DataFrame(groups).rename(columns={0:seriesname})
    for var in df: df[var] = df[var].applymap(lambda x: var if x else "")
    df["Group"] = df.sum(axis=1).replace("",blanks_to)

    #~ Make residuals
    for var in outcomes:
        #~ Make column of residual values
        resid_var = "resid_{}".format(var)
        df[resid_var] = models[var].resid
        #~ Groupby object
        groups = df.dropna(subset=[resid_var]).groupby("Group")[resid_var]

        #~ Plot density by group
        if kind in ("kde", "density"):
            fig, ax = plt.subplots()
            groups.plot(kind=kind, ax=ax, legend=True)
            fig.savefig(figure_dir+resid_var+".png")

        #~ Plot histograms by group
        elif kind in ("hist", "histogram"):
            i=0
            fig, ax = plt.subplots(len(set(df["Group"])),1,sharex=True)
            for group, data in grps[var]:
                ax[i].hist(data.values, bins=20)
                ax[i].set_title(group)
                i+=1
            i=0
            fig.savefig(figure_dir+resid_var+".png")
        print(resid_var+".png created.")


#+end_src
