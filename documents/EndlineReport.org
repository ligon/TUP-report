:SETUP:
#+TITLE: Comparing Cash and Asset Transfers to Low-Income Households in South Sudan
#+AUTHOR: Reajul Chowdhury, Elliott Collins, Ethan Ligon, Munshi Sulaiman
#+DATE: \today
#+OPTIONS: texht:t toc:nil inline:nil todo:nil
#+OPTIONS: title:nil 
#+LATEX_CLASS_OPTIONS: [12pt,letterpaper]
#+LATEX_HEADER: \address{University of California, Berkeley}
#+LATEX_HEADER:       \newcommand{\Eq}[1]{(\ref{eq:#1})}
#+LATEX_HEADER_EXTRA: \newcommand{\Fig}[1]{Figure \ref{fig:#1}} \newcommand{\Tab}[1]{Table \ref{tab:#1}}
#+LATEX_HEADER_EXTRA: \usepackage{stringstrings} \renewcommand{\cite}[1]{\caselower[q]{#1}\citet{\thestring}}
#+LaTeX_HEADER:       \usepackage[T1]{fontenc}
#+PROPERTY: header-args:python :results output raw table :noweb no-export :exports none :prologue "# -*- coding: utf-8 -*-"
:END:


* Title Page                                                         :ignore:
#+BEGIN_export latex
\begin{titlepage}
\title[Cash and Asset Transfers]{Comparing Cash and Asset Transfers to Low-Income Households in South Sudan}

\date{\today}

\begin{abstract}
Several previous studies have found that the ``graduation'' or
``Transfers to the Ultra-Poor'' (TUP) framework is an effective
approach to alleviating the constraints that prevent extremely poor
households from increasing their productivity.  The framework consists
of a sizable transfer of productive physical capital, coupled with
training and continuous support over the course of one or two years.
A second and related literature has found some evidence that
unconditional cash transfers (UCT's) may also improve household
productivity and welfare with fewer fixed costs.  Our field experiment
provides a comparison of these two approaches to transferring wealth to
low-income households during the first two years of BRAC's TUP pilot
in South Sudan.  We consider the effect of each on consumption,
income, asset holdings, and a number of intangible outcomes. We also
consider the TUP program's effect on households' responses to the
outbreak of violence in 2014. We find evidence that both types of
transfer have positive effects on consumption, but only in the
short-run.  We find a persistent increase in asset stocks, but only from
the TUP. We also elicit suggestive evidence that BRAC's support may
have helped TUP beneficiaries cope with the short-term economic
effects of the outbreak of violence in 2014. We tentatively conclude
that in this economic context cash can increase household consumption,
but the goal of improving income or wealth is aided by the additional
services that the ultra-poor graduation framework offer.
\end{abstract}

\maketitle
\thispagestyle{empty}

\begin{center}
  ligon@berkeley.edu\\
  University of California\\
  Berkeley, CA 94720--3310
\end{center}

\end{titlepage}
#+END_export


* Introduction
 
Poor rural households often earn money from low-return activities such
as small-scale cultivation or casual day labor, and face both
financial and human capital constraints, which may prevent them from
investing and expanding into more lucrative activities. Experience and
research over many years has led many to believe that households
facing particularly acute poverty are unable to solve this problem
through the small, high-interest loans typically marketed to them.  It
was these considerations that led to the development of the initial
``Targeting the Ultra-Poor'' (TUP) program in Bangladesh as a
supplement or precursor to credit services. First implemented by BRAC
in 2007, the program aims to simultaneously alleviate physical and
human capital constraints by providing households with a significant
transfer of food and productive assets, followed by two years of
training and support by extension officers citep:bandiera-etal17.  The
general framework[fn:: Also known as the ``graduation framework''
pointing to the original ambition to move households into an activity
where they are able to finance further income growth without costly
transfers.] has since expanded to a wide range of countries, with a
general pattern of success in increasing aggregate investment, labor
supply, and aggregate consumption citep:banerjee-etal15.

A second, older literature that has gained new interest in recent
years studies the effect of offering poor households direct
unconditional cash transfers (UCT's)
citep:haushofer-shapiro16,blattman-etal16.  While this and the TUP
framework are both direct capital transfer interventions, they are
very different in their approach, with TUP programs guiding and
constraining the use of capital towards productive investment while
UCT's allow households to invest, save, and consume as they see
fit. The natural question that arises is whether (and how) these
additional features and constraints in the TUP framework change how
households use their capital transfers.  This study provides the first
side-by-side experimental evaluation of the two approaches.

* Treatments

How do the effects of TUP differ from the effects of a UCT, when the
resources given to recipients have roughly the same value?  To provide
some insight into this question, we worked with BRAC's South Sudanese
branch office in Yei county to randomize selection into either a TUP
or UCT treatment.

We began in April 2013 by conducting a census of all households having
one or more adult women within the area around BRAC's office in the
town of Yei in Western Equatoria, interviewing women in 1279
households.

The census interview included questions to assess eligibility for the
program.  Eligibility depended on several criteria.  First, households
were excluded if they had a salaried worker in the household; if they
were participating in some other NGOs livelihoods program; or if they
had no access to cultivable land (which was in some cases necessary
for the program's model).  If not excluded on these grounds,
households were then deemed eligible if they satisfied three or more
of the following criteria: 

  1. The household head was working as a day laborer (generally an occupation with poverty wages);
  2. The household had two or more children;
  3. At least one child in the household was working;
  4. The household had fewer than three rooms; or
  5. A woman in the household had not completed secondary school.

Of the 1279 households interviewed in the census, 745 were deemed to
be eligible.  We then conducted a baseline survey of these eligible
households, successfully interviewing 649 of eligible households.  Of
the eligible households interviewed at baseline, 250 were randomly
assigned to participate in the TUP program, while another 125 were
randomly assigned to a UCT group which was given cash roughly equal to
the value of the physical assets provided to TUP households.  This
left 274 eligible households interviewed at baseline randomly assigned
to a control group, and another 96 eligible households who were /not/
interviewed at baseline. 

** The TUP Program
   
The TUP program itself is similar to the other TUP/Graduation
programs completed by BRAC elsewhere.  BRAC designed several possible
enterprises: goat husbandry, duck husbandry, maize cultivation,
vegetable cultivation, and the selling of dried fish.
The preferences of enrolled households over these were elicited; we
asked both for an ordering, and a willingness to pay for both the
training and productive assets associated with the five different
enterprises.

#+begin_src python 
import pandas as pd
from cfe.df_utils import df_to_orgtbl

df = pd.read_stata('../../TUP-data/Baseline/Tup programme sudan single file.dta',convert_categoricals=False)

df = df.set_index('idno').iloc[:,-15:]

# Different enumerators handled this question differently; some list
# the assets alphabetically then giving the numeric preference
# ordering of each asset, while others wrote the asset letters in
# order of preference.  Here we sort this out.

idx = df.rename(columns=lambda s:tuple(s.split('_'))).columns
df.columns = pd.MultiIndex.from_tuples(idx)

order  = df['rank'].stack()

ac = df['ac']
ac.columns = range(5)
myorder = order.astype(int)-1

myidx=myorder.droplevel(1).reset_index().set_index(['idno',0])

maybe = ac.stack()[myidx.index]
maybe.index = maybe.sort_index().index
maybe =  maybe[~maybe.index.duplicated()]
maybe.name = 'Choice'

ranks = maybe.unstack()

freq = pd.concat([ranks[x].value_counts() for x in ranks],axis=1)
freq.rename(index={'A':'Goats','B':'Ducks','C':'Maize','D':'Vegetables','E':'Dry Fish','':'Missing'},inplace=True)
freq.columns = freq.columns + 1

print(df_to_orgtbl(freq,float_fmt='%d'))
#+end_src


#+name: tab:enterprise_choice
#+caption: Frequency of Rankings of Enterprise Choices
|            |   1 |   2 |   3 |   4 |   5 |
|------------+-----+-----+-----+-----+-----|
| Dry Fish   | 219 | 105 | 113 |  74 |  73 |
| Goats      | 149 | 115 | 107 | 107 | 116 |
| Ducks      | 120 | 172 | 137 |  88 |  68 |
| Vegetables |  65 |  85 | 108 | 138 | 164 |
| Maize      |  55 | 127 | 127 | 145 | 122 |
| Missing    |   2 |   1 | --- |   3 |   3 |

Table [[tab:enterprise_choice]] reports on the frequency of elicited
preference ranks over the five different choices.  Dry fish was the
most popular single first choice, with animal husbandry (goats then
ducks) the second and third choices.  Crop cultivation (whether of
vegetables or maize) was less popular. 

*** Enterprise Assignment & Training

The second phase of the program was enterprise assignment and
training. Unlike some other programs of this type (e.g., the TUP
program in Bangladesh described by cite:bandiera-etal17), the number
of households given each kind of asset was set in advance, with 75
enrolled in agricultural activities (either maize or vegetable
cultivation), 85 in duck rearing, 45 in goat rearing, and the rest in
small trade involving dried fish.  Within these limits, assignment to
particular activities was made at the discretion of program staff,
taking into account subjects' preferences and skills.

Households then attended training sessions. The first of these were
for general business skills around literacy, numeracy, and financial
management. The next were sector specific and focused on animal
husbandry or crop cultivation.

After training was completed, asset transfers began in late 2013 and
continued through the first few months of 2014. The productive assets
related to each enterprise were valued at around $240 per household,
with a random subset receiving an additional $60 in assets later
in 2014.  Shortly thereafter, households started to attend weekly or
semi-weekly meetings with other nearby participants to discuss with
each other and a BRAC extension officer the details of their
businesses. These meetings also included food transfers for a while,
which were designed to help get households to the point of receiving
revenue from their assets without having to sell them.  In all, the
market value of these food transfers were valued at $110, bringing the
total value of all transfers to either $350 or $410.

** Unconditional Cash Transfers

To implement an unconditional cash transfer, we randomly assigned each
of the 125 households in the UCT group to receive a cash transfer of
either $350 or $410, matching the values of assets received by
households in the TUP treatments.  

Local community leaders were approached in advance of talking to
individual households to reassure recipients of BRAC's intentions. It
was made clear that the transfers were there for their benefit, that
while they were encouraged to put them towards some productive use,
their use was unrestricted, and that under no circunstances would they
receive a second transfer in the future. BRAC was a well-established
institution in the greater Yei community, so these assurances were
taken as credible and no one refused to accept the money.

Cash transfers were delivered by field officers in person and in the
local currency (South Sudanese Pounds, or SSPs).  That transfers were
delivered in cash distinguishes these from several other cash transfer
programs.  For example, cite:haushofer-shapiro16 deposited funds in a
bank or mobile money account, while cite:fafchamps-etal14 used money
transfers.  In all of these experiments transfers were denominated in
the local currency.  Howver, experimental studies of the behavioral
implications of savings accounts citep:dupas-robinson13 would lead one
to expect that the method of disbursal might affect the propensity to
save a cash transfer.

* Data Collection & Timeline

** Timeline                                                          :ignore:
We have four principal sources of data.  First, as discussed above, we
conducted a census of households with women in the area around BRAC's
offices in April of 2013.  Second, in the summer of 2013 we conducted
a baseline survey of eligible households.  We randomly assigned women
who responsed to this baseline survey to one of the TUP, UCT, or
control groups; some randomly selected TUP and UCT assignees were
further chosen to receive an asset/cash "top-up" valued at $60.
Third, we conducted a "midline" survey in June 2014, and an "endline"
survey in the summer of 2015.

Training and asset transfers for the TUP group began in late 2013; our
intention was to make transfers of cash to the UCT group at roughly
the same time that TUP participants were ready to begin their
operations, during the first quarter of 2014.  However, these plans
were complicated by an onset of broad civil unrest in South Sudan, and
disruption of BRAC's operations.

In response to the outbreak of violence in late 2013 and subsequent
closing of the offices in Yei, a midline survey was conducted in June
2014 to try to separate pre- and post-conflict changes in
outcomes. For lack of a valid comparison group, we will not speak with
any authority about the effect of the conflict on economic conditions
in Yei, though we will report estimates of treatment effects on the
severity or likelihood of having been effected exposure to the
conflict.  Some of the original asset transfers were done before the
office closure, which may affect estimates of the difference between
programs if rates of return changed in the few intervening
months.  Finally, an endline survey was conducted in mid-2015 to
estimate the effect of program participation on households' financial
situation and overall welfare. The key here is that the survey
conducted in mid-2014 provides us with /short-term/ treatment effects
of the TUP program within 6 months of the asset transfers, while
providing a second baseline for the Cash transfers. Likewise, the 2015
survey allows us to estimate treatment effects one year after the cash
transfers, and 15--18 months after the asset transfers.

This unfortunately left us without data past one year for the cash
transfer effects.  To get some point estimates on household welfare
for this group in the slightly longer term, we conducted a series of
five short surveys on a monthly basis from November of 2015 to March
of 2016. These collected only a subset of the full consumption modules
and a few questions tracking major transactions and shocks. The short
length of the survey allowed them to be administered via the mobile
network, reducing cost and improving response rate. These data provide
a slightly longer estimate of treatment effects for both cash
transfers and the TUP program, but will be especially valuable in
assessing whether the cash transfers had a long-term impact on
consumption.

** Balance on Obervables

We start by checking whether either treatment arm appears significantly different
from the control group in terms of average baseline observable characteristics.
\Tab{balance_check} presents summary statistics by group on a range of factors
related to consumption, asset holdings, and household characteristics.

*** Output from Elliott's code (older version)              :ignore:noexport:
#+name: balance_check
#+BEGIN_SRC python :dir ../analysis :results value table :exports none
import check_balance
return check_balance.tables
#+END_SRC


#+name: tab:balance_check
#+caption: Means of some analysis variables at baseline.  Asterisks indicate p<.1, .05, and .01 respectively
#+attr_latex: :environment longtable :align lrrrrr
|-----------------+---------+--------------+--------------+-----|
| Consumption     |     CTL | $\Delta$ TUP | $\Delta$ CSH | $N$ |
|-----------------+---------+--------------+--------------+-----|
| Meat            |    4.21 |       -0.568 |       -0.052 | 378 |
| Fuel            |    0.76 |       -0.039 |       -0.072 | 456 |
| Clothesfootwear |    0.67 |       -0.026 |        0.033 | 595 |
| Soap            |    0.48 |       -0.008 |       -0.026 | 536 |
| Fish            |    2.50 |       -0.154 |       -0.156 | 474 |
| Charities       |    0.03 |       -0.006 |          0.0 | 134 |
| Cereals         |    9.19 |       -0.947 |         0.27 | 605 |
| Transport       |    0.18 |       -0.033 |        0.002 | 193 |
| Cosmetics       |    0.68 |        0.027 |       -0.125 | 468 |
| Sugar           |    1.71 |       -0.078 |       -0.189 | 604 |
| Egg             |    1.10 |       -0.091 |        0.038 | 276 |
| Oil             |    1.36 |        -0.13 |       -0.141 | 613 |
| Ceremonies      |    0.13 |        0.006 |        0.026 | 152 |
| Beans           |    0.70 |        0.232 |        0.226 | 192 |
| Fruit           |    0.69 |       -0.089 |        0.001 | 272 |
| Textiles        |    0.16 |       -0.004 |  $0.056^{*}$ | 376 |
| Utensils        |    0.25 |       -0.009 |        0.008 | 442 |
| Dowry           |    1.27 |       -0.041 |        0.028 | 126 |
| Furniture       |    0.20 |       -0.014 |        0.045 | 368 |
| Salt            |    0.45 |       -0.026 |        0.007 | 617 |
| Vegetables      |    1.54 |       -0.165 |        -0.18 | 471 |
|-----------------+---------+--------------+--------------+-----|
| Assets          |     CTL | $\Delta$ TUP | $\Delta$ CSH | $N$ |
|-----------------+---------+--------------+--------------+-----|
| Smallanimals    |  236.60 |      -86.068 |     -123.133 | 123 |
| Bicycle         |  109.08 |      -12.555 |      -11.414 | 171 |
| Radio           |   58.45 |       -5.968 |      -16.529 | 260 |
| Motorcycle      |  341.74 |      192.956 | 353.836^{**} |  93 |
| Net             |   19.16 |        0.668 |        0.247 | 423 |
| Poultry         |   42.40 |       -3.365 |       -8.894 | 161 |
| Bed             |  241.27 |        7.992 |       32.762 | 521 |
| Chairtables     |  206.79 |      -29.368 |        3.617 | 531 |
| Mobile          |   97.54 |       12.627 |       -4.198 | 414 |
| Netitn          |    7.82 |        1.215 |        1.178 | 181 |
| Cosmetics       |    0.68 |        0.027 |       -0.125 | 468 |
|-----------------+---------+--------------+--------------+-----|
| Household       |     CTL | $\Delta$ TUP | $\Delta$ CSH | $N$ |
|-----------------+---------+--------------+--------------+-----|
| Daily Food      |   25.18 |       -2.215 |       -0.261 | 643 |
| Daily Exp       |   29.90 |       -2.167 |       -0.288 | 646 |
| No. Houses      |    2.83 |        0.031 |        0.118 | 543 |
| In Business     |    0.40 |        0.038 |        0.017 | 265 |
| Cereals         |    9.19 |       -0.947 |         0.27 | 605 |
| # Child         |    3.26 |        0.118 |        0.108 | 594 |
| Asset Tot.      | 1757.05 |      -44.791 |       98.654 | 603 |
| Cash Savings    |  236.90 |        28.52 |      -66.812 | 431 |
| HH size         |    7.23 |       -0.175 |          0.3 | 648 |
|-----------------+---------+--------------+--------------+-----|

This is simply suggestive evidence that the treatment and control groups were similar
in observables at baseline, with the exception that the cash group has atypically
more motorcycles and clothing. But it does suggests that our stratified randomization
was not too far from creating comparable group
*** Output from Ethan's code (newer version)                :ignore:noexport:
#+name: balance_check_2
#+BEGIN_SRC python :dir ../analysis :results output raw
import check_balance
#+END_SRC

#+RESULTS: balance_check_2
| Consumption               |   CTL |       TUP |        CSH |
|---------------------------+-------+-----------+------------|
| Cereals                   |   9.0 |       8.2 |        9.5 |
| Beans                     |   0.7 |       0.9 |        0.9 |
| Oil                       |   1.3 |       1.2 |        1.2 |
| Salt                      |   0.4 |       0.4 |        0.5 |
| Sugar                     |   1.7 |       1.6 |        1.5 |
| Meat                      |   4.1 |       3.6 |        4.2 |
| Fish                      |   2.4 |       2.4 |        2.3 |
| Egg                       |   1.0 |       1.0 |        1.1 |
| Milk                      |   1.0 |       1.0 |        1.1 |
| Vegetables                |   1.5 |       1.4 |        1.4 |
| Fruit                     |   0.6 |       0.6 |        0.7 |
| Spices                    |   0.2 |       0.3 |        0.2 |
| Alcohol                   |   0.0 |       0.0 |        0.0 |
| Other foods               |   0.0 |       0.0 |        0.0 |
| Fuel                      |   0.8 |       0.7 |        0.7 |
| Cosmetics                 |   0.7 |       0.7 |        0.6 |
| Soap                      |   0.5 |       0.5 |        0.5 |
| Transport                 |   0.2 |       0.1 |        0.2 |
| Entertainment             |   0.1 |       0.1 |        0.1 |
| Clothes & footwear        |   0.7 |       0.6 |        0.7 |
| Utensils                  |   0.2 |       0.2 |        0.3 |
| Furniture                 |   0.2 |       0.2 |        0.2 |
| Textiles                  |   0.1 |       0.1 |    0.2^{*} |
| Ceremonies                |   0.1 |       0.1 |        0.2 |
| Charities                 |   0.0 |       0.0 |        0.0 |
| Dowry                     |   1.3 |       1.2 |        1.3 |
| Other                     |   0.0 |       0.0 |        0.0 |
|---------------------------+-------+-----------+------------|
| Asset                     |   CTL |       TUP |        CSH |
|---------------------------+-------+-----------+------------|
| Cows                      | 261.8 |     112.7 |      153.6 |
| Small animals             | 252.1 |     150.5 | 113.5^{**} |
| Poultry                   |  43.8 |      39.0 |       33.5 |
| Plough                    |   0.0 |       0.0 |        0.0 |
| Shed                      |   1.8 |  0.0^{**} |        1.6 |
| Shop                      |  95.5 |      79.4 |       69.8 |
| Radio                     |  57.6 |      52.5 |       41.9 |
| Tv                        |  29.0 |      45.9 |       36.1 |
| Fan                       |   1.6 |       1.8 |    6.0^{*} |
| Mobile                    |  92.1 | 110.2^{*} |       93.3 |
| Chairs & Tables           | 205.6 |     177.4 |      210.4 |
| Bed                       | 232.3 |     249.3 |      274.0 |
| Bicycle                   | 112.4 |      96.5 |       97.7 |
| Carts                     |   2.6 |       3.5 |        1.6 |
| Sewing                    |  12.5 |       5.0 |   1.2^{**} |
| Net                       |  19.3 |      19.8 |       19.4 |
| Motorcycle                | 342.7 |     534.7 | 695.6^{**} |
|---------------------------+-------+-----------+------------|
| Household characteristics |   CTL |       TUP |        CSH |
|---------------------------+-------+-----------+------------|
| HH size                   |   7.3 |       7.1 |        7.5 |
| # Children                |   3.3 |       3.4 |        3.4 |
| # Houses                  |   2.8 |       2.9 |        2.9 |
| In Business               |   0.4 |       0.4 |        0.4 |
|---------------------------+-------+-----------+------------|
| $N$                       |   125 |       262 |        249 |



*** Creation of master random assignment file                        :ignore:
# This code creates a file which contains all of the assignments to
# treatment groups, regardless of whether a respondent was found at
# one or all of baseline, midline, and endline. The file reconciles
# differences between asset_assign.csv and locations.csv. We consider
# asset_assign.csv to be the assignment at baseline, and locations.csv
# to be the assignment that was created at endline when some
# respondents who were not found at baseline were still surveyed at
# endline.
# There are 6 households that were interviewed only at endline and
# were not included in asset_assign.csv or locations.csv. These
# households are manually added as Control households in 
# master_assignment.csv, like the other households for which a 
# group was not assigned at baseline and therefore are labeled as 
# Control at endline.

#+name: master_random_assignment
#+begin_src python
"""For making the master treatment groups .csv file"""

data_locations = "../../TUP-data/csv/locations.csv"
data_asset_assign = "../../TUP-data/csv/asset_assign.csv"
import pandas as pd

df_l = pd.read_csv(data_locations) # assignments at endline
df_a = pd.read_csv(data_asset_assign) # assignments at baseline

# take the union of locations.csv and asset_assign.csv, rename groups accordingly
df = pd.merge(df_l, df_a[ ['respid', 'group' ] ], how='outer', left_on='RespID', right_on='respid')
df.replace(to_replace='Gift', value='Control', inplace=True)
df.replace(to_replace='TUP-high asset', value='TUP', inplace=True)
df.replace(to_replace='Cash', value='UCT', inplace=True)

# asset_assign.csv assignments are listed under 'group'
# locations.csv assignments are listed under 'Group'

for i in range(len(df)): 

    # if a group is not assigned at baseline (in asset_assign 'group'), it should be 'Control' at endline (in locations 'Group') (takes care of observation idno 1157)
    if pd.isnull(df.iloc[i, df.columns.get_loc('group')]):
        df.iloc[i, df.columns.get_loc('Group')]='Control'
    
    # if a group is assigned at baseline (in asset_assign 'group'), it should be the same at endline (in locations 'Group') (takes care of observation idno 2131)
    if pd.notna(df.iloc[i, df.columns.get_loc('group')]) and (df.iloc[i, df.columns.get_loc('group')] != df.iloc[i, df.columns.get_loc('Group')]):
        df.iloc[i, df.columns.get_loc('Group')] = df.iloc[i, df.columns.get_loc('group')]

# now all the assignments in 'Group' should be consistent with 'group' so 'group' is a subset of 'Group' and we can drop it
df = df.drop(columns=['group', 'respid'])

# manually add the 6 households which were only interviewed at endline as 'Control'
df = df.append({'RespID': 1359,  'Group': 'Control'}, ignore_index=True)
df = df.append({'RespID': 1484,  'Group': 'Control'}, ignore_index=True)
df = df.append({'RespID': 1553,  'Group': 'Control'}, ignore_index=True)
df = df.append({'RespID': 1960,  'Group': 'Control'}, ignore_index=True)
df = df.append({'RespID': 2142,  'Group': 'Control'}, ignore_index=True)
df = df.append({'RespID': 2174,  'Group': 'Control'}, ignore_index=True)

df.to_csv('master_assignment.csv')
#+end_src

#+RESULTS: master_random_assignment


*** Recreating Elliott's balance table (without some variables which are sums of other variables) :ignore:

#  N represents the number of nonzero values

#+name: mean_balance
#+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
# <<load_data>> can use this!!
data_baseline  = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
import pandas as pd
from scipy import stats
from cfe.df_utils import df_to_orgtbl
import matplotlib. pyplot as plt

df_b = pd.read_stata(data_baseline)
df_a = pd.read_csv(data_assignment)

df = pd.merge(df_b, df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")


df_3_days = df[ ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b'] ].transform(lambda x: x/3)

df_month = df[ ['c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b'] ].transform(lambda x: x/30)

df_year = df[ ['c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b'] ].transform(lambda x: x/365)

df_assets = df.get(['asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b',
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b',
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b',
             'asset_val_netITN_b',
             'asset_n_house_b', 'in_business_b', 'child_total_b', 'hh_size_b',
             'Group'])

df_1 = df_3_days.join(df_month.join(df_year.join(df_assets)))

means = df_1.groupby(['Group']).mean()
means = means.T

means.insert(len(means.columns), '$\Delta$ TUP', means['TUP'] - means['Control'])
means.insert(len(means.columns), '$\Delta$ UCT', means['UCT'] - means['Control'])

d = {'Control': means['Control'], '$\Delta$ TUP': means['TUP'] - means['Control'], '$\Delta$ UCT': means['UCT'] - means['Control']}
means = pd.DataFrame(data=d)

# so we can add * later
means = means.round(decimals = 3)
means = means.astype(str)
means.insert(len(means.columns), 'N', 0)

column_list = ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b', \
              'c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b', \
               'c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b', \
               'asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b', \
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b', \
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b', \
             'asset_val_netITN_b', \
             'asset_n_house_b', 'in_business_b', 'child_total_b', 'hh_size_b']

# some are off by 0.001
for column in column_list:
    
    pvalue_tup = stats.ttest_ind(df_1[df_1['Group'] == 'TUP'][column], df_1[df_1['Group'] == 'Control'][column], nan_policy='omit').pvalue
    
    if pvalue_tup <= 0.01: 
        means.at[column,'$\Delta$ TUP'] = means.at[column,'$\Delta$ TUP'] + '***'
    elif pvalue_tup <= 0.05: 
        means.at[column,'$\Delta$ TUP'] = means.at[column,'$\Delta$ TUP'] + '**'
    elif pvalue_tup <= 0.1: 
        means.at[column,'$\Delta$ TUP'] = means.at[column,'$\Delta$ TUP'] + '*'
        
    pvalue_cash = stats.ttest_ind(df_1[df_1['Group'] == 'UCT'][column], df_1[df_1['Group'] == 'Control'][column], nan_policy='omit').pvalue
    
    if pvalue_cash <= 0.01: 
        means.at[column,'$\Delta$ UCT'] = means.at[column,'$\Delta$ UCT'] + '***'
    elif pvalue_cash <= 0.05: 
        means.at[column,'$\Delta$ UCT'] = means.at[column,'$\Delta$ UCT'] + '**'
    elif pvalue_cash <= 0.1: 
        means.at[column,'$\Delta$ UCT'] = means.at[column,'$\Delta$ UCT'] + '*'
    
    # as it is in the table, N is number of nonzero entries
    means.at[column, 'N'] = df_1[df_1[column] > 0][column].count()

# label the rows using "Better Var Name" in TUP-data/ssudan_survey.org,
# but without (last three days) or (last month) since variables are transformed (i.e. divided by 365 for yearly)
means.rename({'c_meat_b': ' Meat', 'c_fish_b': 'Fish', 
            'c_cereals_b': 'Cereals', 'c_sugar_b': 'Sugar', 
            'c_egg_b': 'Egg', 'c_oil_b': 'Oil', 
            'c_beans_b': 'Beans', 'c_fruit_b': 'Fruit', 
            'c_salt_b': 'Salt', 'c_vegetables_b': 'Vegetables', 
            'c_milk_b': 'Milk', 'c_spices_b': 'Spices', 
            'c_alcohol_b': 'Alcohol', 'c_otherfood_b': 'Other food', 
            'c_fuel_b': 'Cooking fuel', 'c_soap_b': 'Toiletries', 
            'c_transport_b': 'Transportation', 'c_cosmetics_b': 'Cosmetics', 
            'c_entertainment_b': 'Entertainment', 'c_clothesfootwear_b': 'Clothing, footwear', 
            'c_charities_b': 'Charities', 'c_ceremonies_b': 'Rituals/ceremonies', 
            'c_textiles_b': 'Textiles', 'c_utensils_b': 'Utensils',
            'c_dowry_b': 'Dowry', 'c_furniture_b': 'Furniture', 
            'c_other_b': 'Other non-food', 'asset_val_cows_b': 'Large livestock (cows)', 
            'asset_val_smallanimals_b': 'Small livestock', 'asset_val_bicycle_b': 'Bicycle', 
            'asset_val_radio_b': 'Radio', 'asset_val_motorcycle_b': 'Motorcycle', 
            'asset_val_net_b': 'Mosquito net', 'asset_val_poultry_b': 'Poultry', 
            'asset_val_bed_b': 'Bed', 'asset_val_chairtables_b': 'Chair/table', 
            'asset_val_mobile_b': 'Mobile phone', 'asset_val_netITN_b': 'Mosquito net ITN', 
            'asset_n_house_b': '# Houses', 'in_business_b': 'In business', 
            'child_total_b': '# Child', 'hh_size_b':'Household size'}, inplace=True)

newdf = df_to_orgtbl(means, float_fmt = '%.3f')
print(newdf)

df_1.boxplot(column = 'c_textiles_b', by = 'Group')
plt.savefig("textiles_Elliott.png")

df_1.boxplot(column = 'asset_val_motorcycle_b', by = 'Group')
plt.savefig("motorcycle_Elliott.png")

#+end_src

#+RESULTS: mean_balance
|                        | Control | $\Delta$ TUP | $\Delta$ UCT |   N |
|------------------------+---------+-----------+-----------+-----|
| Meat                   |   4.205 |    -0.568 |    -0.052 | 378 |
| Fish                   |   2.505 |    -0.154 |    -0.156 | 474 |
| Cereals                |   9.187 |    -0.947 |      0.27 | 605 |
| Sugar                  |   1.713 |    -0.078 |    -0.189 | 604 |
| Egg                    |   1.096 |    -0.091 |     0.038 | 276 |
| Oil                    |   1.364 |    -0.131 |    -0.141 | 613 |
| Beans                  |   0.696 |     0.231 |     0.226 | 192 |
| Fruit                  |    0.69 |    -0.089 |       0.0 | 272 |
| Salt                   |   0.447 |    -0.026 |     0.007 | 617 |
| Vegetables             |   1.543 |    -0.165 |     -0.18 | 471 |
| Milk                   |   1.284 |    -0.237 |    -0.232 | 114 |
| Spices                 |    0.23 |     0.024 |    -0.045 | 158 |
| Alcohol                |   0.043 |     0.006 |    -0.029 |  18 |
| Other food             |     0.0 |       0.0 |       0.0 |   0 |
| Cooking fuel           |   0.762 |    -0.039 |    -0.072 | 456 |
| Toiletries             |   0.483 |    -0.009 |    -0.026 | 536 |
| Transportation         |   0.176 |    -0.033 |     0.002 | 193 |
| Cosmetics              |   0.682 |     0.027 |    -0.125 | 468 |
| Entertainment          |   0.087 |    -0.024 |     -0.02 | 145 |
| Clothing, footwear     |   0.658 |    -0.026 |     0.033 | 595 |
| Charities              |    0.03 |    -0.006 |      -0.0 | 134 |
| Rituals/ceremonies     |   0.132 |     0.007 |     0.026 | 152 |
| Textiles               |   0.154 |    -0.005 |    0.055* | 376 |
| Utensils               |   0.246 |    -0.008 |     0.008 | 442 |
| Dowry                  |   1.256 |     -0.04 |     0.028 | 126 |
| Furniture              |   0.196 |    -0.014 |     0.044 | 368 |
| Other non-food         |     0.0 |       0.0 |       0.0 |   0 |
| Large livestock (cows) |  253.31 |  -140.605 |   -99.681 |  35 |
| Small livestock        | 236.601 |   -86.069 |  -123.134 | 123 |
| Bicycle                | 109.075 |   -12.554 |   -11.413 | 171 |
| Radio                  |  58.448 |    -5.969 |   -16.529 | 260 |
| Motorcycle             | 341.737 |   192.956 | 353.836** |  93 |
| Mosquito net           |  19.164 |     0.668 |     0.248 | 423 |
| Poultry                |  42.402 |    -3.365 |    -8.894 | 161 |
| Bed                    |  241.27 |     7.992 |    32.762 | 521 |
| Chair/table            | 206.786 |   -29.368 |     3.617 | 531 |
| Mobile phone           |  97.537 |    12.627 |    -4.199 | 414 |
| Mosquito net ITN       |   7.822 |     1.215 |     1.178 | 181 |
| # Houses               |   2.829 |      0.03 |     0.118 | 543 |
| In business            |   0.398 |     0.038 |     0.017 | 265 |
| # Child                |   3.263 |     0.118 |     0.108 | 594 |
| Household size         |   7.232 |    -0.175 |       0.3 | 648 |

*** Box and whisker plots for Elliott's balance table              :noexport:

#+Caption: Box and whisker plot of textiles means
#+NAME: fig:box_whisker_textiles_Elliott
[[../documents/textiles_Elliott.png]]

#+Caption: Box and whisker plot of motorcycles means
#+NAME: fig:box_whisker_motorcycles_Elliott
[[../documents/motorcycle_Elliott.png]]


*** Creating a new balance table with only nonzero values counting toward the mean :ignore:

The following table shows the means of log expenditures, with
expenditures of zero treated as missing data.

# N represents the number of nonzero values, and only nonzero values
# are considered in calculations of the means

#+name: mean_balance_only_nonzero_values
#+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
data_baseline  = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
import pandas as pd
from scipy import stats
from cfe.df_utils import df_to_orgtbl
import matplotlib. pyplot as plt
import numpy as np

df_b = pd.read_stata(data_baseline)
df_a = pd.read_csv(data_assignment)

df = pd.merge(df_b, df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")


df_3_days = df[ ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b'] ]

df_month = df[ ['c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b'] ]

df_year = df[ ['c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b'] ]

df_assets = df.get(['asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b',
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b',
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b',
             'asset_val_netITN_b',
             'asset_n_house_b', 'child_total_b', 'hh_size_b'])

df_1 = df_3_days.join(df_month.join(df_year.join(df_assets)))

df_1.replace(to_replace = 0, value = float("NaN") , inplace = True)

# take the logs of expenditure for all variables except dummy variables
df_1 = df_1.transform(lambda x: np.log(x))

# add in group assignment and the dummy variables that need 0 as one of their values
df_1 = df_1.join(df[ [ 'in_business_b', 'Group'] ])

means = df_1.groupby(['Group']).mean()
means = means.T

means.insert(len(means.columns), '$\Delta$ TUP', means['TUP'] - means['Control'])
means.insert(len(means.columns), '$\Delta$ UCT', means['UCT'] - means['Control'])

d = {'Control': means['Control'], '$\Delta$ TUP': means['TUP'] - means['Control'], '$\Delta$ UCT': means['UCT'] - means['Control']}
means = pd.DataFrame(data=d)

# so we can add * later
means = means.round(decimals = 3)
means = means.astype(str)
means.insert(len(means.columns), 'N', 0)

column_list = ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b', \
              'c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b', \
               'c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b', \
               'asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b', \
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b', \
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b', \
             'asset_val_netITN_b', \
             'asset_n_house_b', 'in_business_b', 'child_total_b', 'hh_size_b']

# some are off by 0.001
for column in column_list:
    
    pvalue_tup = stats.ttest_ind(df_1[df_1['Group'] == 'TUP'][column], df_1[df_1['Group'] == 'Control'][column], nan_policy='omit').pvalue
    
    if pvalue_tup <= 0.01: 
        means.at[column,'$\Delta$ TUP'] = means.at[column,'$\Delta$ TUP'] + '***'
    elif pvalue_tup <= 0.05: 
        means.at[column,'$\Delta$ TUP'] = means.at[column,'$\Delta$ TUP'] + '**'
    elif pvalue_tup <= 0.1: 
        means.at[column,'$\Delta$ TUP'] = means.at[column,'$\Delta$ TUP'] + '*'
        
    pvalue_cash = stats.ttest_ind(df_1[df_1['Group'] == 'UCT'][column], df_1[df_1['Group'] == 'Control'][column], nan_policy='omit').pvalue
    
    if pvalue_cash <= 0.01: 
        means.at[column,'$\Delta$ UCT'] = means.at[column,'$\Delta$ UCT'] + '***'
    elif pvalue_cash <= 0.05: 
        means.at[column,'$\Delta$ UCT'] = means.at[column,'$\Delta$ UCT'] + '**'
    elif pvalue_cash <= 0.1: 
        means.at[column,'$\Delta$ UCT'] = means.at[column,'$\Delta$ UCT'] + '*'
    
   # N is number of nonzero entries
    means.at[column, 'N'] = df_1[column].count()

# label the rows using "Better Var Name" in TUP-data/ssudan_survey.org
means.rename({'c_meat_b': ' Meat (last three days)', 'c_fish_b': 'Fish (last three days)', 
            'c_cereals_b': 'Cereals (last three days)', 'c_sugar_b': 'Sugar (last three days)', 
            'c_egg_b': 'Egg (last three days)', 'c_oil_b': 'Oil (last three days)', 
            'c_beans_b': 'Beans (last three days)', 'c_fruit_b': 'Fruit (last three days)', 
            'c_salt_b': 'Salt (last three days)', 'c_vegetables_b': 'Vegetables (last three days)', 
            'c_milk_b': 'Milk (last three days)', 'c_spices_b': 'Spices (last three days)', 
            'c_alcohol_b': 'Alcohol (last three days)', 'c_otherfood_b': 'Other food (last three days)', 
            'c_fuel_b': 'Cooking fuel (last month)', 'c_soap_b': 'Toiletries (last month)', 
            'c_transport_b': 'Transportation (last month)', 'c_cosmetics_b': 'Cosmetics (last month)', 
            'c_entertainment_b': 'Entertainment (last month)', 'c_clothesfootwear_b': 'Clothing, footwear (last year)', 
            'c_charities_b': 'Charities (last year)', 'c_ceremonies_b': 'Rituals/ceremonies (last year)', 
            'c_textiles_b': 'Textiles (last year)', 'c_utensils_b': 'Utensils (last year)',
            'c_dowry_b': 'Dowry (last year)', 'c_furniture_b': 'Furniture (last year)', 
            'c_other_b': 'Other non-food (last year)', 'asset_val_cows_b': 'Large livestock (cows)', 
            'asset_val_smallanimals_b': 'Small livestock', 'asset_val_bicycle_b': 'Bicycle', 
            'asset_val_radio_b': 'Radio', 'asset_val_motorcycle_b': 'Motorcycle', 
            'asset_val_net_b': 'Mosquito net', 'asset_val_poultry_b': 'Poultry', 
            'asset_val_bed_b': 'Bed', 'asset_val_chairtables_b': 'Chair/table', 
            'asset_val_mobile_b': 'Mobile phone', 'asset_val_netITN_b': 'Mosquito net ITN', 
            'asset_n_house_b': '# Houses', 'in_business_b': 'In business', 
            'child_total_b': '# Child', 'hh_size_b':'Household size'}, inplace=True)

newdf = df_to_orgtbl(means, float_fmt = '%.3f')
print(newdf)

# box and whisker plots of log positive observations
df_1['c_textiles_b'] = df_1['c_textiles_b']
df_1.boxplot(column = 'c_textiles_b', by = 'Group')
plt.savefig("textiles_nonzero.png")

df_1['asset_val_mobile_b'] = df_1['asset_val_mobile_b']
df_1.boxplot(column = 'asset_val_mobile_b', by = 'Group')
plt.savefig("mobile_nonzero.png")

#+end_src

#+RESULTS: mean_balance_only_nonzero_values
|                                | Control | $\Delta$ TUP | $\Delta$ UCT | $N$ |
|--------------------------------+---------+--------------+--------------+-----|
| Meat (last three days)         |   2.845 |        0.018 |        0.033 | 378 |
| Fish (last three days)         |   2.058 |        0.041 |       -0.022 | 474 |
| Cereals (last three days)      |   3.033 |       -0.071 |       -0.019 | 605 |
| Sugar (last three days)        |   1.346 |        -0.06 |       -0.031 | 604 |
| Egg (last three days)          |   1.811 |        0.011 |       -0.077 | 276 |
| Oil (last three days)          |   0.897 |        0.014 |       -0.035 | 613 |
| Beans (last three days)        |   1.669 |        0.135 |        0.127 | 192 |
| Fruit (last three days)        |   1.329 |        0.007 |        0.027 | 272 |
| Salt (last three days)         |   0.201 |       -0.054 |        0.001 | 617 |
| Vegetables (last three days)   |   1.532 |       -0.026 |       -0.047 | 471 |
| Milk (last three days)         |   2.514 |       -0.015 |       -0.217 | 114 |
| Spices (last three days)       |   0.858 |       -0.069 |        -0.21 | 158 |
| Alcohol (last three days)      |   1.304 |        0.104 |        0.305 |  18 |
| Other food (last three days)   |     nan |          nan |          nan |   0 |
| Cooking fuel (last month)      |   2.941 |        -0.01 |       -0.127 | 456 |
| Toiletries (last month)        |   2.328 |       -0.051 |       -0.172 | 536 |
| Transportation (last month)    |   2.395 |       -0.003 |        0.127 | 193 |
| Cosmetics (last month)         |   2.659 |        0.084 |       -0.054 | 468 |
| Entertainment (last month)     |   2.088 |        -0.13 |       -0.064 | 145 |
| Clothing, footwear (last year) |   4.989 |       -0.005 |       -0.074 | 595 |
| Charities (last year)          |   3.348 |       -0.229 |       -0.121 | 134 |
| Rituals/ceremonies (last year) |   4.422 |        0.261 |        0.162 | 152 |
| Textiles (last year)           |   4.096 |        0.052 |        0.112 | 376 |
| Utensils (last year)           |   4.352 |        -0.01 |       -0.003 | 442 |
| Dowry (last year)              |   5.934 |        0.433 |        0.178 | 126 |
| Furniture (last year)          |   4.224 |        0.033 |        0.185 | 368 |
| Other non-food (last year)     |     nan |          nan |          nan |   0 |
| Large livestock (cows)         |   7.461 |       -0.072 |       -0.068 |  35 |
| Small livestock                |   6.425 |       -0.195 |       -0.364 | 123 |
| Bicycle                        |   5.601 |        0.051 |         0.14 | 171 |
| Radio                          |     4.3 |        0.067 |       -0.071 | 260 |
| Motorcycle                     |   6.762 |        0.676 |        0.778 |  93 |
| Mosquito net                   |   3.089 |        0.024 |       -0.008 | 423 |
| Poultry                        |   4.616 |        0.002 |       -0.177 | 161 |
| Bed                            |   5.306 |       -0.001 |        0.116 | 521 |
| Chair/table                    |   4.889 |        0.021 |        0.025 | 531 |
| Mobile phone                   |   4.811 |        0.119 |        0.031 | 414 |
| Mosquito net ITN               |   3.147 |       -0.047 |         0.13 | 181 |
| # Houses                       |   1.064 |       -0.027 |         -0.0 | 543 |
| # Child                        |   1.118 |        0.043 |        0.039 | 594 |
| Household size                 |   1.917 |       -0.021 |         0.05 | 648 |
| In business                    |   0.398 |        0.038 |        0.017 | 638 |

*** Box and whisker plots for the new balance table                  :ignore:

#+Caption: Box and whisker plot of log(positive textiles) for nonzero values
#+NAME: fig:box_whisker_textiles_nonzero
[[../documents/textiles_nonzero.png]]

#+Caption: Box and whisker plot of log(positive mobile) for nonzero values
#+NAME: fig:box_whisker_mobile_nonzero
[[../documents/mobile_nonzero.png]]


** Sample Selection

Our census found 745 eligible households, and made efforts to
interview a respondent from every one of these households.  The degree
to which we succeeded is documented in \Tab{attrition_count}.
In the baseline survey we successfully interviewed
649 of these households; in the midline 606, and in the endline 694.

*** Original attrition tables                                        :ignore:
#+name: attrition_check
#+BEGIN_SRC python :dir ../analysis :results value table :exports none
import check_attrition
return check_attrition.TAB
#+END_SRC

#+caption: Total number of households in sample by group and round.  Numbers in parentheses indicate the size of of the subset also interviewed at baseline.
#+attr_latex: :environment longtable :align lrrrrr
#+name: tab:attrition_count
|-------------+------+-----------+-----------|
| Full Sample | 2013 | 2014      | 2015      |
|-------------+------+-----------+-----------|
| Control     |  281 | 265 (219) | 347 (262) |
| UCT         |  124 | 113 (112) | 111 (110) |
| TUP         |  244 | 228 (223) | 236 (231) |
|-------------+------+-----------+-----------|
| All         |  649 | 606 (554) | 694 (603) |
|-------------+------+-----------+-----------|

Another possible issue has to do with whether crucial question is to what extent attrition in 2014 and 2015 will bias our
results. \Tab{attrition_count} reports the total number of households identified in
each treatment arm and year for the whole sample. \Tab{attrition_balanced_count}
reports the same numbers restricting ourselves to households with baseline surveys.
In the TUP group, we were unable to find 21 participants in 2014 (8% attrition), but
found 5 not identified in the baseline survey. We found 8 additional TUP households
with baseline surveys again in 2015 for a final attrition rate of 5%. Of those in the
UCT group, 12 were lost (9.6%) in 2014, then two more in 2015 (11%). The control
group saw very high attrition in 2014, missing 60 people (22%). Enumerators also found
a large number of households not found at baseline, such that there were only 16
fewer surveys collected in 2014 than in 2013. The high attrition was due largely to
the fact that these households did not enjoy the same consistent contact that BRAC had
with the TUP group, and the local area lacked infrastructure to easily locate people.
This was exacerbated by the uncertain political situation and early harvest.
Attrition in 2015 was 6.7%, with 19 baseline control households not responding, with
85 households found who were originally identified as control households from the
survey but not in the baseline survey. In order to take advantage of the households
not included in the baseline, the main specification below follows
\cite{banerjee-etal} by setting missing values to zero when controlling for baseline
levels of the dependent variable, and including an indicator for
whether the household was in the baseline.

*** Surveyed households also in baseline                           :noexport:
#+caption: Number of households in sample with baseline survey by group and round
#+name: tab:attrition_balanced_count
|-----------------+---------+---------+---------|
| Balanced Sample |    2013 |    2014 |    2015 |
|-----------------+---------+---------+---------|
| UCT            | 124.000 | 112.000 | 110.000 |
| Control         | 281.000 | 219.000 | 262.000 |
| TUP             | 244.000 | 223.000 | 231.000 |
|-----------------+---------+---------+---------|
| All             | 649.000 | 554.000 | 603.000 |
|-----------------+---------+---------+---------|

*************** Further checks on assignment and samples
#+begin_src python :results output raw table
import pandas as pd
from cfe.df_utils import df_to_orgtbl
import sys
sys.path.append('../analysis')
from check_attrition import D

R=pd.read_stata('../../TUP-data/random_assignment.dta')
R=R.set_index('respid')      
R.index.name = 'idno'

A=pd.DataFrame({'D':D['group'],'R':R['group'],'B':D['Base'],'M':D['Mid'],'E':D['End']})

# Fix issue with variant capitalizations
A.D=A.D.str.title()
A.R=A.R.str.title()

# Examine observations where assignments differ:
print(df_to_orgtbl(A[A['D']!=A['R']]))
#+end_src

#+results:
|        | D       | R          | B         | M         | E         |
|--------+---------+------------+-----------+-----------+-----------|
| 1042.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1106.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1154.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1157.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1186.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1190.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1207.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1217.0 | Control | Cash       | \(1.000\) | \(1.000\) | \(1.000\) |
| 1222.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1248.0 | ---     | High Asset | ---       | ---       | ---       |
| 1252.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 1264.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1285.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1293.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 1295.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1300.0 | Control | Cash       | \(0.000\) | \(0.000\) | \(1.000\) |
| 1305.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1311.0 | Control | ---        | \(0.000\) | \(1.000\) | \(0.000\) |
| 1314.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1315.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1316.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1320.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1338.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1348.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1350.0 | Control | ---        | \(0.000\) | \(1.000\) | \(0.000\) |
| 1352.0 | Control | High Asset | \(1.000\) | \(1.000\) | \(1.000\) |
| 1368.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1403.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 1421.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1436.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1453.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1476.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1480.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1486.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1496.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1502.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1505.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1507.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1548.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1561.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1591.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1619.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1642.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 1667.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1679.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1690.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1702.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1703.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1730.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1731.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1744.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1747.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1748.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1749.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1750.0 | Control | ---        | \(1.000\) | \(0.000\) | \(0.000\) |
| 1755.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1773.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1780.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1792.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1797.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1802.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1813.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1814.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1818.0 | Control | Low Asset  | \(1.000\) | \(1.000\) | \(1.000\) |
| 1826.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1878.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 1892.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1894.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1910.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1915.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1922.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1927.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1950.0 | Control | ---        | \(0.000\) | \(1.000\) | \(0.000\) |
| 1956.0 | Control | Low Asset  | \(1.000\) | \(1.000\) | \(1.000\) |
| 1967.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1988.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1991.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1994.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1995.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2002.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2012.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2024.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2030.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2032.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 2039.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2047.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2051.0 | Control | High Asset | \(1.000\) | \(1.000\) | \(1.000\) |
| 2061.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2066.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2071.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2077.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2079.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2082.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2094.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 2103.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2106.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2107.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2114.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2115.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2139.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2160.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2205.0 | Control | ---        | \(1.000\) | \(0.000\) | \(0.000\) |
| 2214.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2225.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2226.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 2232.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2234.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2238.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2239.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2250.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2260.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2261.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 2267.0 | Control | Low Asset  | \(1.000\) | \(1.000\) | \(1.000\) |
| 2268.0 | Control | ---        | \(1.000\) | \(0.000\) | \(0.000\) |
| 2269.0 | Control | ---        | \(1.000\) | \(0.000\) | \(0.000\) |
| 2270.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |

We see seven cases in which there's some discrepancy in the assignment, and 107 cases in which the 
dataframe =D= has a household assigned to "Control" but where
=random_assignment= has no assignment recorded. 

# same as above but with locations.csv
#+begin_src python :results output raw table
import pandas as pd
from cfe.df_utils import df_to_orgtbl
import sys
sys.path.append('../analysis')
from check_attrition import D

R=pd.read_csv('../../TUP-data/csv/locations.csv')
R=R.set_index('RespID')      
R.index.name = 'idno'
R.replace(to_replace='Gift', value='Control', inplace=True)

D.replace(to_replace = 'Low Asset', value = 'Tup', inplace=True)
D.replace(to_replace = 'High Asset', value = 'Tup', inplace=True)


A=pd.DataFrame({'D':D['group'],'R':R['Group'],'B':D['Base'],'M':D['Mid'],'E':D['End']})

# Fix issue with variant capitalizations
A.D=A.D.str.title()
A.R=A.R.str.title()

# Examine observations where assignments differ:
print(df_to_orgtbl(A[A['D']!=A['R']]))
#+end_src

#+RESULTS:
|      | D       | R       | B         | M         | E         |
|------+---------+---------+-----------+-----------+-----------|
| 1157 | Control | Tup     | \(1.000\) | \(0.000\) | \(1.000\) |
| 1248 | ---     | Tup     | ---       | ---       | ---       |
| 1462 | ---     | Control | ---       | ---       | ---       |
| 1647 | ---     | Control | ---       | ---       | ---       |
| 1763 | ---     | Control | ---       | ---       | ---       |
| 2131 | Tup     | Control | \(0.000\) | \(1.000\) | \(1.000\) |
| 2204 | ---     | Control | ---       | ---       | ---       |

# same as above but with checklist_20150602.csv
#+begin_src python :results output raw table
import pandas as pd
from cfe.df_utils import df_to_orgtbl
import sys
sys.path.append('../analysis')
from check_attrition import D

R=pd.read_csv( "../../ElliottTUP/endline/checklists/checklist_20150602.csv")
R=R.set_index('RespID')      
R.index.name = 'idno'
R.replace(to_replace='Gift', value='Control', inplace=True)
R.replace(to_replace='First', value='Cash', inplace=True)
R.replace(to_replace='Second', value='Cash', inplace=True)

D.replace(to_replace = 'Low Asset', value = 'Tup', inplace=True)
D.replace(to_replace = 'High Asset', value = 'Tup', inplace=True)


A=pd.DataFrame({'D':D['group'],'R':R['Give'],'B':D['Base'],'M':D['Mid'],'E':D['End']})

# Fix issue with variant capitalizations
A.D=A.D.str.title()
A.R=A.R.str.title()

# Examine observations where assignments differ:
print(df_to_orgtbl(A[A['D']!=A['R']]))
#+end_src

#+RESULTS:
|      | D       | R       | B         | M         | E         |
|------+---------+---------+-----------+-----------+-----------|
| 1157 | Control | Tup     | \(1.000\) | \(0.000\) | \(1.000\) |
| 1248 | ---     | Tup     | ---       | ---       | ---       |
| 1462 | ---     | Control | ---       | ---       | ---       |
| 1647 | ---     | Control | ---       | ---       | ---       |
| 1763 | ---     | Control | ---       | ---       | ---       |
| 2131 | Tup     | Control | \(0.000\) | \(1.000\) | \(1.000\) |
| 2204 | ---     | Control | ---       | ---       | ---       |

# same as above but with locations.csv as R and asset_assign.csv as D
#+begin_src python :results output raw table
import pandas as pd
from cfe.df_utils import df_to_orgtbl

R=pd.read_csv('../../TUP-data/csv/locations.csv')
R=R.set_index('RespID')      
R.index.name = 'idno'
R.replace(to_replace='Gift', value='Control', inplace=True)

D = pd.read_csv('../../TUP-data/csv/asset_assign.csv')
D=D.set_index('respid')      
D.index.name = 'idno'
D.replace(to_replace = 'TUP-high asset', value = 'Tup', inplace=True)


A=pd.DataFrame({'D':D['group'],'R':R['Group']})

# Fix issue with variant capitalizations
A.D=A.D.str.title()
A.R=A.R.str.title()

# Examine observations where assignments differ:
print(df_to_orgtbl(A[A['D']!=A['R']]))
#+end_src

#+RESULTS:
| idno | D   | R       |
|------+-----+---------|
| 1042 | --- | Control |
| 1106 | --- | Control |
| 1154 | --- | Control |
| 1157 | --- | Tup     |
| 1186 | --- | Control |
| 1190 | --- | Control |
| 1207 | --- | Control |
| 1222 | --- | Control |
| 1252 | --- | Control |
| 1264 | --- | Control |
| 1285 | --- | Control |
| 1293 | --- | Control |
| 1295 | --- | Control |
| 1305 | --- | Control |
| 1311 | --- | Control |
| 1314 | --- | Control |
| 1315 | --- | Control |
| 1316 | --- | Control |
| 1320 | --- | Control |
| 1338 | --- | Control |
| 1348 | --- | Control |
| 1350 | --- | Control |
| 1368 | --- | Control |
| 1403 | --- | Control |
| 1421 | --- | Control |
| 1436 | --- | Control |
| 1453 | --- | Control |
| 1462 | --- | Control |
| 1476 | --- | Control |
| 1480 | --- | Control |
| 1486 | --- | Control |
| 1496 | --- | Control |
| 1502 | --- | Control |
| 1505 | --- | Control |
| 1507 | --- | Control |
| 1548 | --- | Control |
| 1561 | --- | Control |
| 1591 | --- | Control |
| 1619 | --- | Control |
| 1642 | --- | Control |
| 1647 | --- | Control |
| 1667 | --- | Control |
| 1679 | --- | Control |
| 1690 | --- | Control |
| 1702 | --- | Control |
| 1703 | --- | Control |
| 1730 | --- | Control |
| 1731 | --- | Control |
| 1744 | --- | Control |
| 1747 | --- | Control |
| 1748 | --- | Control |
| 1749 | --- | Control |
| 1750 | --- | Control |
| 1755 | --- | Control |
| 1763 | --- | Control |
| 1773 | --- | Control |
| 1780 | --- | Control |
| 1792 | --- | Control |
| 1797 | --- | Control |
| 1802 | --- | Control |
| 1813 | --- | Control |
| 1814 | --- | Control |
| 1826 | --- | Control |
| 1878 | --- | Control |
| 1892 | --- | Control |
| 1894 | --- | Control |
| 1910 | --- | Control |
| 1915 | --- | Control |
| 1922 | --- | Control |
| 1927 | --- | Control |
| 1950 | --- | Control |
| 1967 | --- | Control |
| 1988 | --- | Control |
| 1991 | --- | Control |
| 1994 | --- | Control |
| 1995 | --- | Control |
| 2002 | --- | Control |
| 2012 | --- | Control |
| 2024 | --- | Control |
| 2030 | --- | Control |
| 2032 | --- | Control |
| 2039 | --- | Control |
| 2047 | --- | Control |
| 2061 | --- | Control |
| 2066 | --- | Control |
| 2071 | --- | Control |
| 2077 | --- | Control |
| 2079 | --- | Control |
| 2082 | --- | Control |
| 2094 | --- | Control |
| 2103 | --- | Control |
| 2106 | --- | Control |
| 2107 | --- | Control |
| 2114 | --- | Control |
| 2115 | --- | Control |
| 2131 | Tup | Control |
| 2139 | --- | Control |
| 2160 | --- | Control |
| 2204 | --- | Control |
| 2205 | --- | Control |
| 2214 | --- | Control |
| 2225 | --- | Control |
| 2226 | --- | Control |
| 2232 | --- | Control |
| 2234 | --- | Control |
| 2238 | --- | Control |
| 2239 | --- | Control |
| 2250 | --- | Control |
| 2260 | --- | Control |
| 2261 | --- | Control |
| 2268 | --- | Control |
| 2269 | --- | Control |
| 2270 | --- | Control |

# same as above but with master_assignment.csv as R and TUP_full.dta (group) as D
#+begin_src python :results output raw table
import pandas as pd
from cfe.df_utils import df_to_orgtbl

R=pd.read_csv('../../TUP-report/documents/master_assignment.csv')
R=R.set_index('RespID')      
R.index.name = 'idno'

D = pd.read_stata('../../TUP-data/TUP_full.dta')
D=D.set_index('idno')      
D.replace(to_replace = 'Low Asset', value = 'Tup', inplace=True)
D.replace(to_replace = 'High Asset', value = 'Tup', inplace=True)


A=pd.DataFrame({'D':D['group'],'R':R['Group']})

# Fix issue with variant capitalizations
A.D=A.D.str.title()
A.R=A.R.str.title()

# Examine observations where assignments differ:
print(df_to_orgtbl(A[A['D']!=A['R']]))
#+end_src

#+RESULTS:
|   idno | D       | R       |
|--------+---------+---------|
| 1248.0 | ---     | Tup     |
| 1359.0 | Control | ---     |
| 1462.0 | ---     | Control |
| 1484.0 | Control | ---     |
| 1553.0 | Control | ---     |
| 1647.0 | ---     | Control |
| 1763.0 | ---     | Control |
| 1960.0 | Control | ---     |
| 2142.0 | Control | ---     |
| 2174.0 | Control | ---     |
| 2204.0 | ---     | Control |

# same as above but with master_assignment.csv as R and TUP_full.dta (group_b) as D
#+begin_src python :results output raw table
import pandas as pd
from cfe.df_utils import df_to_orgtbl

R=pd.read_csv('../../TUP-report/documents/master_assignment.csv')
R=R.set_index('RespID')      
R.index.name = 'idno'

D = pd.read_stata('../../TUP-data/TUP_full.dta')
D=D.set_index('idno')      
D.replace(to_replace = 'Low Asset', value = 'Tup', inplace=True)
D.replace(to_replace = 'High Asset', value = 'Tup', inplace=True)


A=pd.DataFrame({'D':D['group_b'],'R':R['Group']})

# Fix issue with variant capitalizations
A.D=A.D.str.title()
A.R=A.R.str.title()

# Examine observations where assignments differ:
print(df_to_orgtbl(A[A['D']!=A['R']]))
#+end_src

#+RESULTS:
|   idno | D   | R       |
|--------+-----+---------|
| 1042.0 |     | Control |
| 1106.0 |     | Control |
| 1151.0 |     | Control |
| 1154.0 |     | Control |
| 1157.0 |     | Control |
| 1186.0 |     | Control |
| 1187.0 |     | Tup     |
| 1190.0 |     | Control |
| 1207.0 |     | Control |
| 1222.0 |     | Control |
| 1248.0 | --- | Tup     |
| 1252.0 |     | Control |
| 1264.0 |     | Control |
| 1285.0 |     | Control |
| 1293.0 |     | Control |
| 1295.0 |     | Control |
| 1300.0 |     | Control |
| 1305.0 |     | Control |
| 1311.0 |     | Control |
| 1314.0 |     | Control |
| 1315.0 |     | Control |
| 1316.0 |     | Control |
| 1318.0 |     | Control |
| 1320.0 |     | Control |
| 1338.0 |     | Control |
| 1344.0 |     | Control |
| 1348.0 |     | Control |
| 1350.0 |     | Control |
| 1359.0 |     | ---     |
| 1368.0 |     | Control |
| 1403.0 |     | Control |
| 1421.0 |     | Control |
| 1436.0 |     | Control |
| 1453.0 |     | Control |
| 1462.0 | --- | Control |
| 1472.0 |     | Control |
| 1476.0 |     | Control |
| 1480.0 |     | Control |
| 1484.0 |     | ---     |
| 1486.0 |     | Control |
| 1496.0 |     | Control |
| 1502.0 |     | Control |
| 1505.0 |     | Control |
| 1507.0 |     | Control |
| 1548.0 |     | Control |
| 1553.0 |     | ---     |
| 1561.0 |     | Control |
| 1591.0 |     | Control |
| 1619.0 |     | Control |
| 1642.0 |     | Control |
| 1647.0 | --- | Control |
| 1667.0 |     | Control |
| 1679.0 |     | Control |
| 1690.0 |     | Control |
| 1702.0 |     | Control |
| 1703.0 |     | Control |
| 1730.0 |     | Control |
| 1731.0 |     | Control |
| 1744.0 |     | Control |
| 1747.0 |     | Control |
| 1748.0 |     | Control |
| 1749.0 |     | Control |
| 1750.0 |     | Control |
| 1755.0 |     | Control |
| 1758.0 |     | Tup     |
| 1760.0 |     | Control |
| 1763.0 | --- | Control |
| 1773.0 |     | Control |
| 1780.0 |     | Control |
| 1792.0 |     | Control |
| 1797.0 |     | Control |
| 1802.0 |     | Control |
| 1807.0 |     | Cash    |
| 1813.0 |     | Control |
| 1814.0 |     | Control |
| 1825.0 |     | Control |
| 1826.0 |     | Control |
| 1878.0 |     | Control |
| 1892.0 |     | Control |
| 1894.0 |     | Control |
| 1910.0 |     | Control |
| 1915.0 |     | Control |
| 1922.0 |     | Control |
| 1927.0 |     | Control |
| 1950.0 |     | Control |
| 1960.0 |     | ---     |
| 1966.0 |     | Control |
| 1967.0 |     | Control |
| 1985.0 |     | Control |
| 1988.0 |     | Control |
| 1991.0 |     | Control |
| 1994.0 |     | Control |
| 1995.0 |     | Control |
| 2002.0 |     | Control |
| 2012.0 |     | Control |
| 2024.0 |     | Control |
| 2030.0 |     | Control |
| 2032.0 |     | Control |
| 2039.0 |     | Control |
| 2047.0 |     | Control |
| 2061.0 |     | Control |
| 2066.0 |     | Control |
| 2071.0 |     | Control |
| 2077.0 |     | Control |
| 2079.0 |     | Control |
| 2082.0 |     | Control |
| 2094.0 |     | Control |
| 2103.0 |     | Control |
| 2106.0 |     | Control |
| 2107.0 |     | Control |
| 2114.0 |     | Control |
| 2115.0 |     | Control |
| 2119.0 |     | Tup     |
| 2131.0 |     | Tup     |
| 2134.0 |     | Control |
| 2139.0 |     | Control |
| 2142.0 |     | ---     |
| 2160.0 |     | Control |
| 2174.0 |     | ---     |
| 2204.0 | --- | Control |
| 2205.0 |     | Control |
| 2212.0 |     | Control |
| 2214.0 |     | Control |
| 2225.0 |     | Control |
| 2226.0 |     | Control |
| 2232.0 |     | Control |
| 2234.0 |     | Control |
| 2238.0 |     | Control |
| 2239.0 |     | Control |
| 2250.0 |     | Control |
| 2252.0 |     | Tup     |
| 2260.0 |     | Control |
| 2261.0 |     | Control |
| 2268.0 |     | Control |
| 2269.0 |     | Control |
| 2270.0 |     | Control |


*************** END

*** Recreating the attrition tables                         :ignore:noexport:

We get the same results as in the original attrition tables.

# using master_assignment.csv
#+name: master_assignment_attrition 
#+begin_src python :exports none  :results output table raw labels=[] :colnames no :tangle master_assignment_attrition.py
data_baseline  = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
data_midline = "../../TUP-data/Midline/TUP_midline.dta"
data_endline = "../../TUP-data/Endline/TUP_endline.dta"
import pandas as pd
import numpy as np
from cfe.df_utils import df_to_orgtbl

df_b = pd.read_stata(data_baseline) 
df_a = pd.read_csv(data_assignment)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)


print("""#+caption: Total number of households in sample by group and round.""")
# Initial assignment (from Census)
dassign = df_a.Group.value_counts(ascending=True)
# change "Cash" to "UCT"
dassign.rename(index={'Cash':'UCT'}, inplace=True)
dassign['All'] = dassign.sum()

# baseline (2013)
# merge baseline data id number and group column from master assignment csv based on respondent id/id number
df = pd.merge(df_b[ ['idno'] ], df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")

# change "Gift" to "Control"
df.replace(to_replace='Gift', value='Control', inplace=True)

# change "Cash" to "UCT"
df.replace(to_replace='Cash', value='UCT', inplace=True)

# count the number in each treatment group
bygroupdf = df.groupby(['Group']).count()
d2013 = {'UCT': bygroupdf.at['UCT','idno'],'Control': bygroupdf.at['Control','idno'],'TUP': bygroupdf.at['TUP','idno'],'All': df.Group.notna().sum()}

df_b = df

# midline (2014)
# merge midline data id number and group column from random assignment data based on respondent id/id number
df = pd.merge(df_m[ ['idno'] ], df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")

# change "Gift" to "Control"
df.replace(to_replace='Gift', value='Control', inplace=True)

# change "Cash" to "UCT"
df.replace(to_replace='Cash', value='UCT', inplace=True)

# count the number in each treatment group
bygroupdf = df.groupby(['Group']).count()
d2014 = {'UCT': bygroupdf.at['UCT','idno'],'Control': bygroupdf.at['Control','idno'],'TUP': bygroupdf.at['TUP','idno'],'All': df.Group.notna().sum()}

df_m = df

# endline (2015)
# merge endline data id number and group column from random assignment data based on respondent id/id number
df = pd.merge(df_e[ ['idno'] ], df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")

# change "Gift" to "Control"
df.replace(to_replace='Gift', value='Control', inplace=True)

# change "Cash" to "UCT"
df.replace(to_replace='Cash', value='UCT', inplace=True)

# count the number in each treatment group
bygroupdf = df.groupby(['Group']).count()
d2015 = {'UCT': bygroupdf.at['UCT','idno'],'Control': bygroupdf.at['Control','idno'],'TUP': bygroupdf.at['TUP','idno'],'All': df.Group.notna().sum()}

df_e = df

# make the table for total number of households in sample by group and round
d = {'Assigned': dassign, '2013': d2013, '2014': d2014, '2015': d2015}
table = pd.DataFrame(data = d) 

newdf = df_to_orgtbl(table, float_fmt = '%d')
print(newdf)


print("""#+caption: Number of households in sample with baseline survey by group and round""")


# find the intersection of people who had surveys at both baseline and midline
df_b_m = pd.merge(df_b[ ['idno'] ], df_m[ ['idno', 'Group'] ], how='inner', left_on="idno", right_on="idno")
bygroupdf_b_m = df_b_m.groupby(['Group']).count()

# find the intersection of people who had surveys at both baseline and endline
df_b_e = pd.merge(df_b[ ['idno'] ], df_e[ ['idno', 'Group'] ], how='inner', left_on="idno", right_on="idno")
bygroupdf_b_e = df_b_e.groupby(['Group']).count()

# count the number in each treatment group
dd2013 = d2013
dd2014 = {'UCT': bygroupdf_b_m.at['UCT','idno'],'Control': bygroupdf_b_m.at['Control','idno'],'TUP': bygroupdf_b_m.at['TUP','idno'],'All': df_b_m.Group.notna().sum()}
dd2015 = {'UCT': bygroupdf_b_e.at['UCT','idno'],'Control': bygroupdf_b_e.at['Control','idno'],'TUP': bygroupdf_b_e.at['TUP','idno'],'All': df_b_e.Group.notna().sum()}

# make the table for number of households in sample with baseline survey by group and round
dd = {'Assigned': dassign, '2013': dd2013, '2014': dd2014, '2015': dd2015}
table2 = pd.DataFrame(data = dd) 

newdf2 = df_to_orgtbl(table2, float_fmt = '%d')
print(newdf2)

#+end_src

#+RESULTS: master_assignment_attrition
#+caption: Total number of households in sample by group and round.
|         | Assigned | 2013 | 2014 | 2015 |
|---------+----------+------+------+------|
| All     |      755 |  649 |  606 |  700 |
| Control |      380 |  281 |  265 |  353 |
| TUP     |      250 |  244 |  228 |  236 |
| UCT     |      125 |  124 |  113 |  111 |

#+caption: Number of households in sample with baseline survey by group and round
|         | Assigned | 2013 | 2014 | 2015 |
|---------+----------+------+------+------|
| All     |      755 |  649 |  554 |  603 |
| Control |      380 |  281 |  219 |  262 |
| TUP     |      250 |  244 |  223 |  231 |
| UCT     |      125 |  124 |  112 |  110 |



*** Selection Regression                                             :ignore:

One reasonable question is whether there's /differential/ selection
by treatment group.  To this end we regress a dummy variable for a
successful interview for each household in each round on a set of
round and treatment group fixed effects.  \Tab{selection_regression}
reports the results.  The probability of an eligible household being
interviewed in 2013 was 79%, falling to 74% in 2014, and rising to 86%
in 2015.  The coefficients associated with TUP and UCT in the table
can be interpreted as the difference in the probability of a household
in one of these treatment groups being interviewed relative to the
control group.  These are not significantly different from each other,
but a household in either of the TUP and UCT groups
has a probability of being interviewed is roughly 0.14 greater than a
household in control group.  This difference seems large enough to
merit further investigation.

#+name: selection_regression
#+begin_src python :exports none  :results output table raw :dir ../ :colnames no :tangle selection_regression.py
import pandas as pd
import numpy as np
from cfe.df_utils import df_to_orgtbl
from cfe.estimation import ols

data_baseline  = "../TUP-data/data/Baseline/TUP_baseline.dta"
data_assignment  = "../TUP-report/documents/master_assignment.csv"
data_midline = "../TUP-data/Midline/TUP_midline.dta"
data_endline = "../TUP-data/Endline/TUP_endline.dta"

df_b = pd.read_stata(data_baseline) 
df_a = pd.read_csv(data_assignment)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)

DF = {}
# baseline (2013)
# merge baseline data id number and group column from master assignment csv based on respondent id/id number
df = pd.merge(df_b[ ['idno'] ], df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")

# change "Gift" to "Control"
df.replace(to_replace='Gift', value='Control', inplace=True)

# change "Cash" to "UCT"
df.replace(to_replace='Cash', value='UCT', inplace=True)

df.set_index('idno', inplace=True)

DF[2013] = df['Group']

# midline (2014)
# merge midline data id number and group column from random assignment data based on respondent id/id number
df = pd.merge(df_m[ ['idno'] ], df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")

# change "Gift" to "Control"
df.replace(to_replace='Gift', value='Control', inplace=True)

# change "Cash" to "UCT"
df.replace(to_replace='Cash', value='UCT', inplace=True)

df.set_index('idno', inplace=True)

DF[2014] = df['Group']

# endline (2015)
# merge endline data id number and group column from random assignment data based on respondent id/id number
df = pd.merge(df_e[ ['idno'] ], df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")

# change "Gift" to "Control"
df.replace(to_replace='Gift', value='Control', inplace=True)

# change "Cash" to "UCT"
df.replace(to_replace='Cash', value='UCT', inplace=True)

df.set_index('idno', inplace=True)

DF[2015] = df['Group']

df = (~pd.DataFrame(DF).isnull() + 0)
df.columns.name = 'Round'

df = df.stack()
df.name = 'Interviewed'

group = df_a.set_index('RespID')['Group']
group.index.name = 'idno'

df = pd.DataFrame(df).join(group,how='outer')

GroupD = pd.get_dummies(df['Group'])

RoundD = pd.get_dummies(df.reset_index(drop=False)['Round'])
RoundD.index = GroupD.index

X = pd.concat([RoundD,GroupD[['TUP','UCT']]],axis=1)
y = df['Interviewed']

b,se = ols(X,y,return_se=True)

print(df_to_orgtbl(b,sedf=se.T,float_fmt='%5.2f'))
#+end_src

#+name: tab:selection_regression
#+caption: Probability of completed interview, by round and treatment group (Control is the excluded group).
|                 | 2013       | 2014       | 2015       | TUP        | UCT        |
|-----------------+------------+------------+------------+------------+------------|
| Pr(Interviewed) | 0.79^{***} | 0.74^{***} | 0.86^{***} | 0.15^{***} | 0.13^{***} |
|                 | ( 0.00)    | ( 0.00)    | ( 0.00)    | ( 0.01)    | ( 0.01)    |

*** Attrition balance table                                          :ignore:

 One possibility is that a random lack of balance in observable
 household characteristics is what drives differences in the
 probability of being interviewed.  To explore this we next
 ask how those who did not turn up in subsequent rounds differed by a range of
 baseline characteristics.  \Tab{attrition_balance} reports the average level of
 various characteristics in 2013. Then we report the difference in means between
 households that were in and out of the midline or endline surveys.
 Note that the sample here consists not of all eligible households,
 but all households interviewed at baseline.
 
 There are some signficant differences.  We see that on average,
 households interviewed in the midline survey were larger with more
 children and reported larger asset stocks. Households found in 2015
 seemed to have, at baseline, had significantly smaller asset stocks and
 less consumption. This imbalance highlights the need for a
 difference-in-difference or ancova citep:mckenzie12 design.

 #+caption: Means of household baseline characteristics and regression coefficients for whether they were ultimately found at baseline or endline. (Note that this does not consider households found only in 2014 or 2015).
 #+attr_latex: :environment longtable :align lrrrrr
 #+name: tab:attrition_balance
 |-----------------+---------------+----------------+----------------|
 | HH Features     | $Mean_{2013}$ | $\beta_{2014}$ | $\beta_{2015}$ |
 |-----------------+---------------+----------------+----------------|
 | HH size         |         7.223 |        0.595** |          0.428 |
 | # Child         |         3.328 |       0.656*** |          0.423 |
 | Asset Prod.     |       512.822 |        126.360 |       -369.190 |
 | Asset Tot.      |      1494.324 |        361.889 |      -689.174* |
 | Daily Exp       |        25.212 |          1.257 |         -4.150 |
 | Daily Food      |        24.300 |          0.299 |        -4.790* |
 | In Business     |         0.415 |          0.038 |          0.007 |
 | Land Access     |         2.324 |          0.014 |          0.305 |
 | No. Houses      |         2.863 |          0.305 |          0.367 |
 | Cash Savings    |       178.662 |         46.322 |         54.295 |
 |-----------------+---------------+----------------+----------------|
 | Assets          |               |                |                |
 |-----------------+---------------+----------------+----------------|
 | Bed             |       250.534 |         12.649 |        -51.133 |
 | Bicycle         |       102.174 |         11.179 |          4.212 |
 | Mobile          |       101.482 |          6.336 |        -13.028 |
 | Motorcycle      |       481.885 |        213.002 |       -241.819 |
 | Carts           |         2.751 |          1.929 |          2.962 |
 | Cows            |       181.402 |         67.862 |        -89.273 |
 | Smallanimals    |       180.716 |         18.966 |        -79.014 |
 |-----------------+---------------+----------------+----------------|
 | Consumption     |               |                |                |
 |-----------------+---------------+----------------+----------------|
 | Cereals         |         8.882 |         -0.084 |       -3.714** |
 | Beans           |         0.826 |          0.269 |         -0.382 |
 | Ceremonies      |         0.141 |         -0.020 |         -0.038 |
 | Charities       |         0.027 |          0.007 |         -0.001 |
 | Clothesfootwear |         0.663 |         0.180* |         -0.206 |
 | Cosmetics       |         0.668 |          0.005 |          0.229 |
 | Dowry           |         1.263 |          0.755 |         -0.399 |
 | Egg             |         1.069 |         -0.005 |          0.106 |
 | Fish            |         2.417 |         -0.132 |          0.036 |
 | Fruit           |         0.656 |          0.009 |         -0.151 |
 | Fuel            |         0.733 |          0.105 |         -0.049 |
 | Meat            |         3.981 |          0.254 |          0.300 |
 | Other           |           0.0 |          0.000 |          0.000 |
 | Poultry         |        39.437 |        23.634* |         -2.243 |
 | Salt            |         0.438 |      -0.140*** |         -0.043 |
 | Soap            |         0.475 |        -0.181* |          0.047 |
 | Sugar           |         1.647 |         -0.285 |         -0.020 |
 | Textiles        |         0.165 |          0.010 |          0.011 |
 | Transport       |         0.163 |          0.004 |          0.018 |
 | Tv              |        39.915 |        -16.377 |          0.845 |
 | Utensils        |         0.247 |          0.062 |         -0.023 |
 | Vegetables      |         1.446 |          0.096 |         -0.151 |
 |-----------------+---------------+----------------+----------------|

**** Recreating attrition balance table                            :noexport:

 #+name: attrition_balance
 #+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
data_baseline  = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_midline  = "/Users/beckycardinali/Desktop/TUP-data/Midline/TUP_midline.dta"
data_endline = "/Users/beckycardinali/Desktop/TUP-data/Endline/TUP_endline.dta"
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
import pandas as pd
from scipy import stats
from cfe.df_utils import df_to_orgtbl

df_b = pd.read_stata(data_baseline)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)
df_a = pd.read_csv(data_assignment)

df_m.insert(len(df_m.columns), 'in_midline', 1)
df_e.insert(len(df_e.columns), 'in_endline', 1)

df = pd.merge(df_b, df_a[ ['RespID', 'Group'] ], how='outer', left_on="idno", right_on="RespID")

# merge in midline and endline
df = pd.merge(df, df_m[ ['idno', 'in_midline'] ], how='outer', left_on="idno", right_on="idno")
df = pd.merge(df, df_e[ ['idno', 'in_endline'] ], how='outer', left_on='idno', right_on='idno')

df['in_midline'].fillna(value=0, inplace=True)
df['in_endline'].fillna(value=0, inplace=True)

df_3_days = df[ ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b'] ].transform(lambda x: x/3)

df_month = df[ ['c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b'] ].transform(lambda x: x/30)

df_year = df[ ['c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b'] ].transform(lambda x: x/365)

df_assets = df.get(['asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b',
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b',
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b',
             'asset_val_netITN_b',
             'asset_n_house_b', 'in_business_b', 'child_total_b', 'hh_size_b',
             'Group', 'in_midline', 'in_endline'])

df_1 = df_3_days.join(df_month.join(df_year.join(df_assets)))

means_b = df_1.mean()
means_m = df_1.groupby(['in_midline']).mean()
means_e = df_1.groupby(['in_endline']).mean()
means_b = means_b.T
means_m = means_m.T
means_e = means_e.T

# some are off by 0.001, clothesfootwear numbers differ
d = {'mean_baseline': means_b, 'beta_mid': means_m[1.0] - means_m[0.0], 'beta_end': means_e[1.0] - means_e[0.0]}
means = pd.DataFrame(data=d)

#default axis=0 for dropping rows
means.drop(['in_midline', 'in_endline'], inplace=True)

# so we can add * later
means = means.round(decimals = 3)
means = means.astype(str)

column_list = ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b', \
              'c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b', \
               'c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b', \
              'asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b', \
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b', \
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b', \
             'asset_val_netITN_b', \
             'asset_n_house_b', 'in_business_b', 'child_total_b', 'hh_size_b']

for column in column_list:
    
    pvalue_mid = stats.ttest_ind(df_1[df_1['in_midline'] == 1][column], df_1[df_1['in_midline'] == 0][column], nan_policy='omit').pvalue
    
    if pvalue_mid <= 0.01: 
        means.at[column,'beta_mid'] = means.at[column,'beta_mid'] + '***'
    elif pvalue_mid <= 0.05: 
        means.at[column,'beta_mid'] = means.at[column,'beta_mid'] + '**'
    elif pvalue_mid <= 0.1: 
        means.at[column,'beta_mid'] = means.at[column,'beta_mid'] + '*'
        
    pvalue_end = stats.ttest_ind(df_1[df_1['in_endline'] == 1][column], df_1[df_1['in_endline'] == 0][column], nan_policy='omit').pvalue
    
    if pvalue_end <= 0.01: 
        means.at[column,'beta_end'] = means.at[column,'beta_end'] + '***'
    elif pvalue_end <= 0.05: 
        means.at[column,'beta_end'] = means.at[column,'beta_end'] + '**'
    elif pvalue_end <= 0.1: 
        means.at[column,'beta_end'] = means.at[column,'beta_end'] + '*'

# label the rows using "Better Var Name" in TUP-data/ssudan_survey.org,
# but without (last three days) or (last month) since variables are transformed (i.e. divided by 365 for yearly)
means.rename({'c_meat_b': ' Meat', 'c_fish_b': 'Fish', 
            'c_cereals_b': 'Cereals', 'c_sugar_b': 'Sugar', 
            'c_egg_b': 'Egg', 'c_oil_b': 'Oil', 
            'c_beans_b': 'Beans', 'c_fruit_b': 'Fruit', 
            'c_salt_b': 'Salt', 'c_vegetables_b': 'Vegetables', 
            'c_milk_b': 'Milk', 'c_spices_b': 'Spices', 
            'c_alcohol_b': 'Alcohol', 'c_otherfood_b': 'Other food', 
            'c_fuel_b': 'Cooking fuel', 'c_soap_b': 'Toiletries', 
            'c_transport_b': 'Transportation', 'c_cosmetics_b': 'Cosmetics', 
            'c_entertainment_b': 'Entertainment', 'c_clothesfootwear_b': 'Clothing, footwear', 
            'c_charities_b': 'Charities', 'c_ceremonies_b': 'Rituals/ceremonies', 
            'c_textiles_b': 'Textiles', 'c_utensils_b': 'Utensils',
            'c_dowry_b': 'Dowry', 'c_furniture_b': 'Furniture', 
            'c_other_b': 'Other non-food', 'asset_val_cows_b': 'Large livestock (cows)', 
            'asset_val_smallanimals_b': 'Small livestock', 'asset_val_bicycle_b': 'Bicycle', 
            'asset_val_radio_b': 'Radio', 'asset_val_motorcycle_b': 'Motorcycle', 
            'asset_val_net_b': 'Mosquito net', 'asset_val_poultry_b': 'Poultry', 
            'asset_val_bed_b': 'Bed', 'asset_val_chairtables_b': 'Chair/table', 
            'asset_val_mobile_b': 'Mobile phone', 'asset_val_netITN_b': 'Mosquito net ITN', 
            'asset_n_house_b': '# Houses', 'in_business_b': 'In business', 
            'child_total_b': '# Child', 'hh_size_b':'Household size'}, inplace=True)

newdf = df_to_orgtbl(means, float_fmt = '%.3f')
print(newdf)


 #+end_src

 #+RESULTS: attrition_balance
 |                        | mean_baseline | beta_mid | beta_end |
 |------------------------+---------------+----------+----------|
 | # Houses               |         2.863 |    0.305 |    0.367 |
 | Bed                    |       250.535 |   12.649 |  -51.133 |
 | Bicycle                |       102.174 |   11.179 |    4.212 |
 | Chair/table            |       196.436 |   -0.303 |  -37.177 |
 | Large livestock (cows) |       181.402 |   67.862 |  -89.273 |
 | Mobile phone           |       101.482 |    6.336 |  -13.028 |
 | Motorcycle             |       481.886 |  213.002 | -241.819 |
 | Mosquito net ITN       |         8.504 |   -1.777 |    0.449 |
 | Mosquito net           |        19.462 |    0.332 |    2.814 |
 | Poultry                |        39.438 |  23.634* |   -2.243 |
 | Radio                  |        53.046 |   -6.333 | -35.093* |
 | Small livestock        |       180.716 |   18.966 |  -79.014 |
 | Alcohol                |          0.04 |    0.005 |   -0.004 |
 | Beans                  |         0.826 |    0.269 |   -0.382 |
 | Cereals                |         8.883 |   -0.084 | -3.714** |
 | Rituals/ceremonies     |         0.139 |   -0.019 |   -0.038 |
 | Charities              |         0.027 |    0.007 |   -0.001 |
 | Clothing, footwear     |         0.655 |   0.177* |   -0.203 |
 | Cosmetics              |         0.668 |    0.005 |    0.229 |
 | Dowry                  |         1.247 |    0.745 |   -0.394 |
 | Egg                    |         1.069 |   -0.005 |    0.106 |
 | Entertainment          |         0.074 |    0.021 |   0.051* |
 | Fish                   |         2.418 |   -0.132 |    0.036 |
 | Fruit                  |         0.657 |    0.009 |   -0.151 |
 | Cooking fuel           |         0.733 |    0.105 |   -0.049 |
 | Furniture              |         0.199 |   -0.023 |     0.02 |
 | Meat                   |         3.982 |    0.254 |      0.3 |
 | Milk                   |          1.15 |    0.283 |    0.239 |
 | Oil                    |         1.288 |    0.037 |  -0.532* |
 | Other non-food         |           0.0 |      0.0 |      0.0 |
 | Other food             |           0.0 |      0.0 |      0.0 |
 | Salt                   |         0.439 | -0.14*** |   -0.043 |
 | Toiletries             |         0.475 |  -0.181* |    0.047 |
 | Spices                 |         0.231 |    0.024 |   -0.048 |
 | Sugar                  |         1.648 |   -0.285 |    -0.02 |
 | Textiles               |         0.163 |     0.01 |    0.011 |
 | Transportation         |         0.164 |    0.004 |    0.018 |
 | Utensils               |         0.245 |    0.061 |   -0.023 |
 | Vegetables             |         1.447 |    0.096 |   -0.151 |
 | # Child                |         3.328 | 0.656*** |    0.423 |
 | Household size         |         7.224 |  0.595** |    0.428 |
 | In business            |         0.415 |    0.038 |    0.007 |


**** Creating a new attrition balance table with only nonzero values counting toward the mean :noexport:

 The following table shows the means of log expenditures. 

 #+name: attrition_balance_nonzero
 #+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
data_baseline  = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_midline  = "/Users/beckycardinali/Desktop/TUP-data/Midline/TUP_midline.dta"
data_endline = "/Users/beckycardinali/Desktop/TUP-data/Endline/TUP_endline.dta"
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
import pandas as pd
import numpy as np
from scipy import stats
from cfe.df_utils import df_to_orgtbl

df_b = pd.read_stata(data_baseline)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)
df_a = pd.read_csv(data_assignment)

df_m.insert(len(df_m.columns), 'in_midline', 1)
df_e.insert(len(df_e.columns), 'in_endline', 1)

df = pd.merge(df_b, df_a[ ['RespID', 'Group'] ], how='outer', left_on="idno", right_on="RespID")

# merge in midline and endline
df = pd.merge(df, df_m[ ['idno', 'in_midline'] ], how='outer', left_on="idno", right_on="idno")
df = pd.merge(df, df_e[ ['idno', 'in_endline'] ], how='outer', left_on='idno', right_on='idno')

df['in_midline'].fillna(value=0, inplace=True)
df['in_endline'].fillna(value=0, inplace=True)

df_3_days = df[ ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b'] ]

df_month = df[ ['c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b'] ]

df_year = df[ ['c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b'] ]

df_assets = df.get(['asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b',
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b',
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b',
             'asset_val_netITN_b',
             'asset_n_house_b', 'child_total_b', 'hh_size_b'])

df_1 = df_3_days.join(df_month.join(df_year.join(df_assets)))
df_1.replace(to_replace = 0, value = float("NaN") , inplace = True)

# take the logs of expenditure for all variables except dummy variables
df_1 = df_1.transform(lambda x: np.log(x))

# add in group assignment and the dummy variables that need 0 as one of their values
df_1 = df_1.join(df[ ['in_midline', 'in_endline', 'in_business_b', 'Group'] ])

means_b = df_1.mean()
means_m = df_1.groupby(['in_midline']).mean()
means_e = df_1.groupby(['in_endline']).mean()
means_b = means_b.T
means_m = means_m.T
means_e = means_e.T

d = {'mean_baseline': means_b, 'beta_mid': means_m[1.0] - means_m[0.0], 'beta_end': means_e[1.0] - means_e[0.0]}
means = pd.DataFrame(data=d)

#default axis=0 for dropping rows
means.drop(['in_midline', 'in_endline'], inplace=True)

# so we can add * later
means = means.round(decimals = 3)
means = means.astype(str)

column_list = ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b', \
              'c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b', \
               'c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b', \
              'asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b', \
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b', \
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b', \
             'asset_val_netITN_b', \
             'asset_n_house_b', 'in_business_b', 'child_total_b', 'hh_size_b']

for column in column_list:
    
    pvalue_mid = stats.ttest_ind(df_1[df_1['in_midline'] == 1][column], df_1[df_1['in_midline'] == 0][column], nan_policy='omit').pvalue
    
    if pvalue_mid <= 0.01: 
        means.at[column,'beta_mid'] = means.at[column,'beta_mid'] + '***'
    elif pvalue_mid <= 0.05: 
        means.at[column,'beta_mid'] = means.at[column,'beta_mid'] + '**'
    elif pvalue_mid <= 0.1: 
        means.at[column,'beta_mid'] = means.at[column,'beta_mid'] + '*'
        
    pvalue_end = stats.ttest_ind(df_1[df_1['in_endline'] == 1][column], df_1[df_1['in_endline'] == 0][column], nan_policy='omit').pvalue
    
    if pvalue_end <= 0.01: 
        means.at[column,'beta_end'] = means.at[column,'beta_end'] + '***'
    elif pvalue_end <= 0.05: 
        means.at[column,'beta_end'] = means.at[column,'beta_end'] + '**'
    elif pvalue_end <= 0.1: 
        means.at[column,'beta_end'] = means.at[column,'beta_end'] + '*'

# label the rows using "Better Var Name" in TUP-data/ssudan_survey.org
means.rename({'c_meat_b': ' Meat (last three days)', 'c_fish_b': 'Fish (last three days)', 
            'c_cereals_b': 'Cereals (last three days)', 'c_sugar_b': 'Sugar (last three days)', 
            'c_egg_b': 'Egg (last three days)', 'c_oil_b': 'Oil (last three days)', 
            'c_beans_b': 'Beans (last three days)', 'c_fruit_b': 'Fruit (last three days)', 
            'c_salt_b': 'Salt (last three days)', 'c_vegetables_b': 'Vegetables (last three days)', 
            'c_milk_b': 'Milk (last three days)', 'c_spices_b': 'Spices (last three days)', 
            'c_alcohol_b': 'Alcohol (last three days)', 'c_otherfood_b': 'Other food (last three days)', 
            'c_fuel_b': 'Cooking fuel (last month)', 'c_soap_b': 'Toiletries (last month)', 
            'c_transport_b': 'Transportation (last month)', 'c_cosmetics_b': 'Cosmetics (last month)', 
            'c_entertainment_b': 'Entertainment (last month)', 'c_clothesfootwear_b': 'Clothing, footwear (last year)', 
            'c_charities_b': 'Charities (last year)', 'c_ceremonies_b': 'Rituals/ceremonies (last year)', 
            'c_textiles_b': 'Textiles (last year)', 'c_utensils_b': 'Utensils (last year)',
            'c_dowry_b': 'Dowry (last year)', 'c_furniture_b': 'Furniture (last year)', 
            'c_other_b': 'Other non-food (last year)', 'asset_val_cows_b': 'Large livestock (cows)', 
            'asset_val_smallanimals_b': 'Small livestock', 'asset_val_bicycle_b': 'Bicycle', 
            'asset_val_radio_b': 'Radio', 'asset_val_motorcycle_b': 'Motorcycle', 
            'asset_val_net_b': 'Mosquito net', 'asset_val_poultry_b': 'Poultry', 
            'asset_val_bed_b': 'Bed', 'asset_val_chairtables_b': 'Chair/table', 
            'asset_val_mobile_b': 'Mobile phone', 'asset_val_netITN_b': 'Mosquito net ITN', 
            'asset_n_house_b': '# Houses', 'in_business_b': 'In business', 
            'child_total_b': '# Child', 'hh_size_b':'Household size'}, inplace=True)

newdf = df_to_orgtbl(means, float_fmt = '%.3f')

print(newdf)


 #+end_src

 #+RESULTS: attrition_balance_nonzero
 |                                | mean_baseline |  beta_mid | beta_end |
 |--------------------------------+---------------+-----------+----------|
 | # Houses                       |         1.054 |     0.083 |    0.071 |
 | Bed                            |         5.328 |     0.028 | -0.408** |
 | Bicycle                        |         5.642 |     0.106 |    0.015 |
 | Chair/table                    |         4.902 |     0.179 |   -0.224 |
 | Large livestock (cows)         |         7.428 |     0.686 |    0.133 |
 | Mobile phone                   |         4.862 |     0.075 |   -0.174 |
 | Motorcycle                     |         7.223 |     0.918 |    0.787 |
 | Mosquito net ITN               |         3.151 |     0.034 |   -0.324 |
 | Mosquito net                   |         3.096 |     0.009 |    0.056 |
 | Poultry                        |         4.582 |     0.259 |    0.111 |
 | Radio                          |         4.314 |    -0.036 |   -0.354 |
 | Small livestock                |          6.29 |     0.351 |    0.081 |
 | Alcohol (last three days)      |         1.367 |    -0.272 |   0.633* |
 | Beans (last three days)        |         1.746 |      0.3* |   -0.272 |
 | Cereals (last three days)      |         3.003 |    -0.052 |  -0.255* |
 | Rituals/ceremonies (last year) |          4.54 |    -0.139 |   -0.085 |
 | Charities (last year)          |         3.237 |    -0.241 |    0.076 |
 | Clothing, footwear (last year) |         4.973 |    0.234* |   -0.169 |
 | Cosmetics (last month)         |          2.68 |     -0.03 |    0.062 |
 | Dowry (last year)              |         6.103 |     0.596 |    0.122 |
 | Egg (last three days)          |         1.797 |    -0.081 |    0.076 |
 | Entertainment (last month)     |         2.033 |    -0.116 |    0.418 |
 | Fish (last three days)         |         2.069 |       0.0 |    0.028 |
 | Fruit (last three days)        |         1.337 |     0.094 |   -0.114 |
 | Cooking fuel (last month)      |         2.912 |     0.033 |   -0.257 |
 | Furniture (last year)          |         4.273 |     0.041 |   -0.338 |
 | Meat (last three days)         |         2.859 |     0.133 |    0.105 |
 | Milk (last three days)         |         2.461 |     0.193 |    0.175 |
 | Oil (last three days)          |         0.896 |    -0.054 |   -0.216 |
 | Other non-food (last year)     |           nan |       nan |      nan |
 | Other food (last three days)   |           nan |       nan |      nan |
 | Salt (last three days)         |          0.18 | -0.154*** |   -0.053 |
 | Toiletries (last month)        |         2.276 |     -0.17 |   -0.086 |
 | Spices (last three days)       |         0.792 |     -0.19 |   -0.156 |
 | Sugar (last three days)        |         1.317 |    -0.092 |   -0.116 |
 | Textiles (last year)           |         4.137 |     0.099 |   -0.109 |
 | Transportation (last month)    |         2.417 |    -0.132 |    0.022 |
 | Utensils (last year)           |         4.347 |      0.21 |    -0.24 |
 | Vegetables (last three days)   |         1.514 |     0.095 |   -0.047 |
 | # Child                        |         1.142 |  0.219*** |   0.173* |
 | Household size                 |         1.919 |   0.081** |    0.062 |
 | In business                    |         0.415 |     0.038 |    0.007 |




* Empirical Strategy

For the main panel (excluding the high-frequency mobile surveys), we
estimate a single model using interactions between time effects and
group assignment, as well as baseline values of the outcome variable
where available; this is 
\begin{equation*}
Y_{it} =\sum_{t=2014}^{2015}\delta_{t}+\beta_{t}^{UCT}I_{t}*UCT_{it}+\beta_{t}^{TUP}I_{t}*TUP_{it}+\gamma Y_{i,2013}+\epsilon_{i}
\end{equation*}
where $\delta_{t}$ are time fixed effects and $I_{t}$ is an indicator if the year
/t/, and $Y_{it}$ is an outcome of interest for household /i/ in year /t/. We take
the interactions of TUP assignment with 2014 and 2015 indicators as the treatment
effects at 6--8 and 15--17 months respectively. The analagous interactions with the
UCT group offer a second baseline and a 12-month treatment effect, respectively.
Since those transfers happened after the midline survey, its interaction with /2014/
acts as a placebo; there is no /ex ante/ reason to expect that they were different
from the rest of the control group at that point. Given the slight difference in
timing, we report a t-test of the hypothesis \(\beta_{TUP,t}-\beta_{UCT,2015}=0\) for
both \(t \in {2014,2015}\). Since the difference in timing is smaller, we consider
\(\beta_{TUP,2015}-\beta_{UCT,2015}=0\) to be the preferred hypothesis, but examine
and report \(\beta_{TUP,2014}-\beta_{UCT,2015}=0\) as well.
 
For the supplementary analysis of the high-frequency panel, we
estimate a separate model, since the underlying data is so
different. A constant parameter takes the place of the fixed
effects. We include 2013 levels as a covariate where possible, and a
dummy variable where not. 

*** High-frequency data                                            :noexport:
 Since we collect expenditures on only ten
consumption items, we report not only the total value of spending on
those goods, but also a more theoretically grounded measure described
in \cite{collins-ligon17}, which uses the composition of expenditures
to derive the marginal utility of expenditures for each household.  We
chose ten relatively demand-elastic items specifically for this
purpose, as those will tend to be the most responsive to changes in
welfare.

*************** TODO
        Include a scatter plot of df[10items].sum(1) vs df[AllItems].sum(1) to speak
        to whether that basket tracks "total" spending.
*************** END 

#+name: scatter_plot
#+begin_src python :results output
import matplotlib. pyplot as plt
import pandas as pd

# 10 consumption items from mobile survey: treatment of sick household
# members, school fee (see s28 and s29 and s210?), food for household
# consumption, clothes, goods to resell with some profit, small size
# livestock, medium size livestock, construction/renovation of houses
# for living, construction/renovation of shop premises, hair dressing


# need to get the correct data for 10items and allitems from mobile survey 
data = [[1,2], [3,4], [5,6], [2, 12], [5, -4]]
tenitems = pd.DataFrame(data, columns = ['Respondent 1', 'Respondent 2'])
data = [[6,5], [4,3], [2,1], [3, 1], [6, -3]]
AllItems = pd.DataFrame(data, columns = ['Respondent 1', 'Respondent 2'])

X = tenitems.sum(1)
Y = AllItems.sum(1)

X = X.values.tolist()
Y = Y.values.tolist()

plt.scatter(X, Y)
plt.xlabel('Sum of 10 Items')
plt.ylabel('Sum of All Items')
plt.savefig("../figures/scatter.png")
#plt.show() # can take this out later

#+end_src

#+results: scatter_plot

#+RESULTS:

#+Caption: Scatter plot of basket vs total spending
#+NAME: fig:Income_group
[[../figures/scatter.png]]

* Results

** Consumption

The first outcome of interest we consider is household consumption, defined as the
market value of goods or services used by the household.  A sizable basket of goods
were included in the survey module. These are separated into three categories: Food
items (with a 3-day recall window), non-durables (a 30-day recall window), and
durables and large expenditures (a one-year recall window). This is perhaps the most
appropriate measure of the welfare or poverty of a household in our survey. 

The results for several important consumption measures are presented
in \Tab{consumption}.  We do not know about prices for each good in
this time, though we can say that inflation was as high as 100%
between 2014 and 2015. We take the sum of all consumption and
expenditure questions together as a measure of welfare. In light of
the fact that we have data on an incomplete basket, we also follow
cite:collins-ligon17, which describes a method for deriving
treatment effects on a structurally estimated index of households'
marginal utility, which we include here as \(\log\lambda_{it}\).

We measured the effect of cash on consumption 12 months after the fact, where we find
a statistically significant increase of 18 SSP per day, representing a roughly 16%
increase over the control-group average. We first measured the consumption effect of
the TUP program 6-9 months after the asset transfers, where we found a similarly
sized effect of 18SSP. However we find no evidence of a consumption effect 18-21
months after the transfers (and reject equality with the 12-month cash results). This
result leaves open the question of whether the cash treatment had a persistent effect
on consumption, or whether the short-term effect found in 2015 is similarly
temporary. It was this question that motivated the collection of an additional five
rounds of data over a 6-month period in late 2015 and early 2016, in which we asked
about ten items, five food and five non-food. In Table \Tab{tab:mobile_consumption},
we consider the average treatment effect on households sampled for these phone
interviews, both for \(\log\lambda_{it}\) and for total consumption of this basket of
goods. We find that, consistent with the TUP program's results in 2015, all evidence
of an effect seem to be gone by 18th months after the transfer date.

These results are consistent with a story in which either sort of transfer has a
short-term consumption effect, but with the result fading in the year after. In
either group, the increase in total consumption appears to be driven mainly by
increased food consumption, with smaller effects on non-food consumption goods and
durables. As such, there is no evidence that the share of food consumed falls, as
might be predicted by Engel's law.

\newpage

#+name: consumption_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<consumption_analysis>>
return tab
#+end_src

#+RESULTS: consumption_results with Treatment Group Fixed Effects
|----------------------------------+-------------------+-----------------+------------------+-----------------|
|                                  | Tot               | logTot          | Food             | FoodShr         |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| CTL mean                         | \(115.404\)       | \(4.509^{***}\) | \(38.468\)       | \(0.333^{***}\) |
|                                  | \((78.750)\)      | \((0.756)\)     | \((26.250)\)     | \((0.000)\)     |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| CSH*2014                         | \(-7.838\)        | \(-0.049\)      | \(-2.613\)       | \(0.000\)       |
|                                  | \((5.891)\)       | \((0.058)\)     | \((1.964)\)      | \((0.000)\)     |
| CSH*2015                         | \(12.931^{**}\)   | \(0.104^{*}\)   | \(4.310^{**}\)   | \(-0.000\)      |
|                                  | \((5.811)\)       | \((0.057)\)     | \((1.937)\)      | \((0.000)\)     |
| TUP*2014                         | \(11.000^{**}\)   | \(0.126^{***}\) | \(3.667^{**}\)   | \(0.000^{***}\) |
|                                  | \((4.681)\)       | \((0.046)\)     | \((1.560)\)      | \((0.000)\)     |
| TUP*2015                         | \(-3.410\)        | \(-0.040\)      | \(-1.137\)       | \(0.000^{***}\) |
|                                  | \((4.547)\)       | \((0.045)\)     | \((1.516)\)      | \((0.000)\)     |
| 2014                             | \(76.831^{***}\)  | \(3.931^{***}\) | \(25.610^{***}\) | \(0.278^{***}\) |
|                                  | \((5.318)\)       | \((0.113)\)     | \((1.773)\)      | \((0.000)\)     |
| 2015                             | \(105.702^{***}\) | \(4.175^{***}\) | \(35.234^{***}\) | \(0.278^{***}\) |
|                                  | \((5.001)\)       | \((0.111)\)     | \((1.667)\)      | \((0.000)\)     |
| TUP                              | \(7.590^{**}\)    | \(0.086^{***}\) | \(2.530^{**}\)   | \(0.000^{***}\) |
|                                  | \((3.012)\)       | \((0.030)\)     | \((1.004)\)      | \((0.000)\)     |
| CSH                              | \(5.093\)         | \(0.056\)       | \(1.698\)        | \(0.000^{***}\) |
|                                  | \((3.774)\)       | \((0.037)\)     | \((1.258)\)      | \((0.000)\)     |
| Bsln2013                         | \(0.081^{**}\)    | \(0.073^{***}\) | \(0.081^{**}\)   | \(0.167^{***}\) |
|                                  | \((0.038)\)       | \((0.026)\)     | \((0.038)\)      | \((0.000)\)     |
| Bsln NA                          | \(20.521^{***}\)  | \(0.447^{***}\) | \(6.840^{***}\)  | \(0.056^{***}\) |
|                                  | \((6.964)\)       | \((0.121)\)     | \((2.321)\)      | \((0.000)\)     |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | \(-1.931\)        | \(0.022\)       | \(-0.644\)       | \(0.000^{**}\)  |
|                                  | \((8.377)\)       | \((0.083)\)     | \((2.792)\)      | \((0.000)\)     |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | \(-16.341^{***}\) | \(-0.145^{**}\) | \(-5.447^{***}\) | \(0.000^{***}\) |
|                                  | \((6.057)\)       | \((0.060)\)     | \((2.019)\)      | \((0.000)\)     |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| F-stat                           | \(10.142\)        | \(8.131\)       | \(10.142\)       | \(-212.981\)    |
| N                                | \(1291.000\)      | \(1291.000\)    | \(1291.000\)     | \(1291.000\)    |
|----------------------------------+-------------------+-----------------+------------------+-----------------|

#+caption: Average treatment effects by Group-Year, controlling for baseline levels.
#+attr_latex: :environment longtable :align lrrrrrrr
#+name: tab:consumption
|----------------------------------+-------------------+-----------------+------------------+-----------------|
|                                  | Tot               | logTot          | Food             | FoodShr         |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| CTL mean                         | \(115.404\)       | \(4.509^{***}\) | \(38.468\)       | \(0.333^{***}\) |
|                                  | \((78.750)\)      | \((0.756)\)     | \((26.250)\)     | \((0.000)\)     |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| CSH*2014                         | \(-2.745\)        | \(0.007\)       | \(-0.915\)       | \(0.000^{***}\) |
|                                  | \((8.008)\)       | \((0.079)\)     | \((2.669)\)      | \((0.000)\)     |
| CSH*2015                         | \(18.023^{**}\)   | \(0.160^{**}\)  | \(6.008^{**}\)   | \(-0.000\)      |
|                                  | \((7.831)\)       | \((0.077)\)     | \((2.610)\)      | \((0.000)\)     |
| TUP*2014                         | \(18.590^{***}\)  | \(0.212^{***}\) | \(6.197^{***}\)  | \(0.000^{***}\) |
|                                  | \((6.426)\)       | \((0.063)\)     | \((2.142)\)      | \((0.000)\)     |
| TUP*2015                         | \(4.179\)         | \(0.045\)       | \(1.393\)        | \(0.000^{***}\) |
|                                  | \((6.130)\)       | \((0.060)\)     | \((2.043)\)      | \((0.000)\)     |
| 2014                             | \(76.831^{***}\)  | \(3.931^{***}\) | \(25.610^{***}\) | \(0.278^{***}\) |
|                                  | \((5.318)\)       | \((0.113)\)     | \((1.773)\)      | \((0.000)\)     |
| 2015                             | \(105.702^{***}\) | \(4.175^{***}\) | \(35.234^{***}\) | \(0.278^{***}\) |
|                                  | \((5.001)\)       | \((0.111)\)     | \((1.667)\)      | \((0.000)\)     |
| Bsln2013                         | \(0.081^{**}\)    | \(0.073^{***}\) | \(0.081^{**}\)   | \(0.167^{***}\) |
|                                  | \((0.038)\)       | \((0.026)\)     | \((0.038)\)      | \((0.000)\)     |
| Bsln NA                          | \(20.521^{***}\)  | \(0.447^{***}\) | \(6.840^{***}\)  | \(0.056^{***}\) |
|                                  | \((6.964)\)       | \((0.121)\)     | \((2.321)\)      | \((0.000)\)     |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | \(0.566\)         | \(0.052\)       | \(0.189\)        | \(0.000^{***}\) |
|                                  | \((9.994)\)       | \((0.098)\)     | \((3.331)\)      | \((0.000)\)     |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | \(-13.844^{*}\)   | \(-0.115\)      | \(-4.615^{*}\)   | \(0.000^{***}\) |
|                                  | \((8.125)\)       | \((0.080)\)     | \((2.708)\)      | \((0.000)\)     |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| F-stat                           | \(10.142\)        | \(8.131\)       | \(10.142\)       | \(-210.422\)    |
| N                                | \(1291.000\)      | \(1291.000\)    | \(1291.000\)     | \(1291.000\)    |
|----------------------------------+-------------------+-----------------+------------------+-----------------|

#+name: mobile_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<mobile_analysis>>
return mtab
#+end_src

\newpage

#+caption: Average treatment effects using mobile data collection (results are robust to controlling for baseline levels)
#+attr_latex: :environment longtable :align lrrrrr
#+name: tab:mobile_consumption
|---------------------------+--------------------+----------------+---------------|
|                           | $\log\lambda_{it}$ | Tot            | logTot        |
|---------------------------+--------------------+----------------+---------------|
| CTL mean                  | $-0.018$           | $30.851$       | $3.158^{***}$ |
|                           | $(1.001)$          | $(27.768)$     | $(0.734)$     |
|---------------------------+--------------------+----------------+---------------|
| TUP                       | $0.023$            | $-0.624$       | $-0.011$      |
|                           | $(0.041)$          | $(1.152)$      | $(0.030)$     |
| CSH                       | $0.056$            | $0.776$        | $0.028$       |
|                           | $(0.052)$          | $(1.459)$      | $(0.038)$     |
| const                     | $-0.018$           | $30.851^{***}$ | $3.158^{***}$ |
|                           | $(0.027)$          | $(0.753)$      | $(0.020)$     |
|---------------------------+--------------------+----------------+---------------|
| $\beta^{TUP}-\beta^{CSH}$ | $-0.033$           | $-1.399$       | $-0.039$      |
|                           | $(0.055)$          | $(1.524)$      | $(0.040)$     |
|---------------------------+--------------------+----------------+---------------|
| F-stat                    | $0.584$            | $0.434$        | $0.475$       |
| N                         | $2877.000$         | $2878.000$     | $2878.000$    |
|---------------------------+--------------------+----------------+---------------|


*** Recreating consumption tables

#+name: consumption_tables
#+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
data_baseline = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_midline  = "../../TUP-data/Midline/TUP_midline.dta"
data_endline = "../../TUP-data/Endline/TUP_endline.dta"

import statsmodels.api as sm
import pandas as pd
import numpy as np
from scipy import stats
from cfe.df_utils import df_to_orgtbl

df_b = pd.read_stata(data_baseline)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)
df_a = pd.read_csv(data_assignment)

df_b.set_index('idno', inplace=True)
df_m.set_index('idno', inplace=True)
df_e.set_index('idno', inplace=True)
df_a.set_index('RespID', inplace=True)

# missing s20_6m_b and s20_7m_b in monthly for baseline - should be othermonth b
df_b.insert(len(df_b.columns), 'c_othermonth_b', df_b[ ['s20_6m_b', 's20_7m_b'] ].sum(axis=1))

# find variables that start with "c_" and remove "c_" and remove "_b", "_m", or "_e"
c2013 = df_b.filter(regex='^c_').rename(columns=lambda x: x[2:-2])
c2014 = df_m.filter(regex='^c_').rename(columns=lambda x: x[2:-2])
c2015 = df_e.filter(regex='^c_').rename(columns=lambda x: x[2:-2])

c2013['Year'] = '2013'
c2014['Year'] = '2014'
c2015['Year'] = '2015'

# missing durables from monthly for midline in previous code - added it in
# doesn't make sense to ask for clothing and footwear in total and then women's clothes, children's clothes, school uniforms, shoes in midline and endline
# ^ and they don't always sum up - change this?
# #some not asked for in baseline

food = ['cereals', 'maize', 'sorghum', 'millet', 'potato', 'sweetpotato', 'rice', 'bread', 'beans', 'oil', 'salt', 'sugar', 'meat', 'livestock', 'poultry', 'fish', 'egg', 'nuts', 'milk', 'vegetables', 'fruit', 'tea', 'spices', 'alcohol', 'otherfood']
month = ['fuel', 'medicine', 'airtime', 'cosmetics', 'soap', 'transport', 'entertainment', 'childcare', 'tobacco', 'batteries', 'durables', 'church', 'othermonth']    
year = ['clothesfootwear', 'womensclothes', 'childrensclothes', 'shoes', 'homeimprovement', 'utensils', 'furniture', 'textiles', 'ceremonies', 'funerals', 'charities', 'dowry', 'other']    

# make the sum variables
# in previous they made aggregate variables wrong?
# Make aggregate variables
#  C["Food"]   = C.filter(items=food).sum(axis=1).replace(0,np.nan)
#  C["Month"]   = C.filter(items=food).sum(axis=1)
#  C["Year"]   = C.filter(items=food).sum(axis=1)
#  C["Tot"]    = C[["Food","Month","Year"]].sum(axis=1).replace(0,np.nan)
# should not all filter by items=food. makes sense why food share is 0.333 because it's 1 over itself * 3

outcomes = ["Total", "logTot", "Food", "FoodShr", "Monthly", "Yearly"]

# not replacing 0s with np.nan as was done in previous code - for total, seems to be 4 observations that have 0
for var in (c2013, c2014, c2015):
    
    var["Food"] = var.filter(items=food).sum(axis=1)
    var["Monthly"] = var.filter(items=month).sum(axis=1)
    var["Yearly"] = var.filter(items=year).sum(axis=1)
    var["Total"] = var[ ["Food", "Monthly", "Yearly"] ].sum(axis=1)
    # make sure not taking the log of 0 or dividing by 0
    var["logTot"] = var["Total"].replace(0, np.nan).transform(lambda x: np.log(x))
    var["FoodShr"] = var["Food"]/var["Total"].replace(0, np.nan)
    
    # set up to make year dummies
    var['2013'] = 0
    var['2014'] = 0
    var['2015'] = 0
    
    # set up baseline2013 variables
    for outcome in outcomes: 
        var[outcome+'2013'] = c2013[outcome]

# make year dummies
c2013['2013'].replace(to_replace = 0, value = 1, inplace=True)
c2014['2014'].replace(to_replace = 0, value = 1, inplace=True)
c2015['2015'].replace(to_replace = 0, value = 1, inplace=True)

df2 = pd.concat((c2013, c2014, c2015))
df = df2.join((df_a[ ['Group'] ].join(pd.get_dummies(df_a['Group']))))   

# make interaction terms
df.insert(len(df.columns), 'TUP*2013', df['2013']*df['TUP'])
df.insert(len(df.columns), 'TUP*2014', df['2014']*df['TUP'])
df.insert(len(df.columns), 'TUP*2015', df['2015']*df['TUP'])
df.insert(len(df.columns), 'UCT*2013', df['2013']*df['UCT'])
df.insert(len(df.columns), 'UCT*2014', df['2014']*df['UCT'])
df.insert(len(df.columns), 'UCT*2015', df['2015']*df['UCT'])

models_ols_FE = {}
models_ols_no_FE = {}
d_FE = {}
d_no_FE = {}
baseline_na = True

controls_FE = ['TUP*2014', 'TUP*2015', 'UCT*2014', 'UCT*2015', '2014', '2015', 'TUP', 'UCT']
controls_no_FE = ['TUP*2014', 'TUP*2015', 'UCT*2014', 'UCT*2015', '2014', '2015']
# from above, outcomes = ["Total", "logTot", "Food", "FoodShr", "Monthly", "Yearly"]

# remove observations from 2013
df = df[df['Year'] != '2013']

for outcome in outcomes: 
    temp_df = df[ [outcome, outcome + "2013", 'Control'] + controls_FE]
    temp_controls_FE = controls_FE
    temp_controls_no_FE = controls_no_FE
    
    if baseline_na==True:
        
        # indicator for whether outcome in 2013 is na, and cast it to be an integer
        temp_df["Baseline_NAN"] = temp_df[outcome + "2013"].isnull().apply(int)
        
        # code missing values of the baseline variable as 0
        temp_df[outcome + "2013"].fillna(0,inplace=True)
        
        temp_controls_FE = temp_controls_FE + ['Baseline_NAN']
        temp_controls_no_FE = temp_controls_no_FE + ['Baseline_NAN']
    
    temp_controls_FE = temp_controls_FE + [outcome+"2013"]
    temp_controls_no_FE = temp_controls_no_FE + [outcome+"2013"]
    temp_df = temp_df.dropna()
    
    models_ols_FE[outcome] = sm.OLS(temp_df[outcome], temp_df[temp_controls_FE]).fit()
    models_ols_no_FE[outcome] = sm.OLS(temp_df[outcome], temp_df[temp_controls_no_FE]).fit()

    # for computing the mean of the control group in 2015
    controlgroup = temp_df[temp_df['Control'] == 1]
    
    """for table with Fixed Effects"""
    
    # for computing the difference in TUP and Cash coefficients
    beta_cash_TUP2014 = models_ols_FE[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP2014 = models_ols_FE[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP2014 <= 0.01: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '***'
    elif pvalue_cash_TUP2014 <= 0.05: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '**'
    elif pvalue_cash_TUP2014 <= 0.1: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '*'
        
    beta_cash_TUP2015 = models_ols_FE[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP2015 = models_ols_FE[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP2015 <= 0.01: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '***'
    elif pvalue_cash_TUP2015 <= 0.05: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '**'
    elif pvalue_cash_TUP2015 <= 0.1: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '*'
    
    
    d_FE[outcome] = [controlgroup[controlgroup['2015'] == 1][outcome].mean(),
                  controlgroup[controlgroup['2015'] == 1][outcome].std(),
                  round(models_ols_FE[outcome].params[0], 3), models_ols_FE[outcome].bse[0],
                  round(models_ols_FE[outcome].params[1], 3), models_ols_FE[outcome].bse[1], 
                  round(models_ols_FE[outcome].params[2], 3), models_ols_FE[outcome].bse[2],
                  round(models_ols_FE[outcome].params[3], 3), models_ols_FE[outcome].bse[3],
                  round(models_ols_FE[outcome].params[4], 3), models_ols_FE[outcome].bse[4],
                  round(models_ols_FE[outcome].params[5], 3), models_ols_FE[outcome].bse[5],
                  round(models_ols_FE[outcome].params[6], 3), models_ols_FE[outcome].bse[6],
                  round(models_ols_FE[outcome].params[7], 3), models_ols_FE[outcome].bse[7],
                  round(models_ols_FE[outcome].params[8], 3), models_ols_FE[outcome].bse[8],
                  round(models_ols_FE[outcome].params[9], 3), models_ols_FE[outcome].bse[9],
                  models_ols_FE[outcome].fvalue, models_ols_FE[outcome].nobs, 
                  beta_cash_TUP2014, beta_cash_TUP2015]
    
    for x in range(0,10):
        # indexes account for standard errors being in the final table
        # e.g. second coefficient is beneath standard error so change d[outcome][2] not d[outcome][1] and same for next coefficients
        if models_ols_FE[outcome].pvalues[x] <= 0.01: 
            d_FE[outcome][x*2 + 2] = str(d_FE[outcome][x*2 + 2]) + '***'
        elif models_ols_FE[outcome].pvalues[x] <= 0.05: 
            d_FE[outcome][x*2 + 2] = str(d_FE[outcome][x*2 + 2]) + '**'
        elif models_ols_FE[outcome].pvalues[x] <= 0.1:
            d_FE[outcome][x*2 + 2] = str(d_FE[outcome][x*2 + 2]) + '*'
    
    """for table WITHOUT Fixed Effects"""
    
    # for computing the difference in TUP and Cash coefficients
    beta_cash_TUP2014 = models_ols_no_FE[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP2014 = models_ols_no_FE[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP2014 <= 0.01: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '***'
    elif pvalue_cash_TUP2014 <= 0.05: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '**'
    elif pvalue_cash_TUP2014 <= 0.1: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '*'
        
    beta_cash_TUP2015 = models_ols_no_FE[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP2015 = models_ols_no_FE[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP2015 <= 0.01: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '***'
    elif pvalue_cash_TUP2015 <= 0.05: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '**'
    elif pvalue_cash_TUP2015 <= 0.1: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '*'
    
    d_no_FE[outcome] = [controlgroup[controlgroup['2015'] == 1][outcome].mean(),
                  controlgroup[controlgroup['2015'] == 1][outcome].std(),
                  round(models_ols_no_FE[outcome].params[0], 3), models_ols_no_FE[outcome].bse[0],
                  round(models_ols_no_FE[outcome].params[1], 3), models_ols_no_FE[outcome].bse[1], 
                  round(models_ols_no_FE[outcome].params[2], 3), models_ols_no_FE[outcome].bse[2],
                  round(models_ols_no_FE[outcome].params[3], 3), models_ols_no_FE[outcome].bse[3],
                  round(models_ols_no_FE[outcome].params[4], 3), models_ols_no_FE[outcome].bse[4],
                  round(models_ols_no_FE[outcome].params[5], 3), models_ols_no_FE[outcome].bse[5],
                  round(models_ols_no_FE[outcome].params[6], 3), models_ols_no_FE[outcome].bse[6],
                  round(models_ols_no_FE[outcome].params[7], 3), models_ols_no_FE[outcome].bse[7],
                  models_ols_no_FE[outcome].fvalue, models_ols_no_FE[outcome].nobs,
                  beta_cash_TUP2014, beta_cash_TUP2015]
    
    for x in range(0,8):
        # indexes account for standard errors being in the final table
        # e.g. second coefficient is beneath standard error so change d[outcome][2] not d[outcome][1] and same for next coefficients
        if models_ols_no_FE[outcome].pvalues[x] <= 0.01: 
            d_no_FE[outcome][x*2 + 2] = str(d_no_FE[outcome][x*2 + 2]) + '***'
        elif models_ols_no_FE[outcome].pvalues[x] <= 0.05: 
            d_no_FE[outcome][x*2 + 2] = str(d_no_FE[outcome][x*2 + 2]) + '**'
        elif models_ols_no_FE[outcome].pvalues[x] <= 0.1:
            d_no_FE[outcome][x*2 + 2] = str(d_no_FE[outcome][x*2 + 2]) + '*'


    del temp_df 

final_FE = pd.DataFrame(data=d_FE)
final_FE.rename(index = {0: "CTL mean", 1: " ", 2: "TUP*2014", 3: "  ", 
                      4: "TUP*2015", 5: "   ", 6: "UCT*2014", 7: "    ",
                      8: "UCT*2015", 9: "     ", 10: "2014", 11: "      ",
                      12: "2015", 13: "       ", 14: "TUP", 15: "        ",
                      16: "UCT", 17: "         ", 18: "Bsln NA", 19: "          ",
                      20: "Bsln2013", 21: "           ", 22: "F-stat", 23: "N", 
                      24: "Beta_TUP_2014-Beta_Cash_2015", 25: "Beta_TUP_2015-Beta_Cash_2015"}, inplace=True)

final_no_FE = pd.DataFrame(data=d_no_FE)
final_no_FE.rename(index = {0: "CTL mean", 1: " ", 2: "TUP*2014", 3: "  ", 
                      4: "TUP*2015", 5: "   ", 6: "UCT*2014", 7: "    ",
                      8: "UCT*2015", 9: "     ", 10: "2014", 11: "      ",
                      12: "2015", 13: "       ", 14: "Bsln NA", 15: "        ",
                      16: "Bsln2013", 17: "         ", 18: "F-stat", 19: "N",
                      20: "Beta_TUP_2014-Beta_Cash_2015", 21: "Beta_TUP_2015-Beta_Cash_2015"}, inplace=True)    
    
newdf_FE = df_to_orgtbl(final_FE, float_fmt = '%.3f')
print("""#+caption:consumption results WITH Tgroup Fixed Effects""")
print(newdf_FE)

newdf_no_FE = df_to_orgtbl(final_no_FE, float_fmt = '%.3f')
print("""#+caption:consumption results WITHOUT Tgroup Fixed Effects""")
print(newdf_no_FE)


#+end_src

#+RESULTS: consumption_tables
#+caption:consumption results WITH Tgroup Fixed Effects
|  |Total|   logTot|   Food|   FoodShr|   Monthly|   Yearly  |
|-
| CTL mean  | 1455.907 | 6.754 | 115.547 | 0.153 | 288.598 | 1051.762 |
|    | 2073.394 | 1.005 | 79.634 | 0.137 | 311.211 | 1939.705 |
| TUP*2014  | 333.466** | 0.167** | 11.561** | -0.003 | 27.903 | 293.11** |
|     | 137.919 | 0.066 | 4.708 | 0.008 | 22.680 | 128.066 |
| TUP*2015  | -152.399 | -0.060 | -4.330 | 0.002 | 3.427 | -150.793 |
|      | 133.641 | 0.064 | 4.562 | 0.008 | 21.977 | 124.094 |
| UCT*2014  | -107.001 | -0.055 | -8.112 | 0.005 | -1.299 | -97.707 |
|       | 172.781 | 0.082 | 5.898 | 0.010 | 28.414 | 160.438 |
| UCT*2015  | 72.251 | 0.089 | 12.481** | -0.007 | 35.555 | 24.732 |
|        | 170.760 | 0.081 | 5.829 | 0.010 | 28.083 | 158.563 |
| 2014  | 1125.402*** | 6.269*** | 77.477*** | 0.13*** | 259.412*** | 808.297*** |
|         | 136.529 | 0.156 | 5.327 | 0.008 | 23.506 | 125.900 |
| 2015  | 1405.175*** | 6.42*** | 106.761*** | 0.153*** | 294.587*** | 1022.193*** |
|          | 125.303 | 0.153 | 4.995 | 0.008 | 21.914 | 115.484 |
| TUP  | 181.067** | 0.106** | 7.231** | -0.001 | 31.33** | 142.317* |
|           | 88.895 | 0.042 | 3.037 | 0.005 | 14.619 | 82.544 |
| UCT  | -34.749 | 0.033 | 4.369 | -0.001 | 34.257* | -72.976 |
|            | 111.380 | 0.053 | 3.802 | 0.007 | 18.320 | 103.425 |
| Bsln NA  | 41.717 | 0.4** | 18.848*** | 0.008 | -13.309 | 17.329 |
|             | 194.528 | 0.168 | 7.119 | 0.012 | 32.807 | 180.056 |
| Bsln2013  | 0.051* | 0.05** | 0.072* | -0.016 | -0.053 | 0.037 |
|              | 0.030 | 0.023 | 0.038 | 0.017 | 0.140 | 0.029 |
| F-stat  | 2.072 | 2.811 | 9.872 | 2.119 | 1.550 | 1.673 |
| N  | 1306.000 | 1305.000 | 1306.000 | 1305.000 | 1306.000 | 1306.000 |
| Beta_TUP_2014-Beta_Cash_2015  | 261.215 | 0.078 | -0.921 | 0.003 | -7.652 | 268.378 |
| Beta_TUP_2015-Beta_Cash_2015  | -224.650 | -0.149* | -16.811*** | 0.009 | -32.129 | -175.525 |

#+caption:consumption results WITHOUT Tgroup Fixed Effects
|  |Total|   logTot|   Food|   FoodShr|   Monthly|   Yearly  |
|-
| CTL mean  | 1455.907 | 6.754 | 115.547 | 0.153 | 288.598 | 1051.762 |
|    | 2073.394 | 1.005 | 79.634 | 0.137 | 311.211 | 1939.705 |
| TUP*2014  | 514.533*** | 0.273*** | 18.791*** | -0.005 | 59.232* | 435.427** |
|     | 189.706 | 0.090 | 6.479 | 0.011 | 31.196 | 176.152 |
| TUP*2015  | 28.668 | 0.046 | 2.900 | 0.001 | 34.756 | -8.476 |
|      | 180.285 | 0.086 | 6.156 | 0.011 | 29.648 | 167.407 |
| UCT*2014  | -141.750 | -0.022 | -3.744 | 0.004 | 32.958 | -170.683 |
|       | 235.283 | 0.112 | 8.032 | 0.014 | 38.695 | 218.476 |
| UCT*2015  | 37.502 | 0.122 | 16.85** | -0.008 | 69.812* | -48.244 |
|        | 230.815 | 0.110 | 7.880 | 0.014 | 37.963 | 214.330 |
| 2014  | 1125.402*** | 6.269*** | 77.477*** | 0.13*** | 259.412*** | 808.297*** |
|         | 136.529 | 0.156 | 5.327 | 0.008 | 23.506 | 125.900 |
| 2015  | 1405.175*** | 6.42*** | 106.761*** | 0.153*** | 294.587*** | 1022.193*** |
|          | 125.303 | 0.153 | 4.995 | 0.008 | 21.914 | 115.484 |
| Bsln NA  | 41.717 | 0.4** | 18.848*** | 0.008 | -13.309 | 17.329 |
|           | 194.528 | 0.168 | 7.119 | 0.012 | 32.807 | 180.056 |
| Bsln2013  | 0.051* | 0.05** | 0.072* | -0.016 | -0.053 | 0.037 |
|            | 0.030 | 0.023 | 0.038 | 0.017 | 0.140 | 0.029 |
| F-stat  | 2.072 | 2.811 | 9.872 | 2.119 | 1.550 | 1.673 |
| N  | 1306.000 | 1305.000 | 1306.000 | 1305.000 | 1306.000 | 1306.000 |
| Beta_TUP_2014-Beta_Cash_2015  | 477.031 | 0.151 | 1.941 | 0.003 | -10.580 | 483.671* |
| Beta_TUP_2015-Beta_Cash_2015  | -8.834 | -0.076 | -13.95* | 0.009 | -35.056 | 39.768 |


** Food Insecurity

Observed changes in total consumption don't translate into an increase in reported
food security. In each year, we ask how often in a given week the respondent has had
experiences indicative of food insecurity. Included are (from left to right) going a
whole day without eating, going to sleep hungry, being without any food in the house,
eating fewer meals than normal at mealtimes, and limiting portions. We report the
percentage of people who report experiencing each in a typical week, as well as a
standardized composite z-score using all of these questions. Coefficients and
standard errors are both small relative to average levels and statistically
insignificant. The first column also aggregates these values as a weighted average
with inverse covariance weighting, yielding an index we call Food Insecurity. Again,
we find no systematic difference in food security.

#+name: foodsecure_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<foodsecure_analysis>>
return tab
#+end_src

|----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------|
|                                  | Food Insecurity | Whole Day       | Hungry          | No Food         | Few Meals       | Portions        |
|----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------|
| CTL mean                         | \(0.024\)       | \(0.790^{*}\)   | \(0.797^{**}\)  | \(0.716\)       | \(0.679\)       | \(0.638\)       |
|                                  | \((0.713)\)     | \((0.408)\)     | \((0.403)\)     | \((0.452)\)     | \((0.467)\)     | \((0.481)\)     |
|----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------|
| CSH*2014                         | \(0.044\)       | \(0.004\)       | \(0.040\)       | \(0.011\)       | \(0.032\)       | \(0.004\)       |
|                                  | \((0.076)\)     | \((0.041)\)     | \((0.039)\)     | \((0.043)\)     | \((0.050)\)     | \((0.051)\)     |
| CSH*2015                         | \(-0.069\)      | \(-0.063\)      | \(-0.032\)      | \(0.014\)       | \(0.003\)       | \(0.034\)       |
|                                  | \((0.074)\)     | \((0.040)\)     | \((0.039)\)     | \((0.042)\)     | \((0.049)\)     | \((0.050)\)     |
| TUP*2014                         | \(0.058\)       | \(0.017\)       | \(0.052^{*}\)   | \(0.024\)       | \(-0.009\)      | \(-0.012\)      |
|                                  | \((0.061)\)     | \((0.033)\)     | \((0.032)\)     | \((0.035)\)     | \((0.040)\)     | \((0.041)\)     |
| TUP*2015                         | \(-0.005\)      | \(-0.030\)      | \(0.006\)       | \(0.030\)       | \(-0.054\)      | \(0.016\)       |
|                                  | \((0.058)\)     | \((0.031)\)     | \((0.030)\)     | \((0.033)\)     | \((0.038)\)     | \((0.039)\)     |
| 2014                             | \(-0.040\)      | \(0.914^{***}\) | \(0.904^{***}\) | \(0.915^{***}\) | \(0.834^{***}\) | \(0.779^{***}\) |
|                                  | \((0.043)\)     | \((0.024)\)     | \((0.023)\)     | \((0.026)\)     | \((0.033)\)     | \((0.031)\)     |
| 2015                             | \(-0.002\)      | \(0.784^{***}\) | \(0.793^{***}\) | \(0.736^{***}\) | \(0.703^{***}\) | \(0.609^{***}\) |
|                                  | \((0.038)\)     | \((0.022)\)     | \((0.022)\)     | \((0.024)\)     | \((0.031)\)     | \((0.029)\)     |
| Bsln2013                         | \(-0.081^{**}\) | \(0.002\)       | \(-0.017\)      | \(-0.032\)      | \(-0.056^{**}\) | \(0.023\)       |
|                                  | \((0.034)\)     | \((0.023)\)     | \((0.021)\)     | \((0.023)\)     | \((0.027)\)     | \((0.027)\)     |
| Bsln_NAN                         | \(0.109^{**}\)  | \(0.022\)       | \(0.033\)       | \(-0.038\)      | \(0.019\)       | \(0.083^{**}\)  |
|                                  | \((0.053)\)     | \((0.031)\)     | \((0.032)\)     | \((0.035)\)     | \((0.043)\)     | \((0.041)\)     |
|----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | \(0.128\)       | \(0.080\)       | \(0.085^{*}\)   | \(0.011\)       | \(-0.013\)      | \(-0.046\)      |
|                                  | \((0.095)\)     | \((0.051)\)     | \((0.049)\)     | \((0.054)\)     | \((0.062)\)     | \((0.064)\)     |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | \(0.064\)       | \(0.033\)       | \(0.038\)       | \(0.017\)       | \(-0.057\)      | \(-0.018\)      |
|                                  | \((0.078)\)     | \((0.042)\)     | \((0.040)\)     | \((0.044)\)     | \((0.051)\)     | \((0.052)\)     |
|----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------|
| F-stat                           | \(1.742\)       | \(9.300\)       | \(8.328\)       | \(10.945\)      | \(6.700\)       | \(5.879\)       |
| N                                | \(1255.000\)    | \(1276.000\)    | \(1291.000\)    | \(1287.000\)    | \(1291.000\)    | \(1286.000\)    |
|----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------|



#+name: tab:foodsecure
#+caption: Percentage of respondents reporting a food security problem occurs at least once a week.
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+-------------+--------------+--------------+--------------+--------------+--------------|
|                                  | Z-score     | Whole Day    | Hungry       | No Food      | Fewmeals     | Portions     |
|----------------------------------+-------------+--------------+--------------+--------------+--------------+--------------|
| CTL mean                         | $-0.01$     | $0.21$       | $0.21$       | $0.28$       | $0.32$       | $0.36$       |
|                                  | $( 1.00)$   | $( 0.41)$    | $( 0.40)$    | $( 0.45)$    | $( 0.47)$    | $( 0.48)$    |
|----------------------------------+-------------+--------------+--------------+--------------+--------------+--------------|
| TUP*2014                         | $-0.10$     | $-0.02$      | $-0.05$      | $-0.03$      | $0.01$       | $0.01$       |
|                                  | $( 0.09)$   | $( 0.03)$    | $( 0.03)$    | $( 0.03)$    | $( 0.04)$    | $( 0.04)$    |
| TUP*2015                         | $-0.02$     | $0.03$       | $-0.01$      | $-0.03$      | $0.05$       | $-0.02$      |
|                                  | $( 0.09)$   | $( 0.03)$    | $( 0.03)$    | $( 0.03)$    | $( 0.04)$    | $( 0.04)$    |
| CSH*2014                         | $-0.05$     | $-0.00$      | $-0.04$      | $-0.01$      | $-0.03$      | $-0.00$      |
|                                  | $( 0.11)$   | $( 0.04)$    | $( 0.04)$    | $( 0.04)$    | $( 0.05)$    | $( 0.05)$    |
| CSH*2015                         | $0.03$      | $0.06$       | $0.03$       | $-0.01$      | $-0.00$      | $-0.04$      |
|                                  | $( 0.11)$   | $( 0.04)$    | $( 0.04)$    | $( 0.04)$    | $( 0.05)$    | $( 0.05)$    |
| Bsln2013                         | $0.07^{**}$ | $-0.00$      | $0.02$       | $0.03$       | $0.06^{**}$  | $-0.02$      |
|                                  | $( 0.03)$   | $( 0.02)$    | $( 0.02)$    | $( 0.02)$    | $( 0.03)$    | $( 0.03)$    |
| 2014                             | $0.07$      | $0.09^{***}$ | $0.10^{***}$ | $0.09^{***}$ | $0.17^{***}$ | $0.22^{***}$ |
|                                  | $( 0.06)$   | $( 0.02)$    | $( 0.02)$    | $( 0.03)$    | $( 0.03)$    | $( 0.03)$    |
| 2015                             | $0.03$      | $0.22^{***}$ | $0.21^{***}$ | $0.26^{***}$ | $0.30^{***}$ | $0.39^{***}$ |
|                                  | $( 0.06)$   | $( 0.02)$    | $( 0.02)$    | $( 0.02)$    | $( 0.03)$    | $( 0.03)$    |
| Bsln NA                          | $-0.17^{*}$ | $-0.02$      | $-0.03$      | $0.03$       | $-0.02$      | $-0.08^{*}$  |
|                                  | $( 0.09)$   | $( 0.03)$    | $( 0.03)$    | $( 0.03)$    | $( 0.04)$    | $( 0.04)$    |
|----------------------------------+-------------+--------------+--------------+--------------+--------------+--------------|
| F-stat                           | $1.45$      | $9.34$       | $8.36$       | $10.84$      | $6.70$       | $5.91$       |
| N                                | $1299.00$   | $1282.00$    | $1297.00$    | $1293.00$    | $1297.00$    | $1292.00$    |
|----------------------------------+-------------+--------------+--------------+--------------+--------------+--------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $-0.13$     | $-0.08$      | $-0.08^{*}$  | $-0.01$      | $0.01$       | $0.05$       |
|                                  | $( 0.14)$   | $( 0.05)$    | $( 0.05)$    | $( 0.05)$    | $( 0.06)$    | $( 0.06)$    |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $-0.06$     | $-0.03$      | $-0.04$      | $-0.02$      | $0.06$       | $0.02$       |
|                                  | $( 0.12)$   | $( 0.04)$    | $( 0.04)$    | $( 0.04)$    | $( 0.05)$    | $( 0.05)$    |
|----------------------------------+-------------+--------------+--------------+--------------+--------------+--------------|

*** Recreating food insecurity table

The table looks at whether respondents experienced going a whole
day without eating, limiting portions, etc. in a typical week.  
The z-score is calculated by taking the sum of binary variables 'worried', 
'portions', 'fewmeals', 'nofood', 'hungry', 'wholeday' for each
individual, separately at baseline, midline, and endline. Call that
'fs_sum_m' for an individual's sum at midline. Then their z-score at
midline  would be (fs_sum_m - fs_sum_m.mean())/fs_sum_m.std(). CTL
mean lists the mean and standard deviation of the control group in 2015.

Differences with previous code: 
-correction for what should be considered "weekly" (this code
includes "Everyday", "3-6 times a week", and "1-2 times a week" as
being considered weekly, whereas the previous code left out "1-2 times a week")
-This code adds 'worried' to the table. It uses 'worried' to
calculate the z-score, which  was also used to calculate
the z-score in the previous table.

Next steps: 
-change how the z-score is being calculated because .sum(axis=1)
equates NaN to 0, so some respondents who had NaN for every measure
will still have a z-score, hence the higher N for 'z-score' than for
the other variables
-divide the standard deviation of CTL mean by the sqrt of the number of control
observations to make it comparable with the other point estimates and
standard errors in the table?
-add another column with f statistic for joint hypothesis that all coefficients are equal to 0

#+name: food_insecurity
#+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
data_baseline = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_midline  = "../../TUP-data/Midline/TUP_midline.dta"
data_endline = "../../TUP-data/Endline/TUP_endline.dta"

import statsmodels.api as sm
import pandas as pd
import numpy as np
from cfe.df_utils import df_to_orgtbl

df_b = pd.read_stata(data_baseline)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)
df_a = pd.read_csv(data_assignment)

# set Respondent ID number as the index
df_b.set_index('idno', inplace=True)
df_m.set_index('idno', inplace=True)
df_e.set_index('idno', inplace=True)
df_a.set_index('RespID', inplace=True)

# merge baseline and midline based on idno index
df = pd.merge(df_b[ ['fs_worried_b', 'fs_portions_b', 
                     'fs_fewmeals_b', 'fs_nofood_b', 
                     'fs_hungry_b', 'fs_wholeday_b'] ], 
              df_m[ ['fs_worried_m', 'fs_portions_m', 
                      'fs_fewmeals_m', 'fs_nofood_m', 
                      'fs_hungry_m', 'fs_wholeday_m'] ], 
              how='outer', left_index=True, right_index=True)

# merge baseline/midline and endline based on idno index
df = pd.merge(df, 
              df_e[ ['fs_worried_e', 'fs_portions_e', 
                     'fs_fewmeals_e', 'fs_nofood_e', 
                     'fs_hungry_e', 'fs_wholeday_e'] ], 
              how='outer', left_index=True, right_index=True)

# replace words with their corresponding code number from the survey
df.replace({"Everyday": 1,
            "everyday": 1,
            "3-6 times a week": 2,
            "1-2 times a week": 3,
            "Less than once a week": 4,
            "less than once a week": 4,
            "Never": 5,
            "never": 5}, inplace=True)

# for weekly, replace 4 and 5 with 0, and 1, 2, and 3 with 1
df.replace({1: 1, 2: 1, 3: 1, 4: 0, 5: 0}, inplace=True)

# find columns that start with "fs_" and end with "_b", "_m", or "_e"
# cut off the first 3 characters "fs_" and the last 2 "_b", "_m", or "_e"
fs2013 = df.filter(regex="^fs_.*_b").rename(columns=lambda x: x[3:-2])
fs2014 = df.filter(regex="^fs_.*_m").rename(columns=lambda x: x[3:-2])
fs2015 = df.filter(regex="^fs_.*_e").rename(columns=lambda x: x[3:-2])

fs2013['Year'] = '2013'
fs2014['Year'] = '2014'
fs2015['Year'] = '2015'

# used to calculate the z-score
index_vars = ['worried', 'portions', 'fewmeals', 'nofood', 'hungry', 'wholeday']
outcomes = ['z-score'] + index_vars 

for FS in (fs2013, fs2014, fs2015): 
    fs_sum = FS[index_vars].sum(axis=1)
    FS['z-score'] = (fs_sum - fs_sum.mean())/fs_sum.std()
    
    # set up to make year dummies
    FS.insert(len(FS.columns), '2013', 0)
    FS.insert(len(FS.columns), '2014', 0)
    FS.insert(len(FS.columns), '2015', 0)
    
    # baseline values as 'var'2013
    for var in outcomes: 
        FS[var+'2013'] = fs2013[var]
    
# make year dummmies
fs2013['2013'].replace(to_replace = 0, value = 1, inplace=True)
fs2014['2014'].replace(to_replace = 0, value = 1, inplace=True)
fs2015['2015'].replace(to_replace = 0, value = 1, inplace=True)

df2 = pd.concat((fs2013, fs2014, fs2015))
# make group dummies using 'Group' from master_assignment.csv
df = df2.join((df_a[ ['Group'] ].join(pd.get_dummies(df_a['Group']))))

# make interaction variables
df.insert(len(df.columns), 'TUP*2013', df['2013']*df['TUP'])
df.insert(len(df.columns), 'TUP*2014', df['2014']*df['TUP'])
df.insert(len(df.columns), 'TUP*2015', df['2015']*df['TUP'])
df.insert(len(df.columns), 'UCT*2013', df['2013']*df['UCT'])
df.insert(len(df.columns), 'UCT*2014', df['2014']*df['UCT'])
df.insert(len(df.columns), 'UCT*2015', df['2015']*df['UCT'])

models_ols = {}
d = {}
baseline_na = True

controls = ['TUP*2014', 'TUP*2015', 'UCT*2014', 'UCT*2015', '2014', '2015']
outcomes = ['z-score', 'wholeday', 'hungry', 'nofood', 'fewmeals', 'portions', 'worried']

# remove observations from 2013
df = df[df['Year'] != '2013']

for outcome in outcomes: 
    temp_df = df[ [outcome, outcome + "2013", 'Control'] + controls]
    temp_controls = controls
    
    if baseline_na==True:
        
        # indicator for whether outcome in 2013 is na, and cast it to be an integer
        temp_df["Baseline_NAN"] = temp_df[outcome + "2013"].isnull().apply(int)
        
        # code missing values of the baseline variable as 0
        temp_df[outcome + "2013"].fillna(0,inplace=True)
        
        temp_controls = temp_controls + ['Baseline_NAN']
    
    temp_controls = temp_controls + [outcome+"2013"]
    temp_df = temp_df.dropna()
    
    models_ols[outcome] = sm.OLS(temp_df[outcome], temp_df[temp_controls]).fit()
    
    # for computing the mean of the control group in 2015
    controlgroup = temp_df[temp_df['Control'] == 1]

    # for computing the difference in TUP and Cash (UCT) coefficients
    beta_cash_TUP2014 = models_ols[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP2014 = models_ols[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP2014 <= 0.01: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '***'
    elif pvalue_cash_TUP2014 <= 0.05: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '**'
    elif pvalue_cash_TUP2014 <= 0.1: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '*'
        
    beta_cash_TUP2015 = models_ols[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP2015 = models_ols[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP2015 <= 0.01: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '***'
    elif pvalue_cash_TUP2015 <= 0.05: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '**'
    elif pvalue_cash_TUP2015 <= 0.1: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '*'
   
    # make a dictionary including coefficients, standard errors, F-stat, N, and difference in TUP and UCT coefficients
    d[outcome] = [controlgroup[controlgroup['2015'] == 1][outcome].mean(),
                  controlgroup[controlgroup['2015'] == 1][outcome].std(),
                  round(models_ols[outcome].params[0], 3), models_ols[outcome].bse[0],
                  round(models_ols[outcome].params[1], 3), models_ols[outcome].bse[1], 
                  round(models_ols[outcome].params[2], 3), models_ols[outcome].bse[2],
                  round(models_ols[outcome].params[3], 3), models_ols[outcome].bse[3],
                  round(models_ols[outcome].params[4], 3), models_ols[outcome].bse[4],
                  round(models_ols[outcome].params[5], 3), models_ols[outcome].bse[5],
                  round(models_ols[outcome].params[6], 3), models_ols[outcome].bse[6],
                  round(models_ols[outcome].params[7], 3), models_ols[outcome].bse[7],
                  models_ols[outcome].fvalue, models_ols[outcome].nobs, 
                  beta_cash_TUP2014, beta_cash_TUP2015]
    
    for x in range(0,8):
        # indexes account for standard errors being in the final table and skipping the two rows for control mean
        # e.g. second coefficient is beneath standard error so add asterisks to d[outcome][2] not d[outcome][1] and same for next coefficients
        if models_ols[outcome].pvalues[x] <= 0.01: 
            d[outcome][x*2 + 2] = str(d[outcome][x*2 + 2]) + '***'
        elif models_ols[outcome].pvalues[x] <= 0.05: 
            d[outcome][x*2 + 2] = str(d[outcome][x*2 + 2]) + '**'
        elif models_ols[outcome].pvalues[x] <= 0.1:
            d[outcome][x*2 + 2] = str(d[outcome][x*2 + 2]) + '*'

    del temp_df   
    
final = pd.DataFrame(data=d)
# name rows of the table
final.rename(index = {0: "CTL mean", 1: " ", 2: "TUP*2014", 3: "  ", 
                      4: "TUP*2015", 5: "   ", 6: "UCT*2014", 7: "    ",
                      8: "UCT*2015", 9: "     ", 10: "2014", 11: "      ",
                      12: "2015", 13: "       ", 14: "Bsln NA", 15: "        ",
                      16: "Bsln2013", 17: "         ", 18: "F-stat", 19: "N", 
                      20: "Beta_TUP_2014-Beta_Cash_2015", 21: "Beta_TUP_2015-Beta_Cash_2015"}, inplace=True)

newdf = df_to_orgtbl(final, float_fmt = '%.3f')
print(newdf)

#+end_src

#+RESULTS: food_insecurity
|                              |   z-score | wholeday |   hungry |   nofood | fewmeals | portions |  worried |
|------------------------------+-----------+----------+----------+----------+----------+----------+----------|
| CTL mean                     |    -0.008 |    0.447 |    0.460 |    0.595 |    0.751 |    0.814 |    0.903 |
|                              |     0.968 |    0.498 |    0.499 |    0.492 |    0.433 |    0.390 |    0.297 |
| TUP*2014                     |  0.266*** |   -0.026 |   -0.057 | -0.095** |   -0.027 |   0.069* |    0.047 |
|                              |     0.082 |    0.045 |    0.045 |    0.045 |    0.040 |    0.039 |    0.034 |
| TUP*2015                     |     0.013 |    0.016 |   0.076* |    0.007 |    0.039 |   -0.019 |   -0.036 |
|                              |     0.082 |    0.043 |    0.043 |    0.043 |    0.038 |    0.037 |    0.032 |
| UCT*2014                     |  0.279*** |   -0.033 |   -0.033 |   -0.008 |    0.001 |    0.040 |    0.032 |
|                              |     0.103 |    0.056 |    0.056 |    0.056 |    0.050 |    0.048 |    0.042 |
| UCT*2015                     |    -0.124 |   -0.006 |    0.037 |    0.037 |    0.001 |   -0.020 |   -0.029 |
|                              |     0.103 |    0.055 |    0.055 |    0.055 |    0.049 |    0.047 |    0.042 |
| 2014                         | -0.135*** | 0.376*** | 0.373*** | 0.484*** | 0.707*** | 0.651*** | 0.763*** |
|                              |     0.051 |    0.036 |    0.037 |    0.042 |    0.043 |    0.032 |    0.040 |
| 2015                         |     0.016 | 0.457*** | 0.433*** | 0.565*** | 0.755*** | 0.806*** | 0.928*** |
|                              |     0.051 |    0.033 |    0.035 |    0.040 |    0.041 |    0.030 |    0.038 |
| Bsln NA                      |     0.000 |   -0.051 |   -0.005 |    0.007 |   -0.046 |   -0.019 |   -0.024 |
|                              |     0.000 |    0.044 |    0.048 |    0.051 |    0.050 |    0.041 |    0.045 |
| Bsln2013                     |  0.103*** |    0.010 |   0.059* |    0.050 |    0.013 |    0.027 |   -0.030 |
|                              |     0.026 |    0.030 |    0.031 |    0.036 |    0.037 |    0.026 |    0.036 |
| F-stat                       |     6.056 |    2.243 |    4.069 |    4.103 |    1.774 |    3.897 |    5.758 |
| N                            |  1500.000 | 1282.000 | 1297.000 | 1293.000 | 1297.000 | 1292.000 | 1291.000 |
| Beta_TUP_2014-Beta_Cash_2015 |   0.39*** |   -0.020 |   -0.094 |  -0.131* |   -0.028 |    0.089 |    0.076 |
| Beta_TUP_2015-Beta_Cash_2015 |     0.137 |    0.022 |    0.040 |   -0.030 |    0.038 |    0.001 |   -0.008 |



** Assets
   
Turning now to asset holdings for the households, we estimate treatment effects for
total value of physical assets owned, total value of potentially productive assets,
as well as land and cash savings.

*** Total Asset Holdings

The cash group does not appear to have seen an increase in the value of assets
measured, with negative and imprecise point estimates. This contrasts sharply with
the TUP group, which seems to get notably wealthier and stay wealthier over time. The
TUP group has significantly more asset wealth than the cash or control groups in both
2014 and 2015, 18 months after receipt of transfers. They have 536 SSP more on
average in 2014 and 624 SSP more in 2015. So-called "Productive" assets include
anything that could plausibly be used in productive activity.[fn:: For now, we
include in this list: small and large livestock, farm equipment, mobiles, carts,
sewing equipment, sheds, and shop premises.] Here we see the TUP group has 320 SSP
(95%) more in this area over the control group, with a similar magnitude at midline.

Note also that the effect on total assets is higher in absolute value than the effect
on productive asset value, suggesting that the increased wealth cannot be explained
purely by households holding onto asset transfers for the length of the program's
monitoring phase. Indeed, we see in \Fig{fig:AssetTotal} that the TUP group is the
only one for whom total measured asset holdings did not fall on average over these
two years, which saw hyperinflation and a significant aggregate economic downturn.
This asset effect (including the savings effect below) is the only feature of
households' financial situation on which we we see a persistent effect.

#+CAPTION: Measured asset wealth by group-year
#+NAME: fig:AssetTotal
[[../figures/AssetTotal_groupyear.png]] 

#+name: asset_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<asset_analysis>>
return tab
#+end_src

RESULTS: asset_results WITH Tgroup Fixed Effects
|----------------------------------+--------------------+-------------------+-------------------|
|                                  | Total              | Productive        | Livestock         |
|----------------------------------+--------------------+-------------------+-------------------|
| CTL mean                         | \(1149.392\)       | \(303.427\)       | \(191.478\)       |
|                                  | \((1494.992)\)     | \((586.417)\)     | \((529.489)\)     |
|----------------------------------+--------------------+-------------------+-------------------|
| TUP*2014                         | \(301.119^{***}\)  | \(185.870^{***}\) | \(145.139^{***}\) |
|                                  | \((100.921)\)      | \((45.928)\)      | \((39.745)\)      |
| TUP*2015                         | \(152.392\)        | \(76.187^{*}\)    | \(87.150^{**}\)   |
|                                  | \((100.921)\)      | \((45.928)\)      | \((39.745)\)      |
| CSH*2014                         | \(124.143\)        | \(50.705\)        | \(10.077\)        |
|                                  | \((127.384)\)      | \((57.967)\)      | \((50.161)\)      |
| CSH*2015                         | \(-112.701\)       | \(-16.201\)       | \(2.840\)         |
|                                  | \((127.384)\)      | \((57.967)\)      | \((50.161)\)      |
| TUP                              | \(453.511^{***}\)  | \(262.057^{***}\) | \(232.289^{***}\) |
|                                  | \((63.925)\)       | \((29.053)\)      | \((25.152)\)      |
| CSH                              | \(11.442\)         | \(34.504\)        | \(12.917\)        |
|                                  | \((80.739)\)       | \((36.668)\)      | \((31.734)\)      |
| 2014                             | \(895.043^{***}\)  | \(268.534^{***}\) | \(136.818^{***}\) |
|                                  | \((88.645)\)       | \((39.674)\)      | \((34.189)\)      |
| 2015                             | \(1056.150^{***}\) | \(295.315^{***}\) | \(183.214^{***}\) |
|                                  | \((88.645)\)       | \((39.674)\)      | \((34.189)\)      |
| Bsln2013                         | \(0.072^{***}\)    | \(0.015\)         | \(0.021\)         |
|                                  | \((0.017)\)        | \((0.012)\)       | \((0.013)\)       |
| Bsln_NAN                         | \(0.000\)          | \(0.000\)         | \(0.000\)         |
|                                  | \((0.000)\)        | \((0.000)\)       | \((0.000)\)       |
|----------------------------------+--------------------+-------------------+-------------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | \(413.820^{**}\)   | \(202.071^{**}\)  | \(142.298^{**}\)  |
|                                  | \((181.534)\)      | \((82.625)\)      | \((71.498)\)      |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | \(265.093^{**}\)   | \(92.388\)        | \(84.310\)        |
|                                  | \((134.945)\)      | \((61.419)\)      | \((53.148)\)      |
|----------------------------------+--------------------+-------------------+-------------------|
| F-stat                           | \(12.984\)         | \(14.753\)        | \(15.917\)        |
| N                                | \(1488.000\)       | \(1488.000\)      | \(1488.000\)      |
|----------------------------------+--------------------+-------------------+-------------------|

RESULTS: asset_results WITHOUT Tgroup Fixed Effects
|----------------------------------+--------------------+-------------------+-------------------|
|                                  | Total              | Productive        | Livestock         |
|----------------------------------+--------------------+-------------------+-------------------|
| CTL mean                         | \(1149.392\)       | \(303.427\)       | \(191.478\)       |
|                                  | \((1494.992)\)     | \((586.417)\)     | \((529.489)\)     |
|----------------------------------+--------------------+-------------------+-------------------|
| TUP*2014                         | \(754.630^{***}\)  | \(447.927^{***}\) | \(377.428^{***}\) |
|                                  | \((135.492)\)      | \((61.624)\)      | \((53.338)\)      |
| TUP*2015                         | \(605.902^{***}\)  | \(338.244^{***}\) | \(319.439^{***}\) |
|                                  | \((135.492)\)      | \((61.624)\)      | \((53.338)\)      |
| CSH*2014                         | \(135.584\)        | \(85.208\)        | \(22.994\)        |
|                                  | \((171.069)\)      | \((77.777)\)      | \((67.307)\)      |
| CSH*2015                         | \(-101.259\)       | \(18.303\)        | \(15.757\)        |
|                                  | \((171.069)\)      | \((77.777)\)      | \((67.307)\)      |
| 2014                             | \(895.043^{***}\)  | \(268.534^{***}\) | \(136.818^{***}\) |
|                                  | \((88.645)\)       | \((39.674)\)      | \((34.189)\)      |
| 2015                             | \(1056.150^{***}\) | \(295.315^{***}\) | \(183.214^{***}\) |
|                                  | \((88.645)\)       | \((39.674)\)      | \((34.189)\)      |
| Bsln2013                         | \(0.072^{***}\)    | \(0.015\)         | \(0.021\)         |
|                                  | \((0.017)\)        | \((0.012)\)       | \((0.013)\)       |
| Bsln_NAN                         | \(0.000\)          | \(0.000\)         | \(0.000\)         |
|                                  | \((0.000)\)        | \((0.000)\)       | \((0.000)\)       |
|----------------------------------+--------------------+-------------------+-------------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | \(855.889^{***}\)  | \(429.624^{***}\) | \(361.670^{***}\) |
|                                  | \((218.012)\)      | \((99.221)\)      | \((85.859)\)      |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | \(707.162^{***}\)  | \(319.941^{***}\) | \(303.682^{***}\) |
|                                  | \((181.065)\)      | \((82.403)\)      | \((71.306)\)      |
|----------------------------------+--------------------+-------------------+-------------------|
| F-stat                           | \(12.984\)         | \(14.753\)        | \(15.917\)        |
| N                                | \(1488.000\)       | \(1488.000\)      | \(1488.000\)      |
|----------------------------------+--------------------+-------------------+-------------------|
#+end_example

*** Recreating asset tables with and without treatment group fixed effects

#+name: asset_values
#+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
data_baseline = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_midline  = "../../TUP-data/Midline/TUP_midline.dta"
data_endline = "../../TUP-data/Endline/TUP_endline.dta"

import statsmodels.api as sm
import pandas as pd
import numpy as np
from cfe.df_utils import df_to_orgtbl

df_b = pd.read_stata(data_baseline)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)
df_a = pd.read_csv(data_assignment)

df_b.set_index('idno', inplace=True)
df_m.set_index('idno', inplace=True)
df_e.set_index('idno', inplace=True)
df_a.set_index('RespID', inplace=True)

# merge baseline and midline based on idno index
df = pd.merge(df_b.filter(regex="^asset_"), 
              df_m.filter(regex="^asset_"), 
              how='outer', left_index=True, right_index=True)

# merge baseline/midline and endline based on idno index
df = pd.merge(df, 
              df_e.filter(regex="^asset_"), 
              how='outer', left_index=True, right_index=True)

# merge baseline/midline/endline with 'Group' from master_assignment.csv
df = pd.merge(df, df_a[ ['Group'] ], 
             how = 'outer', left_index=True, right_index=True)

# Some assets to ignore, either because numbers turned out to be more or less meaningless, 
# or because they overlap (e.g. nets & ITN nets) (as commented in TUP-data/TUP.py)

df = df.drop(columns = df.filter(like='house').columns)
df = df.drop(columns = df.filter(like='homestead').columns)
df = df.drop(columns = df.filter(like='netITN').columns)

# find columns that end with "_b", "_m", or "_e"
# cut off the last 2 "_b", "_m", or "_e"
asset = {}
asset[2013] = df.filter(regex="_b$").rename(columns=lambda x: x[:-2])
asset[2014] = df.filter(regex="_m$").rename(columns=lambda x: x[:-2])
asset[2015] = df.filter(regex="_e$").rename(columns=lambda x: x[:-2])

asset_n = {}
asset_val = {}
price = {}

# Note: value columns have format asset_val_{good}, quantity columns have format asset_n_{good}
for year in (2013, 2014, 2015):
    asset_n[year] = asset[year].filter(like='_n_').rename(columns=lambda col: col[8:])
    asset_val[year] = asset[year].filter(like='_val_').rename(columns=lambda col: col[10:]) 
    price[year] = asset_val[year]/asset_n[year]

# add top code here if needed

productive_2013 = ['cows', 'smallanimals', 'poultry', 'plough', 'shed', 'shop', 'mobile', 'carts', 'sewing']
livestock_2013 = ['cows', 'smallanimals', 'poultry']

# not including 'ploughs again'
productive_2014_2015 = ['cows', 'smallanimals', 'chickens', 'ducks', 'plough', 'shed', 'shop', 'pangas', 'axes', 'mobile', 'carts', 'sewing']
livestock_2014_2015 = ['cows', 'smallanimals', 'chickens', 'ducks']


# for 2013
asset_val[2013]['Productive'] = asset_val[2013][productive_2013].sum(axis=1)
asset_val[2013]['Livestock'] = asset_val[2013][livestock_2013].sum(axis=1)
    
# add year 
asset_val[2013]['Year'] = '2013'

for year in (2014, 2015):
    # 2013 only has poultry, but 2014 and 2015 have chickens and ducks separately
    asset_val[year]['poultry'] = asset_val[year][ ['chickens', 'ducks'] ].sum(axis = 1)
    asset_n[year]['poultry'] = asset_n[year][ ['chickens', 'ducks'] ].sum(axis = 1)
    
    asset_val[year]['Productive'] = asset_val[year][productive_2014_2015].sum(axis=1)
    asset_val[year]['Livestock'] = asset_val[year][livestock_2014_2015].sum(axis=1)
    
    # add year
    asset_val[year]['Year'] = str(year)

outcomes = ["Total", "Productive", "Livestock"]

for year in (2013, 2014, 2015): 
    asset_val[year]['Total'] = asset_val[year].sum(axis=1)
    
    # set up to make year dummies
    asset_val[year]['2013'] = 0
    asset_val[year]['2014'] = 0
    asset_val[year]['2015'] = 0
    
    # baseline values as 'var'2013
    for var in outcomes: 
        asset_val[year][var+'2013'] = asset_val[2013][var]

# make year dummmies
asset_val[2013]['2013'].replace(to_replace = 0, value = 1, inplace=True)
asset_val[2014]['2014'].replace(to_replace = 0, value = 1, inplace=True)
asset_val[2015]['2015'].replace(to_replace = 0, value = 1, inplace=True)

df2 = pd.concat((asset_val[2013], asset_val[2014], asset_val[2015]))
df = df2.join((df[ ['Group'] ].join(pd.get_dummies(df['Group']))))   

# make interaction terms
df.insert(len(df.columns), 'TUP*2013', df['2013']*df['TUP'])
df.insert(len(df.columns), 'TUP*2014', df['2014']*df['TUP'])
df.insert(len(df.columns), 'TUP*2015', df['2015']*df['TUP'])
df.insert(len(df.columns), 'UCT*2013', df['2013']*df['UCT'])
df.insert(len(df.columns), 'UCT*2014', df['2014']*df['UCT'])
df.insert(len(df.columns), 'UCT*2015', df['2015']*df['UCT'])

models_ols_FE = {}
models_ols_no_FE = {}
d_FE = {}
d_no_FE = {}
baseline_na = True

controls_FE = ['TUP*2014', 'TUP*2015', 'UCT*2014', 'UCT*2015', '2014', '2015', 'TUP', 'UCT']
controls_no_FE = ['TUP*2014', 'TUP*2015', 'UCT*2014', 'UCT*2015', '2014', '2015']
# from above, outcomes = ["Total", "Productive", "Livestock"]

# remove observations from 2013
df = df[df['Year'] != '2013']

for outcome in outcomes: 
    temp_df = df[ [outcome, outcome + "2013", 'Control'] + controls_FE]
    temp_controls_FE = controls_FE
    temp_controls_no_FE = controls_no_FE
    
    if baseline_na==True:
        
        # indicator for whether outcome in 2013 is na, and cast it to be an integer
        temp_df["Baseline_NAN"] = temp_df[outcome + "2013"].isnull().apply(int)
        
        # code missing values of the baseline variable as 0
        temp_df[outcome + "2013"].fillna(0,inplace=True)
        
        temp_controls_FE = temp_controls_FE + ['Baseline_NAN']
        temp_controls_no_FE = temp_controls_no_FE + ['Baseline_NAN']
    
    temp_controls_FE = temp_controls_FE + [outcome+"2013"]
    temp_controls_no_FE = temp_controls_no_FE + [outcome+"2013"]
    temp_df = temp_df.dropna()
    
    models_ols_FE[outcome] = sm.OLS(temp_df[outcome], temp_df[temp_controls_FE]).fit()
    models_ols_no_FE[outcome] = sm.OLS(temp_df[outcome], temp_df[temp_controls_no_FE]).fit()

    # for computing the mean of the control group in 2015
    controlgroup = temp_df[temp_df['Control'] == 1]
    
    """for table with Fixed Effects"""

    # for computing the difference in TUP and Cash coefficients
    beta_cash_TUP2014 = models_ols_FE[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP2014 = models_ols_FE[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP2014 <= 0.01: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '***'
    elif pvalue_cash_TUP2014 <= 0.05: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '**'
    elif pvalue_cash_TUP2014 <= 0.1: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '*'
        
    beta_cash_TUP2015 = models_ols_FE[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP2015 = models_ols_FE[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP2015 <= 0.01: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '***'
    elif pvalue_cash_TUP2015 <= 0.05: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '**'
    elif pvalue_cash_TUP2015 <= 0.1: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '*'

    d_FE[outcome] = [controlgroup[controlgroup['2015'] == 1][outcome].mean(),
                  controlgroup[controlgroup['2015'] == 1][outcome].std(),
                  round(models_ols_FE[outcome].params[0], 3), models_ols_FE[outcome].bse[0],
                  round(models_ols_FE[outcome].params[1], 3), models_ols_FE[outcome].bse[1], 
                  round(models_ols_FE[outcome].params[2], 3), models_ols_FE[outcome].bse[2],
                  round(models_ols_FE[outcome].params[3], 3), models_ols_FE[outcome].bse[3],
                  round(models_ols_FE[outcome].params[4], 3), models_ols_FE[outcome].bse[4],
                  round(models_ols_FE[outcome].params[5], 3), models_ols_FE[outcome].bse[5],
                  round(models_ols_FE[outcome].params[6], 3), models_ols_FE[outcome].bse[6],
                  round(models_ols_FE[outcome].params[7], 3), models_ols_FE[outcome].bse[7],
                  round(models_ols_FE[outcome].params[8], 3), models_ols_FE[outcome].bse[8],
                  round(models_ols_FE[outcome].params[9], 3), models_ols_FE[outcome].bse[9],
                  models_ols_FE[outcome].fvalue, models_ols_FE[outcome].nobs, 
                  beta_cash_TUP2014, beta_cash_TUP2015]
    
    for x in range(0,10):
        # indexes account for standard errors being in the final table
        # e.g. second coefficient is beneath standard error so change d[outcome][2] not d[outcome][1] and same for next coefficients
        if models_ols_FE[outcome].pvalues[x] <= 0.01: 
            d_FE[outcome][x*2 + 2] = str(d_FE[outcome][x*2 + 2]) + '***'
        elif models_ols_FE[outcome].pvalues[x] <= 0.05: 
            d_FE[outcome][x*2 + 2] = str(d_FE[outcome][x*2 + 2]) + '**'
        elif models_ols_FE[outcome].pvalues[x] <= 0.1:
            d_FE[outcome][x*2 + 2] = str(d_FE[outcome][x*2 + 2]) + '*'
    
    """for table WITHOUT Fixed Effects"""

    # for computing the difference in TUP and Cash coefficients
    beta_cash_TUP2014 = models_ols_no_FE[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP2014 = models_ols_no_FE[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP2014 <= 0.01: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '***'
    elif pvalue_cash_TUP2014 <= 0.05: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '**'
    elif pvalue_cash_TUP2014 <= 0.1: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '*'
        
    beta_cash_TUP2015 = models_ols_no_FE[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP2015 = models_ols_no_FE[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP2015 <= 0.01: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '***'
    elif pvalue_cash_TUP2015 <= 0.05: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '**'
    elif pvalue_cash_TUP2015 <= 0.1: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '*'
    
    d_no_FE[outcome] = [controlgroup[controlgroup['2015'] == 1][outcome].mean(),
                  controlgroup[controlgroup['2015'] == 1][outcome].std(),
                  round(models_ols_no_FE[outcome].params[0], 3), models_ols_no_FE[outcome].bse[0],
                  round(models_ols_no_FE[outcome].params[1], 3), models_ols_no_FE[outcome].bse[1], 
                  round(models_ols_no_FE[outcome].params[2], 3), models_ols_no_FE[outcome].bse[2],
                  round(models_ols_no_FE[outcome].params[3], 3), models_ols_no_FE[outcome].bse[3],
                  round(models_ols_no_FE[outcome].params[4], 3), models_ols_no_FE[outcome].bse[4],
                  round(models_ols_no_FE[outcome].params[5], 3), models_ols_no_FE[outcome].bse[5],
                  round(models_ols_no_FE[outcome].params[6], 3), models_ols_no_FE[outcome].bse[6],
                  round(models_ols_no_FE[outcome].params[7], 3), models_ols_no_FE[outcome].bse[7],
                  models_ols_no_FE[outcome].fvalue, models_ols_no_FE[outcome].nobs,
                  beta_cash_TUP2014, beta_cash_TUP2015]
    
    for x in range(0,8):
        # indexes account for standard errors being in the final table
        # e.g. second coefficient is beneath standard error so change d[outcome][2] not d[outcome][1] and same for next coefficients
        if models_ols_no_FE[outcome].pvalues[x] <= 0.01: 
            d_no_FE[outcome][x*2 + 2] = str(d_no_FE[outcome][x*2 + 2]) + '***'
        elif models_ols_no_FE[outcome].pvalues[x] <= 0.05: 
            d_no_FE[outcome][x*2 + 2] = str(d_no_FE[outcome][x*2 + 2]) + '**'
        elif models_ols_no_FE[outcome].pvalues[x] <= 0.1:
            d_no_FE[outcome][x*2 + 2] = str(d_no_FE[outcome][x*2 + 2]) + '*'

    del temp_df   
    
final_FE = pd.DataFrame(data=d_FE)
final_FE.rename(index = {0: "CTL mean", 1: " ", 2: "TUP*2014", 3: "  ", 
                      4: "TUP*2015", 5: "   ", 6: "UCT*2014", 7: "    ",
                      8: "UCT*2015", 9: "     ", 10: "2014", 11: "      ",
                      12: "2015", 13: "       ", 14: "TUP", 15: "        ",
                      16: "UCT", 17: "         ", 18: "Bsln NA", 19: "          ",
                      20: "Bsln2013", 21: "           ", 22: "F-stat", 23: "N", 
                      24: "Beta_TUP_2014-Beta_Cash_2015", 25: "Beta_TUP_2015-Beta_Cash_2015"}, inplace=True)

newdf_FE = df_to_orgtbl(final_FE, float_fmt = '%.3f')
print("""#+caption: asset results WITH Tgroup Fixed Effects""")
print(newdf_FE)

final_no_FE = pd.DataFrame(data=d_no_FE)
final_no_FE.rename(index = {0: "CTL mean", 1: " ", 2: "TUP*2014", 3: "  ", 
                      4: "TUP*2015", 5: "   ", 6: "UCT*2014", 7: "    ",
                      8: "UCT*2015", 9: "     ", 10: "2014", 11: "      ",
                      12: "2015", 13: "       ", 14: "Bsln NA", 15: "        ",
                      16: "Bsln2013", 17: "         ", 18: "F-stat", 19: "N",
                      20: "Beta_TUP_2014-Beta_Cash_2015", 21: "Beta_TUP_2015-Beta_Cash_2015"}, inplace=True)
newdf_no_FE = df_to_orgtbl(final_no_FE, float_fmt = '%.3f')
print("""#+caption: asset results WITHOUT Tgroup Fixed Effects""")
print(newdf_no_FE)

#+end_src

#+RESULTS: asset_values
#+caption: asset results WITH Tgroup Fixed Effects
|  |Total|   Productive|   Livestock  |
|-
| CTL mean  | 1634.104 | 301.364 | 189.439 |
|    | 2406.541 | 580.913 | 523.819 |
| TUP*2014  | 645.768*** | 186.369*** | 144.087*** |
|     | 172.158 | 45.467 | 39.318 |
| TUP*2015  | 326.43* | 77.655* | 89.264** |
|      | 172.158 | 45.467 | 39.318 |
| UCT*2014  | 207.519 | 53.612 | 10.924 |
|       | 217.984 | 57.569 | 49.782 |
| UCT*2015  | -122.541 | -16.568 | 3.421 |
|        | 217.984 | 57.569 | 49.782 |
| 2014  | 1291.392*** | 262.936*** | 134.5*** |
|         | 149.060 | 39.001 | 33.588 |
| 2015  | 1540.804*** | 293.132*** | 181.163*** |
|          | 149.060 | 39.001 | 33.588 |
| TUP  | 972.198*** | 264.023*** | 233.35*** |
|           | 108.895 | 28.759 | 24.879 |
| UCT  | 84.979 | 37.044 | 14.346 |
|            | 137.905 | 36.415 | 31.494 |
| Bsln NA  | 0.000 | 0.000 | 0.000 |
|             | 0.000 | 0.000 | 0.000 |
| Bsln2013  | 0.042*** | 0.015 | 0.021* |
|              | 0.014 | 0.012 | 0.012 |
| F-stat  | 15.911 | 15.203 | 16.350 |
| N  | 1510.000 | 1510.000 | 1510.000 |
| Beta_TUP_2014-Beta_Cash_2015  | 768.308** | 202.937** | 140.665** |
| Beta_TUP_2015-Beta_Cash_2015  | 448.971* | 94.223 | 85.842 |

#+caption: asset results WITHOUT Tgroup Fixed Effects
|  |Total|   Productive|   Livestock  |
|-
| CTL mean  | 1634.104 | 301.364 | 189.439 |
|    | 2406.541 | 580.913 | 523.819 |
| TUP*2014  | 1617.966*** | 450.392*** | 377.437*** |
|     | 230.986 | 61.004 | 52.762 |
| TUP*2015  | 1298.628*** | 341.678*** | 322.614*** |
|      | 230.986 | 61.004 | 52.762 |
| UCT*2014  | 292.498 | 90.656 | 25.270 |
|       | 292.493 | 77.242 | 66.798 |
| UCT*2015  | -37.562 | 20.476 | 17.767 |
|        | 292.493 | 77.242 | 66.798 |
| 2014  | 1291.392*** | 262.936*** | 134.5*** |
|         | 149.060 | 39.001 | 33.588 |
| 2015  | 1540.804*** | 293.132*** | 181.163*** |
|          | 149.060 | 39.001 | 33.588 |
| Bsln NA  | 0.000 | 0.000 | 0.000 |
|           | 0.000 | 0.000 | 0.000 |
| Bsln2013  | 0.042*** | 0.015 | 0.021* |
|            | 0.014 | 0.012 | 0.012 |
| F-stat  | 15.911 | 15.203 | 16.350 |
| N  | 1510.000 | 1510.000 | 1510.000 |
| Beta_TUP_2014-Beta_Cash_2015  | 1655.528*** | 429.916*** | 359.67*** |
| Beta_TUP_2015-Beta_Cash_2015  | 1336.19*** | 321.202*** | 304.847*** |




#+name: tab:assets
#+caption: Average treatment effects by group-year on total value (in SSP) of all assets measured and of productive assets measured
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+-----------------+----------------|
|                                  | Total           | Productive     |
|----------------------------------+-----------------+----------------|
| CTL mean                         | $1225.61$       | $337.60$       |
|                                  | $(1502.46)$     | $(605.57)$     |
|----------------------------------+-----------------+----------------|
| TUP*2014                         | $535.79^{***}$  | $361.80^{***}$ |
|                                  | $(154.02)$      | $(74.19)$      |
| TUP*2015                         | $624.79^{***}$  | $320.74^{***}$ |
|                                  | $(146.01)$      | $(68.68)$      |
| CSH*2014                         | $-125.86$       | $18.50$        |
|                                  | $(191.31)$      | $(95.80)$      |
| CSH*2015                         | $-49.99$        | $-5.00$        |
|                                  | $(187.32)$      | $(88.40)$      |
| Bsln2013                         | $0.08^{***}$    | $0.00$         |
|                                  | $( 0.02)$       | $( 0.01)$      |
| 2014                             | $1259.75^{***}$ | $465.53^{***}$ |
|                                  | $(112.68)$      | $(55.96)$      |
| 2015                             | $1124.61^{***}$ | $392.97^{***}$ |
|                                  | $(103.46)$      | $(50.21)$      |
| Bsln NA                          | $21.30$         | $-131.14^{**}$ |
|                                  | $(146.51)$      | $(51.35)$      |
|----------------------------------+-----------------+----------------|
| N                                | $1305.00$       | $1247.00$      |
| F-stat                           | $8.53$          | $10.19$        |
|----------------------------------+-----------------+----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $585.78^{**}$   | $366.79^{***}$ |
|                                  | $(239.76)$      | $(114.58)$     |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $674.78^{***}$  | $325.74^{***}$ |
|                                  | $(194.72)$      | $(92.26)$      |
|----------------------------------+-----------------+----------------|


*** Savings

The TUP households were strongly encouraged to pay into a savings account maintained
by BRAC at each of their weekly meetings. Anecdotally, this discouraged some women
from attending the meetings, but we can see in Table \Tab{tab:Nonzero} that did
manage to change behavior. TUP participants appear to be 48% (19 pp.) more likely to
report having any savings at all at endline compared to control households. It's
worth noting though that since the TUP households also regard their savings behavior
as much more transparent to BRAC (and have received pressure to save from them) than
the other groups, these households may simply be more likely to reveal that they are
saving when asked. Nonetheless, it seems notable that among the group that
received cash, households were no more likely than the control group to report having
positive cash savings. This suggests to us that receiving cash is not in itself
sufficient to induce households to start holding cash on hand or in an account.
Conversely, increases in savings among TUP households were due not only to their
increased wealth, but also to additional pressure to hold cash in a savings account.

In Table \Tab{tab:Savings}, we consider the amount of cash saved in each year among
those who reported savings. Here we see that cash and TUP groups increase the amount
saved by a similar amount, with cash transfer recipients holding 47% (91.4 SSP) more
and TUP households saving 42% (81.3 SSP) more. This is significantly less than was
given to these households, but combined with the short-term consumption results, goes
some distance in explaining the lack of effect on physical asset wealth. The cash
seems to have gone primarily to consumption and savings. The asset transfer program
on the other hand seems to have achieved a similar result among a greater proportion
of households in addition to a notable increase in asset wealth.

It is common in this community (and most in the region) to store non-perishable food
like maize, cassava, or millet as a form of savings. This would seem particularly
reasonable in a high-inflation context, where the price of grain had doubled in the
previous year. At least as many households report saving in food (53%) as in cash
(46%), with an average market value of 106 SSP. However, we find no evidence that
either treatment group increased food savings.[fn:: Note that food savings was not
measured at baseline, so these controls are omitted.]

Neither do we find evidence that either treatment increased the size or likelihood of
giving or receiving interhousehold transfers, either in cash or in kind. These
results are omitted since only 35 and 60 households reported giving and receiving
transfers respectively, with no difference in group means.

#+name: savings_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<savings_analysis>>
return Table
#+end_src

#+name: tab:Nonzero
#+caption: Average treatment effects by group-year on percentage of households reporting any savings or land access 
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+--------------+--------------+--------------+--------------|
| % > 0                            | Savings      | Food Sav     | LandCult     | LandOwn      |
|----------------------------------+--------------+--------------+--------------+--------------|
| CTL mean                         | $0.45$       | $0.82$       | $0.82$       | $0.90$       |
|----------------------------------+--------------+--------------+--------------+--------------|
| CSH*2014                         | $-0.06$      | $0.00$       | $-0.04$      | $-0.01$      |
|                                  | $( 0.06)$    | $( 0.04)$    | $( 0.04)$    | $( 0.04)$    |
| CSH*2015                         | $0.03$       | $0.02$       | $0.05$       | $0.02$       |
|                                  | $( 0.05)$    | $( 0.04)$    | $( 0.04)$    | $( 0.04)$    |
| TUP*2014                         | $0.22^{***}$ | $-0.02$      | $-0.03$      | $-0.00$      |
|                                  | $( 0.04)$    | $( 0.03)$    | $( 0.03)$    | $( 0.03)$    |
| TUP*2015                         | $0.21^{***}$ | $-0.03$      | $0.01$       | $-0.01$      |
|                                  | $( 0.04)$    | $( 0.03)$    | $( 0.03)$    | $( 0.03)$    |
| 2014                             | $0.43^{***}$ | $1.00^{***}$ | $0.83^{***}$ | $0.82^{***}$ |
|                                  | $( 0.04)$    | $( 0.02)$    | $( 0.06)$    | $( 0.05)$    |
| 2015                             | $0.39^{***}$ | $0.82^{***}$ | $0.77^{***}$ | $0.84^{***}$ |
|                                  | $( 0.04)$    | $( 0.02)$    | $( 0.05)$    | $( 0.05)$    |
| Bsln2013                         | $0.05$       |              | $0.05$       | $0.07$       |
|                                  | $( 0.04)$    |              | $( 0.05)$    | $( 0.04)$    |
| Bsln NA                          | $0.08^{*}$   |              | $0.05$       | $0.05$       |
|                                  | $( 0.04)$    |              | $( 0.06)$    | $( 0.05)$    |
|----------------------------------+--------------+--------------+--------------+--------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $0.19$       | $-0.04$      | $-0.07$      | $-0.02$      |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $0.18$       | $-0.05$      | $-0.03$      | $-0.03$      |
|----------------------------------+--------------+--------------+--------------+--------------|
| F-stat                           | $8.83$       | $15.60$      | $0.79$       | $0.76$       |
| N                                | $1259.00$    | $870.00$     | $1231.00$    | $1251.00$    |
|----------------------------------+--------------+--------------+--------------+--------------|

#+name: tab:Savings
#+caption: Average treatment effects by group-year on total value (in SSP) of all cash and food savings and area (in fedan) of land being cultiviated by the household (including rented or temporary-use) and owned by the household.
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+----------------+----------------+----------------+----------------|
| Amt.                             | Savings        | Food Sav       | LandCult       | LandOwn        |
|----------------------------------+----------------+----------------+----------------+----------------|
| CTL mean                         | $191.19$       | $114.78$       | $61.88$        | $46.00$        |
|----------------------------------+----------------+----------------+----------------+----------------|
| CSH*2014                         | $28.74$        | $0.22$         | $10.18$        | $10.50$        |
|                                  | $(42.93)$      | $(15.38)$      | $(15.07)$      | $(12.57)$      |
| CSH*2015                         | $91.40^{**}$   | $-14.34$       | $-39.18^{***}$ | $-32.37^{***}$ |
|                                  | $(40.89)$      | $(14.98)$      | $(14.90)$      | $(11.95)$      |
| TUP*2014                         | $-27.09$       | $17.16$        | $-4.76$        | $-3.02$        |
|                                  | $(29.76)$      | $(12.33)$      | $(11.94)$      | $(10.04)$      |
| TUP*2015                         | $81.33^{***}$  | $1.13$         | $-17.38$       | $-12.56$       |
|                                  | $(29.32)$      | $(12.26)$      | $(11.65)$      | $( 9.41)$      |
| 2014                             | $106.72^{***}$ | $62.03^{***}$  | $11.37$        | $17.31^{**}$   |
|                                  | $(24.85)$      | $( 8.36)$      | $( 9.94)$      | $( 8.56)$      |
| 2015                             | $163.04^{***}$ | $114.78^{***}$ | $61.52^{***}$  | $51.89^{***}$  |
|                                  | $(24.13)$      | $( 7.60)$      | $( 9.54)$      | $( 7.88)$      |
| Bsln2013                         | $0.05^{**}$    |                | $0.94$         | $-2.43$        |
|                                  | $( 0.02)$      |                | $( 3.07)$      | $( 1.95)$      |
| Bsln NA                          | $40.07^{*}$    |                | $-1.60$        | $-6.02$        |
|                                  | $(21.24)$      |                | $( 9.92)$      | $( 8.29)$      |
|----------------------------------+----------------+----------------+----------------+----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $-118.49$      | $31.50$        | $34.42$        | $29.35$        |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $-10.07$       | $15.47$        | $21.79$        | $19.80$        |
|----------------------------------+----------------+----------------+----------------+----------------|
| F-stat                           | $7.41$         | $7.14$         | $4.91$         | $3.72$         |
| N                                | $671.00$       | $777.00$       | $1042.00$      | $1114.00$      |
|----------------------------------+----------------+----------------+----------------+----------------|

*** Recreating savings tables

#+name: savings_tables
#+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
data_baseline = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_midline  = "../../TUP-data/Midline/TUP_midline.dta"
data_endline = "../../TUP-data/Endline/TUP_endline.dta"

import statsmodels.api as sm
import pandas as pd
import numpy as np
from cfe.df_utils import df_to_orgtbl

df_b = pd.read_stata(data_baseline)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)
df_a = pd.read_csv(data_assignment)

df_b.set_index('idno', inplace=True)
df_m.set_index('idno', inplace=True)
df_e.set_index('idno', inplace=True)
df_a.set_index('RespID', inplace=True)

# not including 'land_ownnocult_b' or 'land_ownrent_b'
# merge baseline and midline based on idno index

#format savings_maize_q_m  savings_maize_val_m 
#s8b_2c_m and  s8b_2d_m are for other food savings values
#transfers_get1_m is value of transfers
df = pd.merge(df_b[ ['savings_home_b', 'savings_bank_b', 'savings_BRAC_b', 'savings_NGOs_b', 
                     'savings_other_b', 'land_owncult_b', 'land_rentcult_b', 'land_communitycult_b',
                     'land_ownnocult_b', 'land_ownrent_b', 'transfers_get1_b', 'transfers_get2_b', 
                     'transfers_get3_b', 'transfers_give1_b', 'transfers_give2_b', 'transfers_give3_b'] ], 
              df_m[ ['savings_home_m', 'savings_bank_m', 'savings_BRAC_m', 'savings_NGOs_m', 
                     'savings_other_m', 'savings_maize_val_m', 'savings_sorghum_val_m', 
                     's8b_2c_m', 's8b_2d_m', 'land_owncult_m', 'land_rentcult_m', 'land_communitycult_m',
                     'land_ownnocult_m', 'land_ownrent_m', 'transfers_get1_m', 'transfers_get2_m', 
                     'transfers_get3_m', 'transfers_give1_m', 'transfers_give2_m', 'transfers_give3_m'] ], 
              how='outer', left_index=True, right_index=True)


# # merge baseline/midline and endline based on idno index
# endline issue with how sum being calculated for 'savings_otherfood_val_e' - 40+NaN=NaN, so using s4b_oth_1b_e and s4b_oth_2b_e instead
df = pd.merge(df, 
              df_e[ ['savings_home_e', 'savings_bank_e', 'savings_BRAC_e', 'savings_NGOs_e', 
                     'savings_other_e', 'savings_maize_val_e', 'savings_sorghum_val_e', 
                     's4b_oth_1b_e', 's4b_oth_2b_e', 'land_owncult_e', 'land_rentcult_e', 'land_communitycult_e',
                     'land_ownnocult_e', 'land_ownrent_e', 'transfers_get1_e', 'transfers_get2_e', 
                     'transfers_get3_e', 'transfers_give1_e', 'transfers_give2_e', 'transfers_give3_e'] ], 
              how='outer', left_index=True, right_index=True)

# merge baseline/midline/endline with 'Group' from master_assignment.csv
df = pd.merge(df, df_a[ ['Group'] ], 
             how = 'outer', left_index=True, right_index=True)


df.rename({'s8b_2c_m': 'savings_otherfood1_val_m',
           's8b_2d_m': 'savings_otherfood2_val_m',
           's4b_oth_1b_e': 'savings_otherfood1_val_e',
           's4b_oth_2b_e': 'savings_otherfood2_val_e'}, axis=1, inplace=True)

# find columns that end with "_b", "_m", or "_e"
# cut off the the last 2 characters "_b", "_m", or "_e"
s2013 = df.filter(regex="_b$").rename(columns=lambda x: x[:-2])
s2014 = df.filter(regex="_m$").rename(columns=lambda x: x[:-2])
s2015 = df.filter(regex="_e$").rename(columns=lambda x: x[:-2])

s2013['Year'] = '2013'
s2014['Year'] = '2014'
s2015['Year'] = '2015'

# make the sum variables
for var in (s2013, s2014, s2015):
    var["Savings"] = var[ ['savings_home', 'savings_bank', 'savings_BRAC', 'savings_NGOs', 'savings_other'] ].sum(axis=1)
    
    # no food savings at baseline (2013)
    if (var['Year'] != '2013').any(): 
        var["Food Savings"] = var[ ['savings_maize_val', 'savings_sorghum_val', 'savings_otherfood1_val', 'savings_otherfood2_val'] ].sum(axis=1)
    
    var["LandCult"] = var[ ['land_owncult', 'land_rentcult', 'land_communitycult'] ].sum(axis=1)
    var["LandOwn"] = var[ ['land_owncult', 'land_ownnocult', 'land_ownrent'] ].sum(axis=1)
    var["Get Transfer"] = var[ ['transfers_get1', 'transfers_get2', 'transfers_get3'] ].sum(axis=1)
    var["Give Transfer"] = var[ ['transfers_give1', 'transfers_give2', 'transfers_give3'] ].sum(axis=1)
    
# LandOwn and LandCult are asked in fedan (unit of area)

outcomes_with_baseline = ['Savings', 'LandCult', 'LandOwn', 'Get Transfer', 'Give Transfer'] #excludes Food Savings

for var in (s2013, s2014, s2015): 
    
    # set up to make year dummies
    var.insert(len(var.columns), '2013', 0)
    var.insert(len(var.columns), '2014', 0)
    var.insert(len(var.columns), '2015', 0)
    
    # baseline values as 'var'2013
    for outcome in outcomes_with_baseline: 
        var[outcome+'2013'] = s2013[outcome]
    
# make year dummmies
s2013['2013'].replace(to_replace = 0, value = 1, inplace=True)
s2014['2014'].replace(to_replace = 0, value = 1, inplace=True)
s2015['2015'].replace(to_replace = 0, value = 1, inplace=True)

df2 = pd.concat((s2013, s2014, s2015))
df = df2.join((df[ ['Group'] ].join(pd.get_dummies(df['Group']))))

# make interaction variables
df.insert(len(df.columns), 'TUP*2013', df['2013']*df['TUP'])
df.insert(len(df.columns), 'TUP*2014', df['2014']*df['TUP'])
df.insert(len(df.columns), 'TUP*2015', df['2015']*df['TUP'])
df.insert(len(df.columns), 'UCT*2013', df['2013']*df['UCT'])
df.insert(len(df.columns), 'UCT*2014', df['2014']*df['UCT'])
df.insert(len(df.columns), 'UCT*2015', df['2015']*df['UCT'])

outcomes = ['Savings', 'Food Savings', 'LandCult', 'LandOwn', 'Get Transfer', 'Give Transfer']
nonzero_df = df.copy()

# make positive values = 1, 0 values = NaN, and NaN stay NaN
for i in range(len(nonzero_df)):
    for outcome in outcomes:
         if pd.notna(nonzero_df.iloc[i][outcome]):
            if nonzero_df.iloc[i][outcome] <= 0: 
                nonzero_df.iloc[i, nonzero_df.columns.get_loc(outcome)] = 0
            elif nonzero_df.iloc[i][outcome] > 0:
                nonzero_df.iloc[i, nonzero_df.columns.get_loc(outcome)] = 1
                

regular_models_ols = {}
regular_d = {}
nonzero_models_ols = {}
nonzero_d = {}

baseline_na = True

controls = ['TUP*2014', 'TUP*2015', 'UCT*2014', 'UCT*2015', '2014', '2015']
# from above, outcomes_with_baseline = ['Savings', 'LandCult', 'LandOwn', 'Get Transfer', 'Give Transfer'] #excludes Food Savings

# remove observations from 2013
df = df[df['Year'] != '2013']
nonzero_df = nonzero_df[nonzero_df['Year'] != '2013']

for DF in (df, nonzero_df):
    for outcome in outcomes_with_baseline: 
        temp_df = DF[ [outcome, outcome + "2013", 'Control'] + controls]
        temp_controls = controls
    
        if baseline_na==True:
        
            # indicator for whether outcome in 2013 is na, and cast it to be an integer
            temp_df["Baseline_NAN"] = temp_df[outcome + "2013"].isnull().apply(int)
        
            # code missing values of the baseline variable as 0
            temp_df[outcome + "2013"].fillna(0,inplace=True)
        
            temp_controls = temp_controls + ['Baseline_NAN']
    
        temp_controls = temp_controls + [outcome+"2013"]
        temp_df = temp_df.dropna()
    
        if DF.equals(nonzero_df): 
            models_ols = nonzero_models_ols
        else: 
            models_ols = regular_models_ols
            
        models_ols[outcome] = sm.OLS(temp_df[outcome], temp_df[temp_controls]).fit()
    
        # for computing the mean of the control group in 2015
        controlgroup = temp_df[temp_df['Control'] == 1]

        # for computing the difference in TUP and Cash coefficients
        beta_cash_TUP2014 = models_ols[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["coef"][0]
        pvalue_cash_TUP2014 = models_ols[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
        if pvalue_cash_TUP2014 <= 0.01: 
            beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '***'
        elif pvalue_cash_TUP2014 <= 0.05: 
            beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '**'
        elif pvalue_cash_TUP2014 <= 0.1: 
            beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '*'
        
        beta_cash_TUP2015 = models_ols[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["coef"][0]
        pvalue_cash_TUP2015 = models_ols[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
        if pvalue_cash_TUP2015 <= 0.01: 
            beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '***'
        elif pvalue_cash_TUP2015 <= 0.05: 
            beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '**'
        elif pvalue_cash_TUP2015 <= 0.1: 
            beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '*'
        
        if DF.equals(nonzero_df): 
            d = nonzero_d 
        else: 
            d = regular_d
            
        d[outcome] = [controlgroup[controlgroup['2015'] == 1][outcome].mean(),
                  controlgroup[controlgroup['2015'] == 1][outcome].std(),
                  round(models_ols[outcome].params[0], 3), models_ols[outcome].bse[0],
                  round(models_ols[outcome].params[1], 3), models_ols[outcome].bse[1], 
                  round(models_ols[outcome].params[2], 3), models_ols[outcome].bse[2],
                  round(models_ols[outcome].params[3], 3), models_ols[outcome].bse[3],
                  round(models_ols[outcome].params[4], 3), models_ols[outcome].bse[4],
                  round(models_ols[outcome].params[5], 3), models_ols[outcome].bse[5],
                  round(models_ols[outcome].params[6], 3), models_ols[outcome].bse[6],
                  round(models_ols[outcome].params[7], 3), models_ols[outcome].bse[7],
                  models_ols[outcome].fvalue, models_ols[outcome].nobs,
                  beta_cash_TUP2014, beta_cash_TUP2015]
    
        for x in range(0,8):
            # indexes account for standard errors being in the final table
            # e.g. second coefficient is beneath standard error so change d[outcome][2] not d[outcome][1] and same for next coefficients
            if models_ols[outcome].pvalues[x] <= 0.01: 
                d[outcome][x*2 + 2] = str(d[outcome][x*2 + 2]) + '***'
            elif models_ols[outcome].pvalues[x] <= 0.05: 
                d[outcome][x*2 + 2] = str(d[outcome][x*2 + 2]) + '**'
            elif models_ols[outcome].pvalues[x] <= 0.1:
                d[outcome][x*2 + 2] = str(d[outcome][x*2 + 2]) + '*'

        del temp_df   

# for food savings, which had no baseline value
for DF in (df, nonzero_df):

    temp_df = DF[ ["Food Savings", 'Control'] + controls]
    temp_df = temp_df.dropna()
    
    if DF.equals(nonzero_df): 
        models_ols = nonzero_models_ols
    else: 
        models_ols = regular_models_ols
    
    models_ols["Food Savings"] = sm.OLS(temp_df["Food Savings"], temp_df[controls]).fit()
    
    # for computing the mean of the control group in 2015
    controlgroup = temp_df[temp_df['Control'] == 1]
    
    # for computing the difference in TUP and Cash coefficients
    beta_cash_TUP2014 = models_ols["Food Savings"].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP2014 = models_ols["Food Savings"].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP2014 <= 0.01: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '***'
    elif pvalue_cash_TUP2014 <= 0.05: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '**'
    elif pvalue_cash_TUP2014 <= 0.1: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '*'
        
    beta_cash_TUP2015 = models_ols["Food Savings"].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP2015 = models_ols["Food Savings"].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP2015 <= 0.01: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '***'
    elif pvalue_cash_TUP2015 <= 0.05: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '**'
    elif pvalue_cash_TUP2015 <= 0.1: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '*'
    
    if DF.equals(nonzero_df): 
        d = nonzero_d 
    else: 
        d = regular_d
            
    d["Food Savings"] = [controlgroup[controlgroup['2015'] == 1]["Food Savings"].mean(),
                  controlgroup[controlgroup['2015'] == 1]["Food Savings"].std(),
                  round(models_ols["Food Savings"].params[0], 3), models_ols["Food Savings"].bse[0],
                  round(models_ols["Food Savings"].params[1], 3), models_ols["Food Savings"].bse[1], 
                  round(models_ols["Food Savings"].params[2], 3), models_ols["Food Savings"].bse[2],
                  round(models_ols["Food Savings"].params[3], 3), models_ols["Food Savings"].bse[3],
                  round(models_ols["Food Savings"].params[4], 3), models_ols["Food Savings"].bse[4],
                  round(models_ols["Food Savings"].params[5], 3), models_ols["Food Savings"].bse[5],
                     "", " ", "  ", "   ",
                  models_ols["Food Savings"].fvalue, models_ols["Food Savings"].nobs, 
                  beta_cash_TUP2014, beta_cash_TUP2015]
    
    for x in range(0,6):
        # indexes account for standard errors being in the final table
        # e.g. second coefficient is beneath standard error so change d[outcome][2] not d[outcome][1] and same for next coefficients
        if models_ols["Food Savings"].pvalues[x] <= 0.01: 
            d["Food Savings"][x*2 + 2] = str(d["Food Savings"][x*2 + 2]) + '***'
        elif models_ols["Food Savings"].pvalues[x] <= 0.05: 
            d["Food Savings"][x*2 + 2] = str(d["Food Savings"][x*2 + 2]) + '**'
        elif models_ols["Food Savings"].pvalues[x] <= 0.1:
            d["Food Savings"][x*2 + 2] = str(d["Food Savings"][x*2 + 2]) + '*'

final = pd.DataFrame(data=regular_d)
nonzero_final = pd.DataFrame(data=nonzero_d)

final.rename(index = {0: "CTL mean", 1: " ", 2: "TUP*2014", 3: "  ", 
                      4: "TUP*2015", 5: "   ", 6: "UCT*2014", 7: "    ",
                      8: "UCT*2015", 9: "     ", 10: "2014", 11: "      ",
                      12: "2015", 13: "       ", 14: "Bsln NA", 15: "        ",
                      16: "Bsln2013", 17: "         ", 18: "F-stat", 19: "N",
                      20: "Beta_TUP_2014-Beta_Cash_2015", 21: "Beta_TUP_2015-Beta_Cash_2015"}, inplace=True)
nonzero_final.rename(index = {0: "CTL mean", 1: " ", 2: "TUP*2014", 3: "  ", 
                      4: "TUP*2015", 5: "   ", 6: "UCT*2014", 7: "    ",
                      8: "UCT*2015", 9: "     ", 10: "2014", 11: "      ",
                      12: "2015", 13: "       ", 14: "Bsln NA", 15: "        ",
                      16: "Bsln2013", 17: "         ", 18: "F-stat", 19: "N",
                      20: "Beta_TUP_2014-Beta_Cash_2015", 21: "Beta_TUP_2015-Beta_Cash_2015"}, inplace=True)

newdf_nonzero = df_to_orgtbl(nonzero_final, float_fmt = '%.3f')
print("""#+caption: Average treatment effects by group-year on percentage of households reporting any savings or land access""")
print(newdf_nonzero)

newdf = df_to_orgtbl(final, float_fmt = '%.3f')
print("""#+caption: Total value (in SSP) of all cash and food savings and area (in fedan) of land being cultiviated by the household (including rented or temporary-use) and owned by the household.""")
print(newdf)

#+end_src

#+RESULTS: savings_tables
#+caption: Average treatment effects by group-year on percentage of households reporting any savings or land access
|  |Savings|   LandCult|   LandOwn|   Get Transfer|   Give Transfer|   Food Savings  |
|-
| CTL mean  | 0.389 | 0.711 | 0.797 | 0.168 | 0.079 | 0.718 |
|    | 0.488 | 0.454 | 0.402 | 0.375 | 0.270 | 0.450 |
| TUP*2014  | 0.288*** | 0.162*** | 0.184*** | 0.082*** | 0.07*** | 0.131*** |
|     | 0.040 | 0.037 | 0.035 | 0.031 | 0.025 | 0.037 |
| TUP*2015  | 0.184*** | 0.017 | 0.001 | -0.005 | 0.030 | 0.006 |
|      | 0.040 | 0.037 | 0.035 | 0.031 | 0.025 | 0.037 |
| UCT*2014  | 0.034 | 0.118** | 0.148*** | 0.082** | 0.035 | 0.203*** |
|       | 0.050 | 0.047 | 0.045 | 0.039 | 0.032 | 0.046 |
| UCT*2015  | 0.011 | -0.019 | -0.023 | -0.025 | -0.005 | -0.030 |
|        | 0.050 | 0.047 | 0.045 | 0.039 | 0.032 | 0.046 |
| 2014  | 0.33*** | 0.571*** | 0.571*** | 0.134*** | 0.09*** | 0.629*** |
|         | 0.025 | 0.025 | 0.024 | 0.019 | 0.016 | 0.023 |
| 2015  | 0.385*** | 0.692*** | 0.773*** | 0.168*** | 0.074*** | 0.718*** |
|          | 0.025 | 0.025 | 0.024 | 0.019 | 0.016 | 0.023 |
| Bsln NA  | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 |  |
|           | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 |   |
| Bsln2013  | 0.000 | 0.019** | 0.016*** | 0.000 | 0.0** |    |
|            | 0.000 | 0.008 | 0.006 | 0.000 | 0.000 |     |
| F-stat  | 13.914 | 5.435 | 10.908 | 1.634 | 3.516 | 5.041 |
| N  | 1510.000 | 1510.000 | 1510.000 | 1510.000 | 1510.000 | 1510.000 |
| Beta_TUP_2014-Beta_Cash_2015  | 0.277*** | 0.181*** | 0.207*** | 0.106** | 0.075* | 0.161*** |
| Beta_TUP_2015-Beta_Cash_2015  | 0.174*** | 0.036 | 0.024 | 0.020 | 0.035 | 0.036 |

#+caption: Total value (in SSP) of all cash and food savings and area (in fedan) of land being cultiviated by the household (including rented or temporary-use) and owned by the household.
|  |Savings|   LandCult|   LandOwn|   Get Transfer|   Give Transfer|   Food Savings  |
|-
| CTL mean  | 77.874 | 59.268 | 52.947 | 45.355 | 11.697 | 84.232 |
|    | 221.477 | 296.900 | 290.894 | 197.831 | 63.897 | 162.141 |
| TUP*2014  | 16.860 | -2.306 | 2.668 | 41.972* | 2.859 | 42.649*** |
|     | 23.810 | 15.683 | 14.708 | 22.199 | 12.156 | 14.285 |
| TUP*2015  | 99.353*** | -13.748 | -15.752 | 1.113 | -1.097 | -0.864 |
|      | 23.810 | 15.683 | 14.708 | 22.199 | 12.156 | 14.285 |
| UCT*2014  | 32.947 | 16.540 | 18.160 | 18.809 | -13.676 | 12.801 |
|       | 30.128 | 19.849 | 18.621 | 28.107 | 15.395 | 18.087 |
| UCT*2015  | 33.092 | -43.36** | -41.645** | 8.081 | 3.656 | -3.824 |
|        | 30.128 | 19.849 | 18.621 | 28.107 | 15.395 | 18.087 |
| 2014  | 45.845*** | 8.691 | 12.616 | 22.381 | 17.91** | 44.303*** |
|         | 15.103 | 10.370 | 9.936 | 14.036 | 7.709 | 8.999 |
| 2015  | 73.739*** | 58.16*** | 56.603*** | 44.789*** | 11.187 | 84.232*** |
|          | 15.103 | 10.370 | 9.936 | 14.036 | 7.709 | 8.999 |
| Bsln NA  | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 |  |
|           | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 |   |
| Bsln2013  | 0.027** | 1.105 | -2.503 | 0.011 | 0.016 |    |
|            | 0.012 | 3.181 | 2.460 | 0.026 | 0.027 |     |
| F-stat  | 6.344 | 3.136 | 2.713 | 0.698 | 0.328 | 3.075 |
| N  | 1510.000 | 1510.000 | 1510.000 | 1510.000 | 1510.000 | 1510.000 |
| Beta_TUP_2014-Beta_Cash_2015  | -16.233 | 41.054 | 44.313* | 33.891 | -0.797 | 46.473** |
| Beta_TUP_2015-Beta_Cash_2015  | 66.261** | 29.612 | 25.894 | -6.969 | -4.753 | 2.960 |


*** Land Holdings

We also examine land ownership and cultivation in each year. We find no evidence that
either group is more or less likely to report owning or cultivating at least some
land, though this may be in part because land ownership and cultivation is already
very common. Anecdotaly, divesting from land ownership entirely could be seen as a
relatively drastic decision. However, members of the cash group who are involved in
agriculture report cultivating 65% less and owning 70% less land than the control
group. This raises the interesting question of whether the cash group was likely to
switch occupations from farming to non-farm self-employment.

It could also raise questions around the underlying logic of the agrarian focus
transfer in the TUP program, if unconstrained transfers prompt households to divest
from these opportunities. This concern may be validated somewhat by the fact that TUP
participants primarily stated a preference for small retail training and transfers
over small animal husbandry or vegetable gardening.

** Income

Income was reliably measured only in 2015, and so our estimates do not control for
baseline values. The control group in 2015 has a measured income of roughly 4325 SSP
per year, or roughly $540 US (assuming an exchange rate of around 8). The TUP group
sees a 327 SSP ($41 US, 7%) increase in annual average income, but with a highly
skewed distribution and large standard errors. The related figure shows that total
income is not particularly different among groups. Perhaps the main lesson is that
the TUP group has measurably more reported livestock-related income, and less farm
income, indicating a shift away from farming. The cash group may exhibit some
substitution away from farm and livestock, but as is evident graphically, we do not
observe sizable changes in total income for either treatment group. 

\newpage
#+name: income_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<income_analysis>>
return tab
#+end_src

#+Caption: Distribution of total observed income by group
#+NAME: fig:Income_group
[[../figures/Income_group.png]] 

#+name: tab:Income
#+caption: Average treatment effects by group-year on total value (in SSP) of income reported in 2015 by sector.
#+attr_latex: :environment longtable :align lrrrrrrr
|---------------------------+---------------+---------------+------------+------------|
|                           | Farm          | Livestock     | Non-Farm   | Total      |
|---------------------------+---------------+---------------+------------+------------|
| CTL mean                  | $773.05$      | $640.33$      | $3774.49$  | $4325.54$  |
|---------------------------+---------------+---------------+------------+------------|
| TUP                       | $-142.20^{*}$ | $281.12^{**}$ | $86.24$    | $327.83$   |
|                           | $(77.21)$     | $(126.30)$    | $(469.48)$ | $(455.95)$ |
| CSH                       | $-26.15$      | $-83.81$      | $61.80$    | $7.92$     |
|                           | $(100.82)$    | $(177.25)$    | $(620.53)$ | $(600.43)$ |
|---------------------------+---------------+---------------+------------+------------|
| N                         | $531.00$      | $380.00$      | $606.00$   | $671.00$   |
| F-stat                    | $1.75$        | $3.48$        | $0.02$     | $0.28$     |
|---------------------------+---------------+---------------+------------+------------|
| $\beta^{TUP}-\beta^{CSH}$ | $-116.05$     | $364.94^{**}$ | $24.44$    | $319.91$   |
|                           | $(105.79)$    | $(174.74)$    | $(651.27)$ | $(629.93)$ |
|---------------------------+---------------+---------------+------------+------------|

*** Recreating income table

#+name: income_table
#+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
data_nonfarm = "../../TUP-data/Endline/sections_8_17.csv"
data_farm = "../../TUP-data/Endline/Agriculture_cleaned.csv"

import statsmodels.api as sm
import pandas as pd
import numpy as np
from cfe.df_utils import df_to_orgtbl

df_a = pd.read_csv(data_assignment)
df_n = pd.read_csv(data_nonfarm)
df_f = pd.read_csv(data_farm)

df_n.set_index('res_id', inplace=True)
df_f.set_index('res_id', inplace=True)
df_a.set_index('RespID', inplace=True)

"""Nonagricultural income"""

df_n = df_n.filter(regex='^s16')

# s16_ic is nonfarm months received for a certain source of income for i = 1, 2, 3, 4, 5
# s16_id is nonfarm average income in each month for a certain source of income for i = 1, 2, 3, 4, 5
# using calculation from reported months and avg monthly income instead of reported annual income

nonfarm_yearly = pd.DataFrame() 
for i in range(1,6):
    nonfarm_yearly['source' + str(i)] = df_n['s16_' + str(i) + 'c']*df_n['s16_' + str(i) + 'd']

nonfarm_yearly['nonfarm_yearly_sum'] = nonfarm_yearly.sum(axis=1)


"""Agricultural income"""

# HARVEST VARIABLE NAMES: harvest_type (crop), harvest_size, harvest_unit (qty and unit harvested)
# harvest_price (market price in SSP), harvest_price_unit (market price unit), harvest_sold (qty sold assuming same unit as harvest_unit)

# in case market price unit and harvested/sold unit don't match, 
# calculate median of reported market prices for that crop (type) and market price unit (harvest_price_unit)
unit_prices = df_f.groupby(["harvest_type", "harvest_price_unit"])["harvest_price"].median()
# the format of unit_prices is the first index is harvest_type, the second index is harvest_price_unit, and
# the value is the median of market price units for that crop in that unit

k = 0
d = 0

df_f.insert(len(df_f.columns), 'adjusted_harvest_price', np.nan)

# iterate through each row
for i in range(len(df_f)):
    
    # if neither unit is NaN
    if pd.notna(df_f.iloc[i]['harvest_unit']):
        if pd.notna(df_f.iloc[i]['harvest_price_unit']):
    
            # if units don't match
            if df_f.iloc[i]['harvest_unit'] != df_f.iloc[i]['harvest_price_unit']:
                d = d +1
                # get the data frame for the particular type of crop
                find_type = unit_prices[unit_prices.index.get_level_values(0) == df_f.iloc[i]['harvest_type']]
        
                # check if there is no reported market price for that crop and unit
                if find_type[find_type.index.get_level_values(1) == df_f.iloc[i]['harvest_unit']].empty: 
                    k = k + 1
                else: 
                    # set the adjusted harvest price to be the median price for that type and harvest unit
                    df_f['adjusted_harvest_price'].iloc[i] = find_type[find_type.index.get_level_values(1) == df_f.iloc[i]['harvest_unit']][0]
        
            # if units match
            else: df_f['adjusted_harvest_price'].iloc[i] = df_f.iloc[i]['harvest_price']

#d is the number of rows for which the harvest unit and market price unit don't match (193)
#k is the number of rows for which the harvest unit and crop do not have any market price reported (7)
print("there are {} rows for which no one reported a market price for that crop and unit".format(k))

farm_yearly = pd.DataFrame() 
df_f["farm_income_yearly"] = df_f["harvest_size"]*df_f["adjusted_harvest_price"]

farm_yearly['farm_income_yearly_sum'] = df_f[ ['farm_income_yearly'] ].dropna().groupby("res_id")["farm_income_yearly"].sum()


# LIVESTOCK VARIABLE NAMES: livestock_type (animal), livestock_size, livestock_unit (qty and unit produced)
# livestock_price (market price in SSP), livestock_price_unit (market price unit), livestock_sold (qty sold assuming same unit as livestock_unit)

# in case market price unit and produced/sold unit don't match, 
# calculate median of reported market prices for that animal (type) and market price unit (livestock_price_unit)
unit_prices = df_f.groupby(["livestock_type", "livestock_price_unit"])["livestock_price"].median()
# the format of unit_prices is the first index is livestock_type, the second index is livestock_price_unit, and
# the value is the median of market price units for that animal in that unit

k = 0
d = 0

df_f.insert(len(df_f.columns), 'adjusted_livestock_price', np.nan)

# iterate through each row
for i in range(len(df_f)):
    
    # if neither unit is NaN
    if pd.notna(df_f.iloc[i]['livestock_unit']):
        if pd.notna(df_f.iloc[i]['livestock_price_unit']):
    
            # if units don't match
            if df_f.iloc[i]['livestock_unit'] != df_f.iloc[i]['livestock_price_unit']:
                d = d + 1
                # get the data frame for the particular type of animal
                find_type = unit_prices[unit_prices.index.get_level_values(0) == df_f.iloc[i]['livestock_type']]
        
                # check if there is no reported market price for that animal and unit
                if find_type[find_type.index.get_level_values(1) == df_f.iloc[i]['livestock_unit']].empty: 
                    k = k + 1
                else: 
                    # set the adjusted livestock price to be the median price for that type and livestock unit
                    df_f['adjusted_livestock_price'].iloc[i] = find_type[find_type.index.get_level_values(1) == df_f.iloc[i]['livestock_unit']][0]
        
            # if units match
            else: df_f['adjusted_livestock_price'].iloc[i] = df_f.iloc[i]['livestock_price']

#d is the number of rows for which the livestock unit and market price unit don't match (9)
#k is the number of rows for which the livestock unit and crop do not have any market price reported (3)
print("there are {} rows for which no one reported a market price for that animal and unit".format(k))
 
df_f["livestock_income_yearly"] = df_f["livestock_size"]*df_f["adjusted_livestock_price"]

farm_yearly['livestock_income_yearly_sum'] = df_f[ ['livestock_income_yearly'] ].dropna().groupby("res_id")["livestock_income_yearly"].sum()

df = pd.merge(nonfarm_yearly[ ['nonfarm_yearly_sum'] ], farm_yearly, how='outer', left_index=True, right_index=True)
df = pd.merge(df, df_a[ ['Group'] ].join(pd.get_dummies(df_a['Group'])), how='inner', left_index=True, right_index=True)
df.insert(len(df.columns), 'total', df[['nonfarm_yearly_sum', 'farm_income_yearly_sum', 'livestock_income_yearly_sum']].sum(axis=1))
df.insert(len(df.columns), 'constant', 1)

df.rename(columns = {'farm_income_yearly_sum': "Farm", 'livestock_income_yearly_sum': "Livestock", 
                   'nonfarm_yearly_sum': "Non-Farm", 'total': "Total"}, inplace=True)

outcomes = ["Farm", "Livestock", "Non-Farm", "Total"]
controls = ['constant', 'TUP', 'UCT']
models_ols = {}
d = {}

for outcome in outcomes: 
    temp_df = df[ [outcome] + controls].dropna()
    
    models_ols[outcome] = sm.OLS(temp_df[outcome], temp_df[controls]).fit()
    
    # for computing the difference in TUP and Cash coefficients
    beta_cash_TUP = models_ols[outcome].t_test("TUP - UCT = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP = models_ols[outcome].t_test("TUP - UCT = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP <= 0.01: 
        beta_cash_TUP = str(round(beta_cash_TUP, 3)) + '***'
    elif pvalue_cash_TUP <= 0.05: 
        beta_cash_TUP = str(round(beta_cash_TUP, 3)) + '**'
    elif pvalue_cash_TUP <= 0.1: 
        beta_cash_TUP = str(round(beta_cash_TUP, 3)) + '*'

    d[outcome] = [round(models_ols[outcome].params[0], 3), models_ols[outcome].bse[0],
                  round(models_ols[outcome].params[1], 3), models_ols[outcome].bse[1], 
                  round(models_ols[outcome].params[2], 3), models_ols[outcome].bse[2],
                  models_ols[outcome].fvalue, models_ols[outcome].nobs, 
                  beta_cash_TUP]
   

    for x in range(0,3):
        # indexes account for standard errors being in the final table
        # e.g. second coefficient is beneath standard error so change d[outcome][2] not d[outcome][1] and same for next coefficients
        if models_ols[outcome].pvalues[x] <= 0.01: 
            d[outcome][x*2] = str(d[outcome][x*2]) + '***'
        elif models_ols[outcome].pvalues[x] <= 0.05: 
            d[outcome][x*2] = str(d[outcome][x*2]) + '**'
        elif models_ols[outcome].pvalues[x] <= 0.1:
            d[outcome][x*2] = str(d[outcome][x*2]) + '*'
    

final = pd.DataFrame(data=d)

final.rename(index = {0: "CTL mean", 1: " ", 2: "TUP", 3: "  ", 
                      4: "UCT", 5: "   ", 6: "F-stat", 7: "N", 8: "beta_TUP-beta_CASH"}, inplace=True)

newdf = df_to_orgtbl(final, float_fmt = '%.3f')
print(newdf)

#+end_src

#+RESULTS: income_table
there are 7 rows for which no one reported a market price for that crop and unit
there are 3 rows for which no one reported a market price for that animal and unit
|  |Farm|   Livestock|   Non-Farm|   Total  |
|-
| CTL mean  | 917.315*** | 924.611*** | 3326.162*** | 4317.337*** |
|    | 64.809 | 164.390 | 527.282 | 537.231 |
| TUP  | -139.780 | 117.096 | 964.323 | 1253.478 |
|     | 100.918 | 215.596 | 829.082 | 844.725 |
| UCT  | 48.833 | -282.164 | 507.132 | 510.939 |
|      | 133.947 | 298.534 | 1077.298 | 1097.625 |
| F-stat  | 1.309 | 0.983 | 0.683 | 1.101 |
| N  | 532.000 | 305.000 | 690.000 | 690.000 |
| beta_TUP-beta_CASH  | -188.613 | 399.260 | 457.192 | 742.540 |



** Exposure to Conflict

In 2014, households were surveyed shortly after the NGO's offices had re-opened in
the wake of the outbreak of widespread armed conflict. Respondents were asked a short
set of questions about whether they were directly affected, and if so, in what way.
There has only been a few incidents of violence near Yei town at that point, and the most
directly involved ethnic groups made up a small portion of the local population. There
is no clear comparison group to which we might compare our sample, and the economic
climate changed over this same period in several ways that were probably not directly
caused by the violence. As such, we have no clear means of identifying the effect of
the conflict itself on household welfare. Nonetheless, it is interesting to consider
correlates with self-reported exposure to the conflict, and to see if program
assignment had any effect on households' exposure or response.

Our main outcomes of interest are whether individuals say they were "worried" or
"directly affected" by the violence, unable to invest in a farm or business as a
result, migrated as a cautionary measure, or did something else to protect the lives
of family members. A final question among those who took no cautionary measures was
whether this because they did not have the means (i.e. "NoMeans"). TUP participants
are 24% (13 pp.) less likely to report having been "affected" by the conflict, and
38% (6 pp.) less likely to report that they were affected specifically by being
unable to plant crops or invest in their business. This was the second most common
way in which households reported being affected behind "needed to relocate or
migrate", where respondents are not clearly different. Nonetheless, this raises the
possibility that having received a significant asset transfer and the expectation of
NGO support around the outbreak of
conflict may have helped mitigate the conflict's negative effect on investment and
protect households from being affected overall.

#+name: conflict_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<conflict_analysis>>
return Table
#+end_src

\newpage

#+name: tab:conflict_exposure
#+caption: Average treatment effects by group-year on the probability of having been affected in a significant way by the outbreak of violence in late 2013
#+attr_latex: :environment longtable :align lrrrrrrr
|----------+---------------+--------------+--------------+--------------+--------------+--------------|
|          | Affected      | Migrated     | NoInvest     | NoMeans      | ProtectLives | Worried      |
|----------+---------------+--------------+--------------+--------------+--------------+--------------|
| CTL mean | $0.53^{***}$  | $0.33^{***}$ | $0.16^{***}$ | $0.33^{***}$ | $0.38^{***}$ | $0.93^{***}$ |
|          | $( 0.03)$     | $( 0.02)$    | $( 0.02)$    | $( 0.02)$    | $( 0.03)$    | $( 0.01)$    |
| TUP      | $-0.13^{***}$ | $0.04$       | $-0.06^{**}$ | $-0.06$      | $0.02$       | $-0.02$      |
|          | $( 0.04)$     | $( 0.04)$    | $( 0.03)$    | $( 0.04)$    | $( 0.05)$    | $( 0.02)$    |
|----------+---------------+--------------+--------------+--------------+--------------+--------------|
| F-stat   | $9.20$        | $0.96$       | $3.95$       | $2.55$       | $0.19$       | $0.49$       |
| N        | $601.00$      | $655.00$     | $655.00$     | $655.00$     | $585.00$     | $603.00$     |
|----------+---------------+--------------+--------------+--------------+--------------+--------------|

#+Caption: % of Sample reporting exposure to conflict by group.
#+NAME: fig:conflict_exposure
[[../figures/conflict_exposure.png]] 

*** Recreating the exposure to conflict table

In this table, the 'Cash' group is included within the 'Control' group
because the cash transfers weren't given out until immediately after
the midline survey, and this table uses data from the midline
survey. All variables are binary variables. CTL mean comes from the constant
and standard error in the regression.

Affected and Worried are whether or not respondents answered that they were "directly affected
by any of the violence" or "worried that the conflict would affect you
since December," respectively. NoInvest is whether they said that one
way they were directly affected is that they could not invest in
business. NoMeans is whether they said they did not do anything to
protect the lives of family members or to protect their assets, and
listed the reason as "didn't have the means." ProtectLives is whether
the respondent did anything (including migration) besides "nothing" to
protect the lives of their family members - this could be migrating to
stay with friend or family, migrating and finding own accommodation,
looking for protection with NGO, or looking for protection with
government or military. Migrated is if the respondent listed migration as one of
the ways they were directly affected, and/or if they said they migrated to
protect the lives of their family. 

Differences with previous code: 
-this code adds relevant variables ('s22_4d_m' and 's22_4e_m': other
ways you were directly affected by conflict) which were not renamed to
include 'conflict'
-N for NoMeans in this table is lower than in the previous table
because the question was: if they answered no to the previous two
questions, 'why did you do nothing to protect yourself and assets?'
So only a subset of people who said they didn't do anything to protect 
the lives of their family members AND didn't do anything to protect 
their assets would be asked this question and therefore able to answer 
'didn't have the means' to the question. Therefore, N for NoMeans in 
this table is lower than in the previous table because it takes into 
account that the NaNs should not be changed to 0s because people who 
had NaNs either did something to protect the lives of their family 
members or to protect their assets

Next steps: 
-recreate bar graph figure

#+name: exposure_to_conflict
#+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
data_midline  = "../../TUP-data/Midline/TUP_midline.dta"
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
import statsmodels.api as sm
import pandas as pd
import numpy as np
from cfe.df_utils import df_to_orgtbl

df_m = pd.read_stata(data_midline)
df_a = pd.read_csv(data_assignment)

# change 'Cash' to 'Control' because the cash transfers weren't given out until immediately after midline survey
df_a.replace(to_replace="Cash", value="Control", inplace=True)

df = pd.merge(df_m, df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")

df.set_index(['idno', 'Group'], inplace=True)

# add relevant variables which were not renamed to include 'conflict'
named_without_conflict = df.filter(regex="^s22_.")

# make a dataframe of variables with 'conflict' in them and join it with the other relevant variables
df = df.filter(like='conflict')
df = df.join(named_without_conflict)

# Affected and Worried are the binary variables of whether they responded yes (1) or no (0) to being affected and worried
df.rename(columns={"conflict_affected_m": "Affected", "conflict_worried_m": "Worried"}, inplace = True)

# NoMeans variable is whether they answered 'didn't have the means' to question 'why did you do nothing to protect yourself and assets?'
# NaNs should not be counted as 0s because some people were not asked this question, so the people who were asked this question would be a smaller subset 
df.replace({"Didn't have the means": 1, 
 "Not necessary/Didn't fee in danger": 0, 
 "Didn't have the suport from othes": 0, 
 "Felt in danger but decided to rist it": 0}, inplace=True)

df.rename(columns={'conflict_whynotprotect_m': 'NoMeans'}, inplace = True)

# no invest is only if directly affected by could not invest in business
affected_business = ['conflict_affected1_m', 'conflict_affected2_m', 'conflict_affected3_m', 's22_4d_m', 's22_4e_m']
df_business = df[affected_business]

df_business.replace({"Could not plant crop or invest in business": 1, 
 "Death in Familuy": 0, 
 "Needed to elocate or migrate": 0, 
 "Loss of assets": 0, 
 "HH memebr went to fight": 0,
 "Business was harmed": 0}, inplace=True)

df_business.insert(len(df_business.columns), 'NoInvest', df_business[affected_business].sum(axis=1))

df = df.join(df_business['NoInvest'])
del df_business

# migrated is only if directly affected by migration or migrated to protect lives 
affected_migration = ['conflict_affected1_m', 'conflict_affected2_m', 'conflict_affected3_m', 's22_4d_m', 's22_4e_m', 'conflict_protectlives_m']
df_migration = df[affected_migration]

df_migration.replace({"Could not plant crop or invest in business": 0, 
 "Death in Familuy": 0, 
 "Needed to elocate or migrate": 1, 
 "Loss of assets": 0, 
 "HH memebr went to fight": 0,
 "Business was harmed": 0,
 "Migrate to stay with friend/family": 1,
 "Migrated and found new accommodation": 1,                     
 "Nothing": 0,                     
 "Looked for protection with Govt. Military": 0,
 "Looked for protection with NGO": 0}, inplace=True)

df_migration.insert(len(df_migration.columns), 'Migrated', df_migration[affected_migration].sum(axis=1))
# replace 2s with 1s because the 2s just double count migration
df_migration.replace(to_replace=2, value=1, inplace=True)

df = df.join(df_migration['Migrated'])
del df_migration

# protect lives is anything (including migration) besides nothing 
df['conflict_protectlives_m'].replace({"Nothing": 0, 
                                       "Migrate to stay with friend/family": 1, 
                                       "Migrated and found new accommodation": 1, 
                                       "Looked for protection with Govt. Military": 1,
                                       "Looked for protection with NGO": 1}, inplace = True)

df.rename(columns={'conflict_protectlives_m': 'ProtectLives'}, inplace = True)

outcomes = ["Affected", "Migrated", "NoInvest", "NoMeans", "ProtectLives", "Worried"]

df.insert(len(df.columns), 'constant', 1)

# make 'Group' index a column
df.reset_index(level=['Group'], inplace=True)

df = df.join(pd.get_dummies(df['Group']))
controls = ["constant", "TUP"]

models_ols = {}
d={}

for outcome in outcomes: 
    
    temp_df = df[controls + [outcome]].dropna()
    models_ols[outcome] = sm.OLS(temp_df[outcome], temp_df[controls]).fit()
    
    # make a dictionary with the coefficients, standard errors, F-Stats and Ns
    d[outcome] = [round(models_ols[outcome].params[0], 3), models_ols[outcome].bse[0],
                  round(models_ols[outcome].params[1], 3), models_ols[outcome].bse[1], 
                  models_ols[outcome].fvalue, models_ols[outcome].nobs]
   
    if models_ols[outcome].pvalues[0] <= 0.01: 
        d[outcome][0] = str(d[outcome][0]) + '***'
    elif models_ols[outcome].pvalues[0] <= 0.05: 
        d[outcome][0] = str(d[outcome][0]) + '**'
    elif models_ols[outcome].pvalues[0] <= 0.1:
        d[outcome][0] = str(d[outcome][0]) + '*'
        
    # second coefficient is beneath standard error so add asterisks to d[outcome][2] not d[outcome][1]
    if models_ols[outcome].pvalues[1] <= 0.01: 
        d[outcome][2] = str(d[outcome][2]) + '***'
    elif models_ols[outcome].pvalues[1] <= 0.05: 
        d[outcome][2] = str(d[outcome][2]) + '**'
    elif models_ols[outcome].pvalues[1] <= 0.1:
        d[outcome][2] = str(d[outcome][2]) + '*'
    
    del temp_df

final = pd.DataFrame(data=d)
final.rename(index = {0: "CTL mean", 1: "  ", 2: "TUP", 3: "   ", 4: "F-stat", 5: "N"}, inplace=True)
newdf1 = df_to_orgtbl(final.loc[ ["CTL mean", "  ", "TUP", "   "] ], float_fmt = '%.3f')
newdf2 = df_to_orgtbl(final.loc[ ["F-stat", "N"] ], float_fmt = '%.3f')
print("|-\n" + newdf1 + "|-\n" + newdf2 + "|-\n")

#+end_src

#+RESULTS: exposure_to_conflict
|----------+-----------+----------+----------+----------+--------------+----------|
|          |  Affected | Migrated | NoInvest |  NoMeans | ProtectLives |  Worried |
|----------+-----------+----------+----------+----------+--------------+----------|
| CTL mean |  0.532*** | 0.373*** | 0.177*** | 0.538*** |     0.311*** | 0.928*** |
|          |     0.026 |    0.025 |    0.018 |    0.031 |        0.025 |    0.014 |
| TUP      | -0.127*** |   -0.005 | -0.076** | -0.109** |        0.040 |   -0.016 |
|          |     0.042 |    0.041 |    0.030 |    0.052 |        0.040 |    0.023 |
|----------+-----------+----------+----------+----------+--------------+----------|
|          |  Affected | Migrated | NoInvest |  NoMeans | ProtectLives |  Worried |
|----------+-----------+----------+----------+----------+--------------+----------|
| F-stat   |     9.203 |    0.013 |    6.609 |    4.381 |        1.003 |    0.486 |
| N        |   601.000 |  606.000 |  606.000 |  402.000 |      585.000 |  603.000 |
|----------+-----------+----------+----------+----------+--------------+----------|

* Concluding Remarks
  
BRAC's South Sudan pilot of the TUP program represents the only such test of the
ultra-poor graduation framework conducted in an area of significant political and
economic instability. It also represents one of the only direct comparisons of this
model to a similarly expensive unconditional cash transfer, arguably its most
sensible benchmark for success. As such, while our study may not generalize to
contexts with high-functioning cash economies and relative political stability, it
provides suggestive evidence as to the best way of transfering wealth in order to
help poor and vulnerable households.

Cash transfers appear to increase consumption and possibly shift investment from
agriculture to non-farm activities, without a related increase in wealth or income.
Conversely, the TUP program increased wealth and directly shifted work from
agriculture to livestock, with increased consumption in the short run. We also find
that having received asset transfers dampened the negative investment effects
following the outbreak of violence.[fn:: Whether a cash transfer would have had a
similar mitigating effect is hard to say.] We tentatively conclude that targeted
asset transfers can play a constructive role in helping poor, self-employed
households when they face economic uncertainty. And while cash increases household
consumption, the goal of improving income or wealth is well served by the additional
services that the ultra-poor graduation framework offer.
  
* References
   \renewcommand{\refname}{}
   \printbibliography


* COMMENT Extra Analysis

** Good-level analysis

Next, \ref{tab:consumption_full} sets aside these aggregated measures to look more
carefully at potential changes in the composition of consumption in each group. Given
the large number of zeros, we use a linear model to consider first the 
frequency of non-zero consumption of each good among treatment and control
households, then look at levels of consumption among households with non-zero
consumption. \Tab{consumption_full} presents point estimates.

A few changes in the composition of consumption are interesting. TUP households appear to consume 17% less
sorghum (often considered an inferior good in Yei) and more on rice, which is
considered a higher-quality staple. While almost everyone reports some health
spending over the past month, both treatment groups spent more, though only
statistically significant in the cash group, which saw a 50% increase over the
control group. The cash group was also 30% (14 pp) more likely to have spent money
on funerals, though they did not spend more on average.

#+name: consumption_disaggreate_results
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
<<consumption_analysis>>
from matplotlib import pyplot as plt
#~ Only keep disaggregate items
Goods = C.filter(regex="^c_").rename(columns=lambda col: col[2:] if col.startswith("c_") else col)
#~ 1 if coded as >0, 0 if zero, keep NaN's missing, since those are only from missing surveys.
Nonzero = Goods.applymap(lambda i: float(i>0) if not np.isnan(i) else np.nan)
#~ Restrict Goods df to positive responses.
Goods = Goods.replace(0,np.nan)
too_many_zeros = 30
many_zeros = [item for item in Goods if Goods[item].notnull().sum()<too_many_zeros]
Nonzero = Nonzero.drop(many_zeros,1)
Goods = Goods.drop(many_zeros,1)

#~ Merge in Control Vars
controls = ["cons","TUP","CSH"]
Goods = Goods.join(C[controls],how="left")
Nonzero = Nonzero.join(C[controls],how="left")
Items = [item[:-2] for item in Goods if item.endswith("_e")]
CTL = Goods[controls].sum(axis=1)==1 #~ i.e. only constant ==1, TUP & UCT ==0
Goods_ctl_mean =   Goods.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Zeros_ctl_mean = Nonzero.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])

Zero, Zero_se = reg_table(regressions(Nonzero,"_e", outcomes=Items, Baseline="_b"), resultdf=True, Transpose=True)
Good, Good_se = reg_table(regressions(Goods,"_e", outcomes=Items, Baseline="_b"  ), resultdf=True, Transpose=True)
#~ Make full table of Standard errors
SE = Zero_se[["TUP","CSH"]].join(Good_se[["TUP","CSH"]], lsuffix=" (%>0)", rsuffix=" (Amt.)")

#~ Make full table of point estimates and control-group means
FullTable = pd.DataFrame({"Mean (CTL)":Goods_ctl_mean, "% >0 (CTL)":Zeros_ctl_mean}).join(Zero[["TUP","CSH"]])
FullTable = FullTable.join(Good[["TUP","CSH","N"]], lsuffix=" (%>0)", rsuffix=" (Amt.)")

#~ Make % change graph
fig, ax = plt.subplots(2,1, figsize=(6,9))
for i, group in enumerate(("TUP","CSH")):
    pct_change = FullTable[group+" (Amt.)"]/FullTable["Mean (CTL)"]
    se_change  = SE[group+" (Amt.)"]/FullTable["Mean (CTL)"]
    pct_change.sort()
    pct_change.plot(kind="bar", yerr=SE[group+" (Amt.)"], ax=ax[i])
    ax[i].set_title(group, fontsize=6)
fig.savefig("../figures/Consumption.png")

FullTable = df_to_orgtbl(FullTable, sedf=SE)
return FullTable
#+end_src

#+name: tab:consumption_full
#+caption: Control group means and estimated treatment effects for percent consuming any and total amounts consumed.
#+attr_latex: :environment longtable :align lrrrrrrr
 #+RESULTS: consumption_disaggreate_results

** Disaggregate Asset Results 

#+name: asset_disaggreate_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<assets_disaggreate_analysis>>
return tab
#+end_src

#+name: tab:asset_disaggregate
#+caption: Control means and treatment effects for Assets owned by >40 households
#+RESULTS: asset_disaggreate_results
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|
|              | # own  |              |             | Value     |                |                |          |
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|
| Item         | CTL    | TUP          | UCT        | CTL       | TUP            | UCT           | N        |
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|
| Pangas       | $1.06$ | $0.01$       | $0.13^{**}$ | $11.86$   | $1.66^{**}$    | $0.04$         | $410.00$ |
| Chickens     | $3.79$ | $0.70$       | $-0.32$     | $154.35$  | $23.31$        | $0.80$         | $162.00$ |
| Mobile       | $1.88$ | $-0.09$      | $0.08$      | $113.96$  | $2.62$         | $1.70$         | $569.00$ |
| Radio        | $1.62$ | $0.84$       | $-0.40$     | $57.25$   | $4.98$         | $5.10$         | $333.00$ |
| Shed         | $1.22$ | $-0.14$      | $-0.22$     | $48.81$   | $37.57$        | $6.81$         | $53.00$  |
| Stoves       | $1.44$ | $0.34$       | $0.31$      | $20.32$   | $18.19^{**}$   | $8.31$         | $84.00$  |
| Potspans     | $4.46$ | $-0.06$      | $-0.23$     | $102.73$  | $15.90$        | $-15.40$       | $582.00$ |
| Ducks        | $5.72$ | $4.26^{***}$ | $-0.16$     | $230.93$  | $109.99^{***}$ | $-19.34$       | $223.00$ |
| Motorcycle   | $1.51$ | $-0.48$      | $0.12$      | $2288.48$ | $300.46$       | $-196.32$      | $66.00$  |
| Chairtables  | $5.02$ | $0.25$       | $0.39$      | $167.62$  | $19.00$        | $-24.73$       | $638.00$ |
| Net          | $3.07$ | $0.03$       | $-0.08$     | $24.49$   | $0.66$         | $-3.81$        | $382.00$ |
| Axes         | $1.02$ | $0.03$       | $-0.02$     | $17.74$   | $0.02$         | $-3.94^{**}$   | $218.00$ |
| Smallanimals | $3.39$ | $0.29$       | $-0.90$     | $767.26$  | $-151.35$      | $-311.05^{**}$ | $155.00$ |
| Charcoal     | $2.20$ | $-0.26$      | $-0.83$     | $35.81$   | $-1.43$        | $-4.65$        | $176.00$ |
| Bicycle      | $6.34$ | $-5.46$      | $-5.52$     | $272.90$  | $-31.50$       | $-42.67$       | $135.00$ |
| Bed          | $3.17$ | $-0.23$      | $-0.40$     | $300.64$  | $19.32$        | $-57.78^{*}$   | $628.00$ |
| Tv           | $1.48$ | $-0.36$      | $-0.26$     | $380.45$  | $121.95$       | $348.23^{**}$  | $45.00$  |
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|


* COMMENT Code appendix
  
** Food Security

 #+name: foodsecure_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle foodsecurity.py
 #~ DevLunch_analysis.py
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
 D = full_data(DIR = DATADIR,balance=[])
 WEEKLY = True

 codes = {"1-2 times a week": 3,
         "3-6 times a week": 2,
         "Everyday": 1,
         "everyday": 1,
         "Less than once a week": 4,
         "less than once a week": 4,
         "Never": 5,
         "never": 5}

 recode = lambda x: codes.setdefault(x,x)

 Aval2013 = D.filter(regex="^fs_.*_b").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Aval2014 = D.filter(regex="^fs_.*_m").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Aval2015 = D.filter(regex="^fs_.*_e").rename(columns=lambda x: x[3:-2]).applymap(recode)

 if WEEKLY:
     weekly = lambda i: float(i<3) if pd.notnull(i) else np.nan
     Aval2013 = Aval2013.applymap(weekly)
     Aval2014 = Aval2014.applymap(weekly)      
     Aval2015 = Aval2015.applymap(weekly)
 
 index_vars = "worried,portions,fewmeals,nofood,hungry,wholeday".split(",")
 Outcomes = index_vars+["z-score"]
 #~ Creates Year dummies, z-scores and baseline values as `var'2013
 for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
     Aval["Year"]=Year
     if not weekly:
        for var in index_vars:
            Aval[index_vars] = (Aval[index_vars]-Aval[index_vars].mean())/Aval[index_vars].std()
     FS_sum = Aval[index_vars].sum(axis=1)
     Aval["z-score"] = (FS_sum-FS_sum.mean())/FS_sum.std()
     for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
    
 Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
 Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
 Vals = Vals.join(D[["TUP","CSH"]])

 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

 Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 Vals=Vals.loc[2014:2015]
 regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = Vals["TUP"]+Vals["CSH"] ==0
 CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+end_src

** Consumption

 #+name: lambda_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/SS-lambdas.py

 import numpy as np
 import pandas as pd
 import cfe.estimation as nd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append("../../data")
 from TUP import full_data, consumption_data, regressions, reg_table, df_to_orgtbl, mobile_data
 food =  ['cereals', 'maize', 'sorghum', 'millet', 'potato', 'sweetpotato', 'rice', 'bread', 'beans', 'oil', 'salt', 'sugar', 'meat', 'livestock', 'poultry', 'fish', 'egg', 'nuts', 'milk', 'vegetables', 'fruit', 'tea', 'spices', 'alcohol', 'otherfood']
 month = ['fuel', 'medicine', 'airtime', 'cosmetics', 'soap', 'transport', 'entertainment', 'childcare', 'tobacco', 'batteries', 'church', 'othermonth']    
 ConsumptionItems = food+['airtime','fuel']
 mobile=True

 D = full_data(DIR=DATADIR)
 C, HH, T = consumption_data(D,WRITE=False,include2016=False)
 HH['log HHSIZE'] = HH["hh_size"].apply(np.log)
 HH = HH.drop("hh_size",1)
 y,z = C.replace(0,np.nan).apply(np.log).sort_index(level=[0,1,2])[ConsumptionItems].copy(),HH.sort_index(level=[0,1,2]).copy()
 y.index.names, z.index.names = ['j','t','mkt'], ['j','t','mkt']
 keep = pd.notnull(y.index.get_level_values("mkt"))
 y,z = y.loc[keep,:].align(z,join="left",axis=0)
 b,ce,d,sed= nd.estimate_reduced_form(y,z,return_se=True,VERBOSE=True)
 ce = ce.dropna(how='all')
 print("Getting Loglambdas")
 bphi,logL=nd.get_loglambdas(ce,TEST="warn")
 try:
    xrange
    logL.to_pickle(DATADIR + "ss-lambdas.df")
 except NameError: logL.to_pickle(DATADIR + "ss-lambdas3.df")
 
 if mobile:
     M,Mc,Mhh = mobile_data(use_dates=True,DIR = DATADIR+"Mobile/")
     y = Mc.replace(0,np.nan).apply(np.log).sort_index(level=[0,1,2]).filter(items=ConsumptionItems).copy()
     z = Mhh.sort_index(level=[0,1,2]).copy()
     y.index.names, z.index.names = ['j','t','mkt'], ['j','t','mkt']
     keep = pd.notnull(y.index.get_level_values("mkt"))
     y,z = y.loc[keep,:].align(z,join="left",axis=0)
     b,ce,d,sed= nd.estimate_reduced_form(y,z,return_se=True,VERBOSE=True)
     ce = ce.dropna(how='all')
     print("Getting Loglambdas")
     Mbphi,MlogL=nd.get_loglambdas(ce,TEST="warn")
     MlogL -= MlogL.mean()
     MlogL /= MlogL.std()
     MlogL = MlogL.unstack('t').drop('4February',1).stack()
     try:
       xrange
       MlogL.to_pickle(DATADIR + "ss-lambdas_mobile.df")
     except NameError: MlogL.to_pickle(DATADIR + "ss-lambdas_mobile3.df")

 #+end_src 

 #+name: consumption_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/SS-consumption.py
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 from TUP import full_data, consumption_data, regressions, reg_table, df_to_orgtbl, mobile_data
 
 USE_LAMBDAS = False #~ Whether to use the output of SS-lambdas.py as an outcome (You may have to run it first.)
 TGROUP_FE   = True  #~ Whether to use TUP & CSH dummy variables as controls (I'm pretty sure we should be)

 food = ['cereals', 'maize', 'sorghum', 'millet', 'potato', 'sweetpotato', 'rice', 'bread', 'beans', 'oil', 'salt', 'sugar', 'meat', 'livestock', 'poultry', 'fish', 'egg', 'nuts', 'milk', 'vegetables', 'fruit', 'tea', 'spices', 'alcohol', 'otherfood']
 month = ['fuel', 'medicine', 'airtime', 'cosmetics', 'soap', 'transport', 'entertainment', 'childcare', 'tobacco', 'batteries', 'church', 'othermonth']    
 year = ['clothesfootwear', 'womensclothes', 'childrensclothes', 'shoes', 'homeimprovement', 'utensils', 'furniture', 'textiles', 'ceremonies', 'funerals', 'charities', 'dowry', 'other']    


 D = full_data(DIR=DATADIR)
 C, HH, T = consumption_data(D,WRITE=True) #"csv")
 if USE_LAMBDAS:
    logL = pd.read_pickle(DATADIR + "ss-lambdas.df")
    logL.index.names=["HH","Year","Location"]
    C = C.join(logL,how="left").rename(columns={"loglambda":"$\log\lambda_{it}$"})
 C = C.reorder_levels([1,2,0])#.sort_index(level=0)
 keep = pd.notnull(C.index.get_level_values("Location"))
 C = C.loc[keep,:]

 # Make aggregate variables
 C["Food"]   = C.filter(items=food).sum(axis=1).replace(0,np.nan)
 C["Month"]   = C.filter(items=food).sum(axis=1)
 C["Year"]   = C.filter(items=food).sum(axis=1)
 C["Tot"]    = C[["Food","Month","Year"]].sum(axis=1).replace(0,np.nan)

 def align_indices(df1,df2):
    """
    Reorder levels of df2 to match that of df1
    Must have same index.names
    """
    I1, I2 = df1.index, df2.index
    try: assert(not set(I1.names).difference(I2.names))
    except AssertionError: raise ValueError("Index names must be the same")
    new_order = []
    for lvl in I1.names: new_order.append(I2.names.index(lvl))
    df2 = df2.reorder_levels(new_order)
    return df1, df2

 def winsorize(Series, **kwargs):
    """
    Need to implement two-sided censoring as well.
    WARNING: if Top<0, all zeros will be changed to Top
    """
    percent    = kwargs.setdefault("percent",99)
    stdev      = kwargs.setdefault("stdev",False)
    drop       = kwargs.setdefault("drop",False)
    drop_zeros = kwargs.setdefault("drop_zeros",True)
    twoway     = kwargs.setdefault("twoway",False)

    if drop_zeros: S = Series.replace(0,np.nan).dropna()
    else: S = Series.dropna()
    N_OBS = S.notnull().sum()
    if N_OBS<10: return S

    if percent: Top = np.percentile(S, percent)
    if stdev:   
        Top =  S.dropna().mean()
        Top += stdev*S.dropna().std()
    try: assert((not drop_zeros) or Top>0)
    except AssertionError: raise ValueError("Top < 0 but zeros excluded")
    if drop: replace_with = np.nan
    else:    replace_with = Top

    _Series = Series.copy()
    _Series[_Series>Top]=replace_with

    if not twoway: return _Series
    else:
        kwargs['twoway'] = False
        return -1*winsorize(-1*_Series, **kwargs)

 def USD_conversion(Exp,exchange_rate=1.,PPP=1.,inflation=1.,time='Year'):
    """
    Convert nominal local currency into price- and inflation-adjusted USD

    Exp - A numeric or pd.Series object 
    exchange_rate - Taken as LCU/USD. 
    PPP - Taken as $Real/$nominal
    inflation - Taken as % inflation compared to some baseline.
    time - If a list is passed, `time' indicates the name or position of the time level in Exp.index
        NOTE: This has to be a cumulative number, so if inflation is 20% for two straight years, that year should be divided by (1+.2)**2
    Final calculation will basically be Exp_usdppp = Exp*(exchange_rate*PPP)/inflation

    if pd.Series are passed for any kwarg, index name needs to be in the multi-index of Exp.
    """
    if type(inflation)==list: inflation=[1./i for i in inflation]
    else: inflation = 1/inflation
    if type(exchange_rate)==list: exchange_rate=[1./i for i in exchange_rate]
    else: exchange_rate = 1/exchange_rate
    
    _Exp = Exp.copy()
    VARS = (exchange_rate, PPP,inflation)
    if list in map(type,VARS):
        if time in _Exp.index.names: time=_Exp.index.names.index(time)
        time = _Exp.index.levels[time]
    for var in VARS:
        if type(var)==list: var=pd.Series(var,index=time)
        try: _Exp = _Exp.mul(var)
        except ValueError: #~ If Series index doesn't have a name, try this...
            var.index.name = var.name
            _Exp = _Exp.mul(var)
    return _Exp

 def percapita_conversion(Exp,HH,children=["boys","girls"],adult_equivalent=1.,minus_children='hh_size'):
    """
    Returns household per-capita expenditures given:
        `Exp'- Total household expenditures
        `HH' - Total number of individuals in the household
            If HH is a pd.DataFrame, Exp is divided by HH.sum(1)
            if `children' is the name of a column or a list of column names, 
            those first get divided by the factor adult_equivalent
    """
    try: HH.columns #~ If HH is a series, just divide though
    except AttributeError: return Exp.div(HH)
    _HH = HH.copy()
    if type(children)==str: children=[children]
    children = _HH.columns.intersection(children).tolist()
    if minus_children: _HH[minus_children] -= _HH[children].sum(1)
    if children: _HH[children] *= adult_equivalent
    Exp,_HH = align_indices(Exp,_HH)
    return Exp.div(_HH.sum(1).replace(0,1))

 #~ Source: http://data.worldbank.org/indicator/PA.NUS.PRVT.PP?locations=SS&name_desc=false
 xrate = [ 2.161, 2.162, 3.293] #~ Using PPP adjusted xrate and just setting PPP=1.
 PPP = 1.
 inflation= 1. #~ Bank data uses international $, which is inflation adjusted.
 C["Exp_usd"] = winsorize(USD_conversion(C["Tot"],exchange_rate=xrate,PPP=PPP,inflation=inflation))
 C["Tot_pc"] = percapita_conversion(C["Exp_usd"],HH,adult_equivalent=.5)
 #C["Exp_usdpc_tc"] = winsorize(C["Exp_usdpc"])

 #C["z-score"]  = (C["Tot"]-C["Tot"].mean())/C["Tot"].std()
 C["FoodShr"]= C["Food"].div(C["Tot"]) #$\approx$ FoodShare variable
 C["logTot"] = C["Tot"].apply(np.log)
 C = C.join(T, how="left",lsuffix="_")

 if USE_LAMBDAS: Outcomes = ["Tot","logTot","FoodShr", "Food", "$\log\lambda_{it}$"]
 else: Outcomes = ["Tot","logTot","FoodShr", "Food"]

 #$\approx$ Make Baseline variable
 for var in Outcomes: 
     Bl = C.loc[2013,var].reset_index("Location",drop=True)
     #if var in mC: mC = mC.join(Bl,rsuffix="2013", how="left")
     C = C.join(Bl,rsuffix="2013", how="left")


 C["Y"]=np.nan
 for yr in (2013, 2014, 2015): C.loc[yr,"Y"]=str(int(yr))

 C = C.join(pd.get_dummies(C["Y"]), how="left",lsuffix="_")
 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         interaction = C[group]*C[year]
         if interaction.sum()>0: C["{}*{}".format(group,year)] = interaction

 if TGROUP_FE: Controls = ["2014","2015", 'TUP*2014', 'CSH*2014', 'TUP*2015', 'CSH*2015', 'TUP','CSH']
 else:         Controls = ["2014","2015", 'TUP*2014', 'CSH*2014', 'TUP*2015', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 C = C.loc[2014:2015]
 regs  = regressions(C, outcomes=Outcomes,  controls=Controls,  Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = C["TUP"]+C["CSH"] ==0
 CTLmean = {var: C[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: C[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+END_SRC
   
 #+name: mobile_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/SS-mobile.py
 import sys
 DATADIR = "../../data/"
 sys.path.append("../../data")
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 from TUP import full_data, consumption_data, regressions, reg_table, df_to_orgtbl, mobile_data
 ITEMS = ["beans", "sugar", "fish", "nuts", "vegetables", "airtime", "fuel"]

 D = full_data(DIR=DATADIR)
 HH, T = consumption_data(D,WRITE=True)[1:] #"csv")
 M, C,mHH= mobile_data(DIR = DATADIR+"Mobile/")
 try: logL = pd.read_pickle(DATADIR+"ss-lambdas_mobile.df")
 except EnvironmentError: raise IOError("Need to run SS-lambdas.py")
 logL.index.names=["HH","Year","Location"]
 logL.name       =["loglambda"]
 C    = C.join(logL,how="left").rename(columns={"loglambda":"$\log\lambda_{it}$"})
 C    = C.reorder_levels([1,0,2]).sortlevel()
 keep = pd.notnull(C.index.get_level_values("Location"))
 C    = C.loc[keep,:]
 # Make aggregate variables
 C["Tot"]    = C.filter(ITEMS).sum(axis=1).replace(0,np.nan)
 C["logTot"] = C["Tot"].apply(np.log)
 C           = C.join(T, how="left",lsuffix="_")
 C['const']  = 1.

 Outcomes =["Tot",  "logTot", "$\log\lambda_{it}$"]
 Controls= ['const', 'TUP', 'CSH']

 regs = regressions(C,outcomes=Outcomes, controls=Controls, Baseline=2013)
 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])
 CTL = C["TUP"]+C["CSH"] ==0
 CTLmean = {var: C.loc[CTL,var].mean() for var in Outcomes}
 CTLsd = {var: C.loc[CTL,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest= regs[var].t_test("TUP - CSH = 0").summary_frame()
     diff.loc[   r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest["coef"][0]
     diff_se.loc[r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest["std err"][0]

 results = results.append(diff)
 SE = SE.append(diff_se)
 mtab = df_to_orgtbl(results, sedf=SE)

 #+end_src
   
** Assets
   
#+name: asset_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/asset_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
from matplotlib import pyplot as plt
import sys
sys.path.append("../../data")
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

TGROUP_FE = True

D = full_data(DIR="../../data/")

Outcomes = ["Total", "Productive","Livestock"]
Aval2013 = asset_vars(D,year=2013)[0][Outcomes]
Aval2014 = asset_vars(D,year=2014)[0][Outcomes]
Aval2015 = asset_vars(D,year=2015)[0][Outcomes]

#$\approx$ Creates Year dummies and baseline values as `var'2013
for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
    Aval["Year"]=Year
    for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
Vals = Vals.join(D[["TUP","CSH"]])
Vals["CTL"] = (Vals["TUP"]+Vals["CSH"] ==0).apply(int)

for group in ("TUP", "CSH"):
    for year in ("2013", "2014", "2015"):
        Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

#~ Make graph of distribution
stringify = lambda var: Vals[var].apply(lambda x: var if x else "")
Vals["Group"] = stringify("TUP")+stringify("CSH")+stringify("CTL")


amean = Vals.groupby(["Year","Group"]).mean()[["Total","Productive","Livestock"]]
aN = Vals.groupby(["Year","Group"]).count()[["Total","Productive","Livestock"]]
astd = Vals.groupby(["Year","Group"]).std()[["Total","Productive","Livestock"]]
ase = astd/np.sqrt(aN)
asset_pctchange = (amean/amean.ix[2013]).unstack("Year") - 1

for var in ("Total","Productive","Livestock"):
   #~ fig,ax = plt.subplots(1,2)
   #~ for i,yr in enumerate((2014,2015)):
   #~     Vals.ix[yr].dropna(subset=[[var,"TUP","CSH","CTL"]]).groupby("Group")[var].plot(kind="kde",ax=ax[i])
   #~     ax[i].set_title("{} Asset Value in {}".format(var,yr))
   #~     ax[i].legend()
   #~     #~ ax[i].set_aspect(1)
   #~     ax[i].set_xlim(left=0)
   #~ plt.savefig("../figures/Asset{}_kde.png".format(var))
   #~ plt.clf()
   amean.unstack("Group")["Total"].plot(kind="bar",yerr=ase.unstack("Group")["Total"].values)
   plt.tight_layout()
   plt.xticks(rotation=45)
   plt.savefig("../figures/Asset{}_groupyear.png".format(var))
   plt.clf()

if TGROUP_FE: Controls = ["2014","2015", 'TUP*2014', 'CSH*2014', 'TUP*2015', 'CSH*2015', 'TUP','CSH']
else:         Controls = ["2014","2015", 'TUP*2014', 'CSH*2014', 'TUP*2015', 'CSH*2015']


#$\approx$ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
Vals=Vals.loc[2014:2015]
regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

CTL = Vals["TUP"]+Vals["CSH"] ==0
CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

for var in Outcomes:
    ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


results = results.append(diff)
SE = SE.append(diff_se)

tab = df_to_orgtbl(results, sedf=SE)
#+end_src

#+name: assets_disaggreate_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
import numpy as np
import pandas as pd
import sys
sys.path.append("../../data")
from TUP import full_data, regressions, asset_vars, reg_table, df_to_orgtbl

D = full_data(balance=[])
D["cons"] = 1.
Count = D.filter(regex="^asset_n_").rename(columns=lambda col: col[8:])
Vals = D.filter(regex="^asset_val_").rename(columns=lambda col: col[10:])
#~ 1 if coded as >0, 0 if zero, keep NaN's missing, since those are only from missing surveys.
Nonzero = Count.applymap(lambda i: float(i>0) if not np.isnan(i) else np.nan)
#~ Restrict both df's to nonzero responses.
Count = Count.replace(0,np.nan)
Vals  =  Vals.replace(0,np.nan)
too_many_zeros = 30
many_zeros = [item for item in Vals if Vals[item].notnull().sum()<too_many_zeros]
for df in (Nonzero, Count, Vals): df.drop(many_zeros,1, inplace=True)

#~ Merge in Control Vars
controls = ["cons","TUP","UCT"]
Nonzero  = Nonzero.join(D[controls],how="left")
Count    =   Count.join(D[controls],how="left")
Vals     =    Vals.join(D[controls],how="left")

Items = [item[:-2] for item in Vals if item.endswith("_e")]
CTL = Vals[controls].sum(axis=1)==1 #~ i.e. only constant ==1, TUP & UCT ==0
Zeros_ctl_mean = Nonzero.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Count_ctl_mean =   Count.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Vals_ctl_mean  =    Vals.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])

ZeroTable       = reg_table(regressions(Nonzero,"_e", outcomes=Items, controls = ["cons",'UCT','TUP'], Baseline="_b"), orgtbl=True, Transpose=True)
Count, Count_se = reg_table(regressions(Count,"_e",   outcomes=Items, controls = ["cons",'UCT','TUP'], Baseline="_b"), resultdf=True, Transpose=True)
Vals, Vals_se   = reg_table(regressions(Vals,"_e",    outcomes=Items, controls = ["cons",'UCT','TUP'], Baseline="_b"), resultdf=True, Transpose=True)

#~ Make full table of Standard errors-- MAKE SURE YOU HAVE THE SUFFIXES RIGHT.
SE = Count_se[["TUP","UCT"]].join(Vals_se[["TUP","UCT"]], rsuffix=" (SSP)", lsuffix=" (# own)")

#~ Make full table of point estimates and control-group means
FullTable = pd.DataFrame({"# own (CTL)":Count_ctl_mean, "Value (CTL)":Vals_ctl_mean}).join(Count[["TUP","UCT"]])
FullTable = FullTable.join(Vals[["TUP","UCT","N"]], rsuffix=" (SSP)", lsuffix=" (# own)")
FullTable = df_to_orgtbl(FullTable, sedf=SE)
AllTables = FullTable+"\n\n"+ZeroTable
return AllTables
#+end_src

** Savings

#+name: savings_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/savings_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
import sys
sys.path.append("../../data")
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl

def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

D = full_data(balance=[])

Sav = pd.DataFrame(index=D.index) #~ Set up empty DataFrame to fill

years = [("_b",2013), ("_m",2014), ("_e", 2015)]
for suff,year in years: #~ Make Aggregate savings and land holding variables
    Sav["Savings{}".format(year)]  = D.filter(regex="^savings_(home|bank|BRAC|NGOs|other){}".format(suff)).sum(1)
    Sav["Food Sav{}".format(year)] = D.filter(regex="^savings_(maize|sorghum|otherfood)_val{}".format(suff)).sum(1)

    Sav["LandCult{}".format(year)] = D.filter(regex="^land_(owncult|rentcult|communitycult){}".format(suff)).sum(1)
    Sav["LandOwn{}".format(year)]  = D.filter(regex="^land_own.*{}".format(suff)).sum(1)

    Sav["Get Trans{}".format(year)]  = D.filter(regex="^transfers_get.*{}".format(suff)).sum(1)
    Sav["Give Trans{}".format(year)] = D.filter(regex="^transfers_give.*{}".format(suff)).sum(1)

Outcomes = ["Savings","Food Sav","LandCult","LandOwn", "Get Trans", "Give Trans"] #~ Loans give/received omitted

by_year = []
for yr in ("2013","2014","2015"): #~ Provide Baseline Values & put in long format
    S_Year = Sav.filter(like=yr).rename(columns=lambda x:x[:-4])
    for var in Outcomes: 
        if var+"2013" in Sav: S_Year[var+"2013"] = Sav[var+"2013"]
    #~ Note that adding "Year" has to come after topcode, which switches strings to Nan...
    S_Year["Year"] = yr
    by_year.append(S_Year)

#~ In long format with Year,HH index
Sav = pd.concat(by_year).reset_index().set_index(["Year", "HH"], drop=False)
#~ Make Year Dummies for fixed effects


Sav = Sav.join(pd.get_dummies(Sav["Year"]).rename(columns=lambda col: str(int(col))), how="left")
Sav = Sav.drop(["HH","Year"],1)
Sav = Sav.join(D[["TUP","CSH"]])

for group in ("TUP", "CSH"): #~ Make Treatment-by-year interactions
    for year in ("2013", "2014", "2015"):
        Sav["{}*{}".format(group,year)] = Sav[group]*Sav[year]

def isPositive(i):
    #~ Returns 1 if number is positive, 0 if number<=0, nan if already nan, and self if string.
    #~ Note that it's safe to run dummy variables through..
    try:
        if np.isnan(i): return i
        else: return float(i>0)
    except TypeError: return i

too_many_null = 30
Nonzero = Sav.applymap(isPositive)

#~ Save DataFrame with zeros
Savings = Sav.copy()
#~ Naturally, only do this after Creating Nonzero dataframe.
for var in Outcomes: #~ Set zeros to missing And topcode among non-zero values
    for outcome in (var,var+"2013"):
       if outcome in Sav:
           Sav[outcome] = Sav[outcome].replace(0,np.nan)
           Sav[outcome] = Sav.groupby(level="Year")[outcome].apply(topcode) #~ (Untested)

many_null = [item for item in Sav if Sav[item].count()<too_many_null]
many_null2 =[item for item in Savings if Savings[item].count()<too_many_null]
Sav = Sav.drop(many_null,1).copy()
Savings = Savings.drop(many_null,1).copy()

Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

Sav = Sav.loc["2014":"2015"]
Savings = Savings.loc["2014":"2015"]
Nonzero = Nonzero.loc["2014":"2015"]
Sav_regs = regressions(Sav,     outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)
Zer_regs = regressions(Nonzero, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)
Amt_regs = regressions(Savings, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

Sav_results, Sav_SE  = reg_table(Sav_regs,  resultdf=True,table_info=["N","F-stat"])
Zer_results, Zer_SE  = reg_table(Zer_regs,  resultdf=True,table_info=["N","F-stat"])
Amt_results, Amt_SE  = reg_table(Amt_regs,  resultdf=True,table_info=["N","F-stat"])

CTL  = Sav["TUP"]+Sav["CSH"] ==0
CTL2 = Savings["TUP"]+Savings["CSH"] ==0
#~ Get control group means and standard deviations
Sav_CTLmean = {var: Sav[CTL].loc["2015",var].mean() for var in Outcomes}
Zer_CTLmean = {var: Nonzero[CTL].loc["2015",var].mean() for var in Outcomes}
Amt_CTLmean = {var: Savings[CTL2].loc["2015",var].mean() for var in Outcomes}

Sav_CTLsd = {var: Sav[CTL].loc["2015",var].std() for var in Outcomes}
Zer_CTLsd = {var: Nonzero[CTL].loc["2015",var].std() for var in Outcomes}
Amt_CTLsd = {var: Savings[CTL2].loc["2015",var].std() for var in Outcomes}

Sav_diff, Sav_diff_se = pd.DataFrame(Sav_CTLmean,index=["CTL mean"]), pd.DataFrame(Sav_CTLsd,index=["CTL mean"])
Zer_diff, Zer_diff_se = pd.DataFrame(Zer_CTLmean,index=["CTL mean"]), pd.DataFrame(Zer_CTLsd,index=["CTL mean"])
Amt_diff, Amt_diff_se = pd.DataFrame(Amt_CTLmean,index=["CTL mean"]), pd.DataFrame(Amt_CTLsd,index=["CTL mean"])

for var in Outcomes:
    #~ Savings regressions first
    ttest1= Sav_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Sav_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Sav_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Sav_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Sav_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Sav_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

    #~ Nonzero regressions second
    ttest1= Zer_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Zer_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Zer_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Zer_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Zer_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Zer_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

    #~ Savings regressions third
    ttest1= Amt_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Amt_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Amt_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Amt_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Amt_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Amt_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

Save_results = Sav_results.append(Sav_diff)
Zero_results = Zer_results.append(Zer_diff)
Amnt_results = Amt_results.append(Amt_diff)
Save_SE = Sav_SE.append(Sav_diff_se)
Zero_SE = Zer_SE.append(Zer_diff_se)
Amnt_SE = Amt_SE.append(Amt_diff_se)

#~ Land = ["LandCult","LandOwn"] 
#~ Savings = ["Savings","Food Sav", "Get Trans", "Give Trans"] 
#~ Land_results = Sav_results[Land]
#~ zLan_results = Zer_results[Land]
#~ Land_SE = Sav_SE[Land]
#~ zLan_SE = Zer_SE[Land]
#~ 
#~ Sav_results = Sav_results[Savings]
#~ Zer_results = Zer_results[Savings]
#~ Sav_SE =           Sav_SE[Savings]
#~ Zer_SE =           Zer_SE[Savings]

Save_tab = df_to_orgtbl(Save_results, sedf=Sav_SE)
Zero_tab = df_to_orgtbl(Zero_results, sedf=Zer_SE)
Amnt_tab = df_to_orgtbl(Amnt_results, sedf=Amt_SE)

Table = Zero_tab +"\n"+ Save_tab + "\n"+ Amnt_tab

#+end_src

** Income

#+name: income_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
from pandas.io import stata
import statsmodels.api as sm
from matplotlib import pyplot as plt
import sys
sys.path.append("../../data")
from TUP import full_data, regressions, reg_table, df_to_orgtbl
"""
Note that topcoding has a large effect on the distribution here, and we see only a small (presumably non-random) portion of actual income for each household.
"""

# Top-Code or censor outliers?
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

D = full_data(balance=[])
keep = D.index

I_file = '../../data/Endline/sections_8_17.csv'
I = pd.read_csv(I_file).rename(columns={"id":"HH"}).set_index("HH", drop=True).ix[keep]

#~Getting non-agriculture income data is easy
I = I.filter(regex="^s16")
Imonths    = I.filter(regex="s16_\dc").rename(columns=lambda x: x[:-1])
Ipermonth  = I.filter(regex="s16_\dd").rename(columns=lambda x: x[:-1])
Income_12m = Imonths.mul(Ipermonth).sum(axis=1)
Iyear      = I.filter(regex="s16_\de").rename(columns=lambda x: x[:-1]).sum(axis=1)

A_file = "../../data/Endline/Agriculture_cleaned.csv"
A = pd.read_csv(A_file).rename(columns={"id":"HH"}).set_index("HH",drop=False).ix[keep]
unit_prices = A.groupby(["harvest_type", "harvest_price_unit"])["harvest_price"].median()
prices = unit_prices.loc[zip(A["harvest_type"],A["harvest_price_unit"])]
A["price"]=list(prices)

A["harvest_unit_match"] = A["harvest_price_unit"] == A["harvest_unit"]
A["price"] = A["harvest_unit_match"]*A["harvest_price"] + (1-A["harvest_unit_match"])*A["price"]

A["income_farm_year"] = A["harvest_size"]*A["price"]
Ayear = A.groupby("HH")["income_farm_year"].sum()

unit_prices = A.groupby(["livestock_type", "livestock_price_unit"])["livestock_price"].median()
prices = unit_prices.loc[zip(A["livestock_type"],A["livestock_price_unit"])]
A["price"]=list(prices)
A["livestock_unit_match"] = A["livestock_price_unit"] == A["livestock_unit"]
A["price"] = A["livestock_unit_match"]*A["livestock_price"] + (1-A["livestock_unit_match"])*A["price"]

A["income_livestock_year"] = A["livestock_size"]*A["price"]
Lyear = A.groupby("HH")["income_livestock_year"].sum()

Outcomes = ["Total", "Non-Farm", "Farm",  "Livestock"]
Controls = ["cons", "TUP","CSH"]
Vals = pd.DataFrame({"Non-Farm": Income_12m, "Farm":Ayear, "Livestock":Lyear})
Vals = Vals.apply(topcode)

Vals["Total"] = Vals.sum(axis=1)
Vals["cons"] = 1.

Vals = Vals.join(D[["TUP","CSH"]],how="left")
Vals["CTL"] = (Vals["TUP"]+Vals["CSH"] ==0).apply(int)

#~ Make graph of distribution
stringify = lambda var: Vals[var].apply(lambda x: var if x else "")
Vals["Group"] = stringify("TUP")+stringify("CSH")+stringify("CTL")
Vals.dropna(subset=[["Total","TUP","CSH","CTL"]]).groupby("Group")["Total"].plot(kind="kde")
plt.title("Total Income Distribution by Group")
plt.savefig("../figures/IncomeDistribution.png")
plt.clf()
#~ Make bar graphs
Imean  = Vals.groupby("Group").mean()[Outcomes]
Icount = Vals.groupby("Group").count()[Outcomes]
Istd = Vals.groupby("Group").std()[Outcomes]
Ise=Istd/np.sqrt(Icount)
Imean.T.plot(kind="bar",yerr=Ise.T)
plt.tight_layout()
plt.xticks(rotation=45)
plt.savefig('../figures/Income_group.png')
plt.clf()

regs = {var: sm.OLS(Vals[var], Vals[Controls], missing="drop").fit() for var in Outcomes}
results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

CTL = Vals["CTL"] 
CTLmean = {var: Vals.query("CTL==1")[var].mean() for var in Outcomes}
CTLsd = {var: Vals.query("CTL==1").std() for var in Outcomes}
diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

for var in Outcomes:
    ttest1= regs[var].t_test("TUP - CSH = 0").summary_frame()

    diff.loc[   r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest1["coef"][0]
    diff_se.loc[r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest1["std err"][0]

results = results.append(diff)
SE = SE.append(diff_se)

tab = df_to_orgtbl(results, sedf=SE)

#+end_src

#+RESULTS: income_analysis
: None

** Conflict Exposure
#+name: conflict_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
import sys
sys.path.append("../../data")
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl

# Top-Code or censor outliers?
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

#~ Read in data
D = full_data("../../data/TUP_full.dta", balance=[])
D = D[D.merge_midline != 1]
C = D.filter(like="conflict").rename(columns = lambda x: x[:-2]) #~ Set up empty DataFrame to fill

#~ Make Outcome variables
#~ NOTE: Not looking at whether they protected assets as only 50 #~ said they did, and all said "Migrated to stay with other family"
C["Bsln_NaN"] = D["merge_midline"] == 2
C["Worried"]  = C["conflict_worried"]
C["Affected"] = C["conflict_affected"]
protect_lives_codes = lambda x: {"Nothing": 0, "Migrate to stay with friend/family": 1, "Migrated and found new accommodation": 1,
                                 "Looked for protection with Govt. Military": 2, "Looked for protection with NGO": 2}.get(x)
C["ProtectLives"] = C["conflict_protectlives"].apply(protect_lives_codes)
C["Migrated"] = (C["ProtectLives"]==1) + \
                (C["conflict_affected1"]=="Needed to elocate or migrate")
C["NoMeans"] = C["conflict_whynotprotect"]=="Didn't have the means"
C["NoInvest"]= C.filter(like="affected").applymap(lambda x: x=="Could not plant crop or invest in business").sum(axis=1)
C = C.drop([var for var in C if var.startswith("conflict_")], 1)
Outcomes = ["ProtectLives", "Worried", "Affected", "Migrated", "NoMeans", "NoInvest"]

#~ Bring in Treatment variables
C["TUP"] = D["TUP"]
C["cons"] = 1.
C = C.applymap(lambda x: int(x) if not np.isnan(x) else x)
Controls = ["cons", "TUP"] #~, "Bsln_NaN"]

#~ Plot
byT=C.groupby("TUP")
Cbar=byT.mean().drop(["Bsln_NaN",'cons'],1)
Cstd=byT.std().drop(["Bsln_NaN",'cons'],1)
Cn=byT.count().drop(["Bsln_NaN",'cons'],1)
for df in (Cbar,Cstd,Cn): 
        df.index=["CTL","TUP"]
        df.index.name="Group"
Cse=Cstd/np.sqrt(Cn)
Cbar.T.plot(kind="bar",yerr=Cse.T)
plt.tight_layout()
plt.xticks(rotation=45)
plt.savefig("../figures/conflict_exposure.png")
plt.clf()


C_regs = regressions(C, outcomes=Outcomes, controls=Controls, Baseline=False, baseline_na=False)
C_results, C_SE  = reg_table(C_regs,  resultdf=True,table_info=["N","F-stat"])

#~ Get control group means and standard deviations and add to regression table
CTLmean = {var: C.query("TUP==0")[var].mean() for var in Outcomes}
CTLsdv  = {var: C.query("TUP==0")[var].std()  for var in Outcomes}
CTLmean, CTLsdv = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsdv,index=["CTL mean"])
C_results = C_results.append(CTLmean)
C_SE      = C_SE.append(CTLsdv)
C_results.drop('cons',inplace=True)
C_SE.drop('cons',inplace=True)

Table = df_to_orgtbl(C_results, sedf=C_SE)
return Table

#+end_src

** Confidence & Autonomy

#+name: decision_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
 D = full_data(DIR = DATADIR,balance=[])

 Decide14 = D.filter(regex="^decide_.*_m").rename(columns=lambda x: x[:-2].split("_")[-1])#~.applymap(recode)
 Decide15 = D.filter(regex="^decide_.*_e").rename(columns=lambda x: x[:-2].split("_")[-1])#~.applymap(recode)

{'decide_arguments',
 'decide_fearful',

 'decide_pressuretospend'}

 who_decides = ['dailypurchase', 'familyvisits', 'healthchild', 'healthown', 'majorpurchase', 'moneyown']
 who_recode  = lambda x: float("Herself" in x) if pd.notnull(x) else x

 if WEEKLY:
     weekly = lambda i: float(i<3) if pd.notnull(i) else np.nan
     Aval2013 = Aval2013.applymap(weekly)
     Aval2014 = Aval2014.applymap(weekly)      
     Aval2015 = Aval2015.applymap(weekly)
 
 index_vars = "worried,portions,fewmeals,nofood,hungry,wholeday".split(",")
 Outcomes = index_vars+["z-score"]
 #~ Creates Year dummies, z-scores and baseline values as `var'2013
 for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
     Aval["Year"]=Year
     if not weekly:
        for var in index_vars:
            Aval[index_vars] = (Aval[index_vars]-Aval[index_vars].mean())/Aval[index_vars].std()
     FS_sum = Aval[index_vars].sum(axis=1)
     Aval["z-score"] = (FS_sum-FS_sum.mean())/FS_sum.std()
     for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
    
 Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
 Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
 Vals = Vals.join(D[["TUP","CSH"]])

 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

 Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 Vals=Vals.loc[2014:2015]
 regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = Vals["TUP"]+Vals["CSH"] ==0
 CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+end_src

#+name: confidence_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
 D = full_data(DIR = DATADIR,balance=[])

 autonomy_codes = {'go_NGO',
                   'go_church',
                   'go_healthcenter',
                   'go_housenonrelative',
                   'go_houserelative',
                   'go_market',
                   'go_school',
                   'go_water'}

 codes = {"1-2 times a week": 3,
         "3-6 times a week": 2,
         "Everyday": 1,
         "everyday": 1,
         "Less than once a week": 4,
         "less than once a week": 4,
         "Never": 5,
         "never": 5}

 recode = lambda x: codes.setdefault(x,x)

 Autnmy14 = D.filter(regex="^cango_.*_m").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Autnmy15 = D.filter(regex="^cango_.*_e").rename(columns=lambda x: x[3:-2]).applymap(recode)

{'decide_arguments',
 'decide_fearful',

 'decide_dailypurchase',
 'decide_familyvisits',
 'decide_healthchild',
 'decide_healthown',
 'decide_majorpurchase',
 'decide_moneyown',
 'decide_pressuretospend'}

 Decide14 = D.filter(regex="^decide_.*_m").rename(columns=lambda x: x[:-2])#~.applymap(recode)
 Decide15 = D.filter(regex="^decide_.*_e").rename(columns=lambda x: x[:-2])#~.applymap(recode)

 Confid14 = D.filter(regex="^conf_.*_m").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Confid15 = D.filter(regex="^conf_.*_e").rename(columns=lambda x: x[3:-2]).applymap(recode)

 if WEEKLY:
     weekly = lambda i: float(i<3) if pd.notnull(i) else np.nan
     Aval2013 = Aval2013.applymap(weekly)
     Aval2014 = Aval2014.applymap(weekly)      
     Aval2015 = Aval2015.applymap(weekly)
 
 index_vars = "worried,portions,fewmeals,nofood,hungry,wholeday".split(",")
 Outcomes = index_vars+["z-score"]
 #~ Creates Year dummies, z-scores and baseline values as `var'2013
 for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
     Aval["Year"]=Year
     if not weekly:
        for var in index_vars:
            Aval[index_vars] = (Aval[index_vars]-Aval[index_vars].mean())/Aval[index_vars].std()
     FS_sum = Aval[index_vars].sum(axis=1)
     Aval["z-score"] = (FS_sum-FS_sum.mean())/FS_sum.std()
     for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
    
 Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
 Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
 Vals = Vals.join(D[["TUP","CSH"]])

 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

 Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 Vals=Vals.loc[2014:2015]
 regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = Vals["TUP"]+Vals["CSH"] ==0
 CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+end_src

** Extras
#+name: get_loglambdas
#+BEGIN_SRC python :noweb no-export :results silent

"""
This code does not yet work in this file. Come back to it...
"""
  df = TUP.process_data(C, HH, T, year = year) #~ Process_data() builds consumption data if not given as an argument
  df['Constant']=1
  df["CTL"] = 1-df["TUP"] #~ Code the cash group as controls since they're not in the midline analysis

  explist=[s[2:-2] for s in df.columns[[s.startswith('c_') and s.endswith(year[1]) for s in df.columns]]]
  df = df.rename(columns= lambda x: x[:-2] if x.endswith(year[1]) else x)

  bothdf=[]
  xvars=['hh_size_b','child_total_b','Loc']
  for x in explist:
      if 'c_'+x+r'_b' not in df:
          #~ When you take out the baseline controls in favor of repeated cross-sections, this is where to start...
          print(x+" has no baseline data or had too few non-zero responses at baseline. Skipping.")
          continue
      ydf=pd.DataFrame(df[['c_'+x]].rename(columns={'c_'+x:x.capitalize()}).stack())
      rdict=dict(zip(xvars+['c_'+x+r'_b'],["%s_%s" % (s,x.capitalize()) for s in xvars]+['Baseline_%s' % x.capitalize()]))
      xdf=pd.DataFrame(df[xvars+['c_'+x+r'_b']])
      xdf.index=pd.MultiIndex.from_tuples([(i,x.capitalize()) for i in xdf.index])
      locations=pd.get_dummies(xdf['Loc'],prefix='Loc_%s' % x.capitalize())
      del xdf['Loc']
      xdf.rename(columns=rdict,inplace=True)
      xdf=xdf.join(locations)
      xdf.replace(to_replace=np.NaN,value=0,inplace=True)

      # Add row to restrict location dummies to sum to one
      ydf=pd.concat([ydf,pd.DataFrame([0],index=[(0,x.capitalize())])])
      xdf=pd.concat([xdf,pd.DataFrame([s.startswith('Loc_')+0. for s in xdf.columns],index=xdf.columns,columns=[(0,x.capitalize())]).T]) 

      xdf[0]=ydf
      xdf.dropna(how='any',inplace=True)
      bothdf.append(xdf)

  #~ Are this fillna() call and the xdf.replace call above a problem? It seems necessary for the block-diagonal ols function
  #~ we're using, but aren't we coding zeros as missing and calculating residuals for only those positive consumption? Wouldn't
  #~ replacing them to zero insert some non-zero residual for households that never consume a given good?
  #~ And isn't this the motivation behind svd_missing?
  mydf=pd.concat(bothdf).fillna(value=0)

  X=mydf.iloc[:,1:]

  y=mydf[[0]]

  x=np.exp(y.unstack().iloc[1:,:]) # Expenditures (in levels)
  xshares=x.divide(x.sum(axis=1),axis=0).fillna(value=0).mean() # Expenditure shares (taking missing as zero)
  xshares.index=xshares.index.droplevel(0)

  b,se=ols(X,y)

  ## betahat=b[['Constant_%s' % s.capitalize() for s in explist]]
  ## betahat.rename(columns=dict(zip(betahat.columns,[s.capitalize() for s in explist])),inplace=True)

  e=y-X.dot(b.T)

  e.rename(columns={0:'Resid'},inplace=True)
  e.index.names=['HH','Good']

  testdf=pd.merge(df[['TUP','CTL']].reset_index(),e.reset_index(),how='outer',on=['HH'])
  testdf.set_index(['HH','Good'],inplace=True)

  TUP=testdf['TUP'].mul(testdf['Resid']).dropna().unstack()
  CTL=testdf['CTL'].mul(testdf['Resid']).dropna().unstack()

  e=(e-e.mean()).unstack()

  # Test of significant differences between treatment and control:
  # Weighting matrix:
  A=np.matrix((TUP-CTL).cov().as_matrix()).I
  g=np.matrix((TUP-CTL).mean())
  J=e.shape[0]*g*A*g.T # Chi2 statistic

  p=1-chi2.cdf(J,e.shape[1])

  chi2test="Chi2 test: %f (%f)" % (J,p)

  N=pd.Series([d.shape[0]-1 for d in bothdf],index=[d.index.levels[1][0] for d in bothdf])

  resultdf=pd.DataFrame({'TUP':TUP.mean(),'CTL':CTL.mean(),'$N$':N})
  sedf=pd.DataFrame({'TUP':TUP.std()/np.sqrt(resultdf['$N$']),'CTL':CTL.std()/np.sqrt(resultdf['$N$'])})
  resultdf['Diff.']=resultdf['TUP']-resultdf['CTL']
  sedf['Diff.']=np.sqrt((sedf['TUP']**2) + (sedf['CTL']**2))

  # Use svd (with missing data) to construct beta & log lambda

  myb,myl = get_loglambdas(e,TEST=True)

  myb.index=myb.index.droplevel(0)

  # Normalize log lambdas
  l=myl/myl.std()
#+END_SRC

#+name: residuals_by_group
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
def residuals_by_group(models, groups, outcomes=[], kind="kde", figure_dir="../figures/", seriesname="Treat", blanks_to = "Control"):
    """
     Takes a set of statsmodels regression results and,
     for each outcome, produces a plot comparing the
     distribution of residuals by group.

     models:
         A dictionary of the form {variable name: sm.RegressionResults}. Empty defaults to all available.
     groups:
         A list, pd.Series, or pd.DataFrame with variables
         (A later version could contain an arbitrary set of categorical and give groups for every combination.)
     outcomes:
         A list specifying which variables in models to make plots for.
     kind:
         kde (or density) or histogram (or hist)
         Density plots are on a single axis. Histograms are stacked by group.
     figure_dir:
         The directory into which the figures get saved. If doesn't exist, throws error (future version might make that directory on the fly.)
     Seriesname:
         If a series or list is passed without a name, defaults to `seriesname'
     blanks_to:
         Observations with no treatment status from "groups" gets renamed to `blanks_to'
    """

    #~ Make outcomes a list. If empty, defaults to all variables in models
    if type(outcomes)==str: outcomes=[outcomes]
    if not outcomes: outcomes = sorted(models.keys())

    #~ Make data frame and make "Group" categorical
    df = pd.DataFrame(groups).rename(columns={0:seriesname})
    for var in df: df[var] = df[var].applymap(lambda x: var if x else "")
    df["Group"] = df.sum(axis=1).replace("",blanks_to)

    #~ Make residuals
    for var in outcomes:
        #~ Make column of residual values
        resid_var = "resid_{}".format(var)
        df[resid_var] = models[var].resid
        #~ Groupby object
        groups = df.dropna(subset=[resid_var]).groupby("Group")[resid_var]

        #~ Plot density by group
        if kind in ("kde", "density"):
            fig, ax = plt.subplots()
            groups.plot(kind=kind, ax=ax, legend=True)
            fig.savefig(figure_dir+resid_var+".png")

        #~ Plot histograms by group
        elif kind in ("hist", "histogram"):
            i=0
            fig, ax = plt.subplots(len(set(df["Group"])),1,sharex=True)
            for group, data in grps[var]:
                ax[i].hist(data.values, bins=20)
                ax[i].set_title(group)
                i+=1
            i=0
            fig.savefig(figure_dir+resid_var+".png")
        print(resid_var+".png created.")


#+end_src
   



 
