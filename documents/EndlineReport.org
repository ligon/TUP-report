:SETUP:
#+TITLE: Comparing Cash and Asset Transfers to Low-Income Households in South Sudan
#+AUTHOR: Reajul Chowdhury, Elliott Collins, Ethan Ligon, Munshi Sulaiman
#+DATE: \today
#+BIBLIOGRAPHY: main chicago
#+OPTIONS: texht:t toc:nil num:nil ':t inline:nil todo:nil
#+OPTIONS: title:nil  author:nil
#+LATEX_CLASS_OPTIONS: [12pt,letterpaper,titlepage]
#+LATEX_HEADER: \usepackage{amsaddr}
#+LATEX_HEADER:       \newcommand{\Eq}[1]{(\ref{eq:#1})}
#+LaTeX_HEADER:       \usepackage[T1]{fontenc}
#+LATEX_HEADER_EXTRA: \newcommand{\Fig}[1]{Figure \ref{fig:#1}} \newcommand{\Tab}[1]{Table \ref{tab:#1}}
#+LATEX_HEADER_EXTRA: \usepackage{stringstrings} \renewcommand{\cite}[1]{\caselower[q]{#1}\citet{\thestring}}
#+LATEX_HEADER_EXTRA: \usepackage{lscape}
#+PROPERTY: header-args:python :results output raw table :noweb no-export :exports none :prologue "# -*- coding: utf-8 -*-"
:END:

* Title Page                                                         :ignore:
#+BEGIN_export latex
\begin{titlepage}
\title[Cash vs. Asset Transfers]{Comparing Cash and Asset Transfers to Low-Income Households in South Sudan}

\date{\today}

\author{Reajul Chowdhury}
\address{University of Illinois, Champaign-Urbana}

\author{Elliott Collins}
\address{Innovations for Poverty Action}

\author{Ethan Ligon}
\address{University of California, Berkeley}

\author{Munshi Sulaiman}
\address{BRAC International}
\begin{abstract}
Several previous studies have found that the ``graduation'' or
``Transfers to the Ultra-Poor'' (TUP) framework is an effective
approach to alleviating the constraints that prevent extremely poor
households from increasing their productivity.  The framework consists
of a sizable transfer of productive physical capital, coupled with
training and continuous support over the course of one or two years.
A second and related literature has found some evidence that
unconditional cash transfers (UCT's) may also improve household
productivity and welfare with fewer fixed costs.  Our field experiment
provides a comparison of these two approaches to transferring wealth to
low-income households during the first two years of BRAC's TUP pilot
in South Sudan.  We consider the effect of each on consumption,
income, asset holdings, and a number of intangible outcomes. We also
consider the TUP program's effect on households' responses to the
outbreak of violence in 2014. We find evidence that both types of
transfer have positive effects on consumption, but only in the
short-run.  We find a persistent increase in asset stocks, but only from
the TUP. We also elicit suggestive evidence that BRAC's support may
have helped TUP beneficiaries cope with the short-term economic
effects of the outbreak of violence in 2014. We tentatively conclude
that in this economic context cash can increase household consumption,
but the goal of improving income or wealth is aided by the additional
services that the ultra-poor graduation framework offer.
\end{abstract}

\renewcommand{\keywordsname}{Keywords}
\keywords{South Sudan, Unconditional Cash Transfers, Graduation Program, Transfers to the Ultra-Poor}

\maketitle
\thispagestyle{empty}
\end{titlepage}
#+END_export

* Introduction
 
Poor rural households often earn money from low-return activities such
as small-scale cultivation or casual day labor, and face both
financial and human capital constraints, which may prevent them from
investing and expanding into more lucrative activities. Experience and
research over many years has led many to believe that households
facing particularly acute poverty are unable to solve this problem
through the small, high-interest loans typically marketed to them.  It
was these considerations that led to the development of the initial
``Targeting the Ultra-Poor'' (TUP) program in Bangladesh as a
supplement or precursor to credit services. First implemented by BRAC
in 2007, the program aims to simultaneously alleviate physical and
human capital constraints by providing households with a significant
transfer of food and productive assets, followed by two years of
training and support by extension officers citep:bandiera-etal17.  The
general framework[fn:: Also known as the ``graduation framework''
pointing to the original ambition to move households into an activity
where they are able to finance further income growth without costly
transfers.] has since expanded to a wide range of countries, with a
general pattern of success in increasing aggregate investment, labor
supply, and aggregate consumption citep:banerjee-etal15.

A second, older literature that has gained new interest in recent
years studies the effect of offering poor households direct
unconditional cash transfers (UCT's)
citep:haushofer-shapiro16,blattman-etal16.  While this and the TUP
framework are both direct capital transfer interventions, they are
very different in their approach, with TUP programs guiding and
constraining the use of capital towards productive investment while
UCT's allow households to invest, save, and consume as they see
fit. The natural question that arises is whether (and how) these
additional features and constraints in the TUP framework change how
households use their capital transfers.  This study provides the first
side-by-side experimental evaluation of the two approaches.

* Treatments

How do the effects of TUP differ from the effects of a UCT, when the
resources given to recipients have roughly the same value?  To provide
some insight into this question, we worked with BRAC's South Sudanese
branch office in Yei county to randomize selection into either a TUP
or UCT treatment.

We began in April 2013 by conducting a census of all households having
one or more adult women within the area around BRAC's office in the
town of Yei in Western Equatoria, interviewing women in 1279
households.

The census interview included questions to assess eligibility for the
program.  Eligibility depended on several criteria.  First, households
were excluded if they had a salaried worker in the household; if they
were participating in some other NGOs livelihoods program; or if they
had no access to cultivable land (which was in some cases necessary
for the program's model).  If not excluded on these grounds,
households were then deemed eligible if they satisfied three or more
of the following criteria: 

  1. The household head was working as a day laborer (generally an occupation with poverty wages);
  2. The household had two or more children;
  3. At least one child in the household was working;
  4. The household had fewer than three rooms; or
  5. A woman in the household had not completed secondary school.

Of the 1279 households interviewed in the census, 755 were deemed to
be eligible.  We then conducted a baseline survey of these eligible
households, successfully interviewing 649 of eligible households.  Of
the eligible households interviewed at baseline, 250 were randomly
assigned to participate in the TUP program, while another 125 were
randomly assigned to a UCT group which was given cash roughly equal to
the value of the physical assets provided to TUP households.  This
left 274 eligible households interviewed at baseline randomly assigned
to a control group, and another 96 eligible households who were /not/
interviewed at baseline. 

** The TUP Program
   
The TUP program itself is similar to the other TUP/Graduation
programs completed by BRAC elsewhere.  BRAC designed several possible
enterprises: goat husbandry, duck husbandry, maize cultivation,
vegetable cultivation, and the selling of dried fish.
The preferences of enrolled households over these were elicited; we
asked both for an ordering, and a willingness to pay for both the
training and productive assets associated with the five different
enterprises.

#+begin_src python 
import pandas as pd
from cfe.df_utils import df_to_orgtbl

df = pd.read_stata('../../TUP-data/Baseline/Tup programme sudan single file.dta',convert_categoricals=False)

df = df.set_index('idno').iloc[:,-15:]

# Different enumerators handled this question differently; some list
# the assets alphabetically then giving the numeric preference
# ordering of each asset, while others wrote the asset letters in
# order of preference.  Here we sort this out.

idx = df.rename(columns=lambda s:tuple(s.split('_'))).columns
df.columns = pd.MultiIndex.from_tuples(idx)

order  = df['rank'].stack()

ac = df['ac']
ac.columns = range(5)
myorder = order.astype(int)-1

myidx=myorder.droplevel(1).reset_index().set_index(['idno',0])

maybe = ac.stack()[myidx.index]
maybe.index = maybe.sort_index().index
maybe =  maybe[~maybe.index.duplicated()]
maybe.name = 'Choice'

ranks = maybe.unstack()

freq = pd.concat([ranks[x].value_counts() for x in ranks],axis=1)
freq.rename(index={'A':'Goats','B':'Ducks','C':'Maize','D':'Vegetables','E':'Dry Fish','':'Missing'},inplace=True)
freq.columns = freq.columns + 1

print(df_to_orgtbl(freq,float_fmt='%d'))
#+end_src


#+name: tab:enterprise_choice
#+caption: Frequency of Rankings of Enterprise Choices
|            |   1 |   2 |   3 |   4 |   5 |
|------------+-----+-----+-----+-----+-----|
| Dry Fish   | 219 | 105 | 113 |  74 |  73 |
| Goats      | 149 | 115 | 107 | 107 | 116 |
| Ducks      | 120 | 172 | 137 |  88 |  68 |
| Vegetables |  65 |  85 | 108 | 138 | 164 |
| Maize      |  55 | 127 | 127 | 145 | 122 |
| Missing    |   2 |   1 | --- |   3 |   3 |

Table [[tab:enterprise_choice]] reports on the frequency of elicited
preference ranks over the five different choices.  Dry fish was the
most popular single first choice, with animal husbandry (goats then
ducks) the second and third choices.  Crop cultivation (whether of
vegetables or maize) was less popular. 

*** Enterprise Assignment & Training

The second phase of the program was enterprise assignment and
training. Unlike some other programs of this type (e.g., the TUP
program in Bangladesh described by cite:bandiera-etal17), the number
of households given each kind of asset was set in advance, with 75
enrolled in agricultural activities (either maize or vegetable
cultivation), 85 in duck rearing, 45 in goat rearing, and the rest in
small trade involving dried fish.  Within these limits, assignment to
particular activities was made at the discretion of program staff,
taking into account subjects' preferences and skills.

Households then attended training sessions. The first of these were
for general business skills around literacy, numeracy, and financial
management. The next were sector specific and focused on animal
husbandry or crop cultivation.

After training was completed, asset transfers began in late 2013 and
continued through the first few months of 2014. The productive assets
related to each enterprise were valued at around $240 per household,
with a random subset receiving an additional $60 in assets later
in 2014.  Shortly thereafter, households started to attend weekly or
semi-weekly meetings with other nearby participants to discuss with
each other and a BRAC extension officer the details of their
businesses. These meetings also included food transfers for a while,
which were designed to help get households to the point of receiving
revenue from their assets without having to sell them.  In all, the
market value of these food transfers were valued at $110, bringing the
total value of all transfers to either $350 or $410.

** Unconditional Cash Transfers

To implement an unconditional cash transfer, we randomly assigned each
of the 125 households in the UCT group to receive a cash transfer of
either $350 or $410, matching the values of assets received by
households in the TUP treatments.  

Local community leaders were approached in advance of talking to
individual households to reassure recipients of BRAC's intentions. It
was made clear that the transfers were there for their benefit, that
while they were encouraged to put them towards some productive use,
their use was unrestricted, and that under no circunstances would they
receive a second transfer in the future. BRAC was a well-established
institution in the greater Yei community, so these assurances were
taken as credible and no one refused to accept the money.

Cash transfers were delivered by field officers in person and in the
local currency (South Sudanese Pounds, or SSPs).  That transfers were
delivered in cash distinguishes these from several other cash transfer
programs.  For example, cite:haushofer-shapiro16 deposited funds in a
bank or mobile money account, while cite:fafchamps-etal14 used money
transfers.  In all of these experiments transfers were denominated in
the local currency.  However, experimental studies of the behavioral
implications of savings accounts citep:dupas-robinson13 would lead one
to expect that the method of disbursal might affect the propensity to
save a cash transfer.

* Data Collection & Timeline

** Timeline                                                          :ignore:
We have four principal sources of data.  First, as discussed above, we
conducted a census of households with women in the area around BRAC's
offices in April of 2013.  Second, in the summer of 2013 we conducted
a baseline survey of eligible households.  We randomly assigned women
who responsed to this baseline survey to one of the TUP, UCT, or
control groups; some randomly selected TUP and UCT assignees were
further chosen to receive an asset/cash "top-up" valued at $60.
Third, we conducted a "midline" survey in June 2014, and an "endline"
survey in the summer of 2015.

Training and asset transfers for the TUP group began in late 2013; our
intention was to make transfers of cash to the UCT group at roughly
the same time that TUP participants were ready to begin their
operations, during the first quarter of 2014.  However, these plans
were complicated by an onset of broad civil unrest in South Sudan, and
disruption of BRAC's operations.

In response to the outbreak of violence in late 2013 and subsequent
closing of the offices in Yei, we conducted a midline survey in June
2014 to try to separate pre- and post-conflict changes in outcomes.
We cannot test any claims about the effect of the conflict on outcomes
for lack of a valid comparison group, though we do report estimates of
treatment effects on the severity or likelihood of having been
exposed to the conflict.  Some of the original asset
transfers were done before the office closure, which may affect
estimates of the difference between programs if rates of return
changed in the few intervening months.  Finally, we conducted an endline survey
in mid-2015 to estimate the effect of program participation
on households' financial situation and overall welfare. The key here
is that the survey conducted in mid-2014 provides us with /short-term/
treatment effects of the TUP program within 6--8 months of the asset
transfers, while providing a second baseline for the UCT treatment.
Likewise, the 2015 survey allows us to estimate treatment
effects one year after the cash transfers, and 15--18 months after the
asset transfers.

** Balance, Attrition, Selection

In Appendix [[Balance & Selection]] we examine in detail whether the
randomly assigned treatment groups are balanced on observables; the
attrition of households from the sample; and whether this attrition is
selective, varying systematically either with assignment or baseline
characteristics of the household.

The means of a long list of household characteristics are balanced
across groups, with minor exceptions attributable to chance (a larger
share of households in the UCT group owned motorcycles at baseline,
and the same group had made larger average purchases of textiles
during the past year; see Appendix Tables [[tab:balance_check]] and
[[tab:mean_log_balance]]).

Given the environment, attrition was quite low (see Appendix Table
[[tab:attrition_count]]).  We attempted to survey 755 eligible households
in total.  Of these, 649 were surveyed at baseline, 606 at midline,
and 700 at endline.  Thus, the probability of an eligible household
being interviewed in 2013 was 87%, falling to 81% in 2014, and rising
to 93% in 2015.  However, households in the control group were
significantly less likely to be interviewed at baseline (Appendix
Table [[tab:interview_probabilities]]).  In order to take advantage of the
households not included in the baseline, as well as to deal with any
issues with imperfect balance on observables, our specification below
follows cite:banerjee-etal15 by setting missing values to zero when
controlling for baseline levels of the dependent variable, and
including an indicator for whether the household was in the baseline,
in a sort of adjusted ANCOVA specification of the sort discussed by
cite:mckenzie12.

* Empirical Strategy

We estimate a single model using interactions between time effects and
group assignment, as well as baseline values of the outcome variable
where available; this is 
\begin{equation*}
Y_{it} =\alpha_{t}+\beta_{t,UCT}I_{t}*UCT_{i}+\beta_{t,TUP}I_{t}*TUP_{i}+\gamma Y_{i,2013}(1-M_{i,2013}) + \delta M_{i,2013} +\epsilon_{it},
\end{equation*}
for $t\in\{2014,2015\}$, with $i$ indexing households.  The
$\alpha_{t}$ are time fixed effects; $I_{t}$ is an indicator if the
year is $t$; and $Y_{it}$ is an outcome of interest for household $i$ in
year $t$.  If baseline values of the dependent variable are missing
for household $i$ then this is indicated by the dummy variable
$M_{i,2013}$.  Thus, baseline values of the dependent variable appear
as $Y_{i,2013}$ where these are non-missing; but the coefficient
$\delta$ estimates the average baseline value conditional on it being
missing.

We take the interactions of TUP assignment with 2014 and 2015
indicators as the treatment effects at 6--8 and 15--17 months
respectively. The analogous interactions with the UCT group offer a
second baseline and a 12-month treatment effect, respectively.  Since
those transfers happened after the midline survey, its interaction
with $I_{2014}$ acts as a placebo; there is no /ex ante/ reason to
expect that households in the UCT group were different from the rest
of the control group at that point.  Given the slight difference in
timing, we report a \(t\)-test of the hypothesis
\(\beta_{\mbox{TUP},t}=\beta_{\mbox{UCT},2015}\) for both of \(t \in
{2014,2015}\).

Since the comparison between (TUP,2014) and (UCT,2015) allows for
something less than a year of exposure to the respective treatments we
regard \(\beta_{\mbox{TUP},2014}=\beta_{\mbox{UCT},2015}\) as the
most interesting hypothesis, but since these points of comparison
aren't contemporaneous we can't rule out the possibility that
aggregate shocks might have interacted somehow with the two programs
differently.  Accordingly, we also report
\(\beta_{\mbox{TUP},2015}-\beta_{\mbox{UCT},2015}\), which has the
interpretation as being the difference between the average effect of
having been enrolled in either program from its inception until 2015.
 
* Results
** Assets
   
We first turn our attention to effects of the different programs on
asset holdings.  Since TUP households were directly /given/ a variety
of different assets, for this program this can be regarded as a rather
direct test---eight months later, do TUP recipients still own these
assets, or have they sold (or eaten) them? 

*** Total Asset Holdings
**** Discussion                                                      :ignore:
The cash group does not appear to have seen a significant increase in
the value of assets measured.  This contrasts sharply with the TUP
group.  The TUP group gains significantly more asset wealth than the
cash or control groups in both 2014 and 2015, 18 months after receipt
of transfers. They have nearly double the total asset value of control
group in 2014 (midline), and roughly three-quarters more at endline.
These patterns reproduce themselves for both productive[fn:: So-called
"Productive" assets include anything that could plausibly be used in
productive activity.  Included in this list: small and large
livestock, farm equipment, mobiles, carts, sewing equipment, sheds,
and shop premises.] assets (341 SSP more than the control group at
endline, and 450 SSP at midline). 

**** Recreating asset tables with and without treatment group fixed effects :ignore:
*************** NOTE
 In these tables, "Productive" includes "Livestock." CTL mean lists 
 the mean and standard deviation of the control group in 2015. Some
 assets were not included ('house', 'homestead', and 'netITN') either 
 because numbers turned out to be more or less meaningless,  or because 
 they overlap (e.g. nets & ITN nets) (as commented in
 TUP-data/TUP.py). All of the Ns are the same because NaN values were
 considered as having a value of 0 for that asset (this occurs
 automatically when using .sum(axis=1) to sum values in different
 columns for a given row). 

 Differences with previous code: 
 -This code does not do any censoring, truncating, or converting of
 values to US dollars or anything like that

 Next steps: 
 -recreate bar graph figure
 -divide the standard deviation of CTL mean by the sqrt of the number of control
 observations to make it comparable with the other point estimates and
 standard errors in the table?
*************** END


 #+name: asset_values
 #+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no :tangle asset_values.py
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
data_baseline = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_midline  = "../../TUP-data/Midline/TUP_midline.dta"
data_endline = "../../TUP-data/Endline/TUP_endline.dta"

import statsmodels.api as sm
import pandas as pd
import numpy as np
from cfe.df_utils import df_to_orgtbl
from scipy.linalg import block_diag

df_b = pd.read_stata(data_baseline)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)
df_a = pd.read_csv(data_assignment)

df_b.set_index('idno', inplace=True)
df_m.set_index('idno', inplace=True)
df_e.set_index('idno', inplace=True)
df_a.set_index('RespID', inplace=True)

# merge baseline and midline based on idno index
df = pd.merge(df_b.filter(regex="^asset_"), 
              df_m.filter(regex="^asset_"), 
              how='outer', left_index=True, right_index=True)

# merge baseline/midline and endline based on idno index
df = pd.merge(df, 
              df_e.filter(regex="^asset_"), 
              how='outer', left_index=True, right_index=True)

# Some assets to ignore, either because numbers turned out to be more or less meaningless, 
# or because they overlap (e.g. nets & ITN nets) (as commented in TUP-data/TUP.py)

df = df.drop(columns = df.filter(like='house').columns)
df = df.drop(columns = df.filter(like='homestead').columns)
df = df.drop(columns = df.filter(like='netITN').columns)

# find columns that end with "_b", "_m", or "_e"
# cut off the last 2 "_b", "_m", or "_e"
asset = {}
asset[2013] = df.filter(regex="_b$").rename(columns=lambda x: x[:-2])
asset[2014] = df.filter(regex="_m$").rename(columns=lambda x: x[:-2])
asset[2015] = df.filter(regex="_e$").rename(columns=lambda x: x[:-2])

asset_n = {}
asset_val = {}
price = {}

# Note: value columns have format asset_val_{good}, quantity columns have format asset_n_{good}
for year in (2013, 2014, 2015):
    asset_n[year] = asset[year].filter(like='_n_').rename(columns=lambda col: col[8:])
    asset_val[year] = asset[year].filter(like='_val_').rename(columns=lambda col: col[10:]) 
    price[year] = asset_val[year]/asset_n[year]

# add top code here if needed

productive_2013 = ['cows', 'smallanimals', 'poultry', 'plough', 'shed', 'shop', 'mobile', 'carts', 'sewing']
livestock_2013 = ['cows', 'smallanimals', 'poultry']

# not including 'ploughs again' variable
productive_2014_2015 = ['cows', 'smallanimals', 'chickens', 'ducks', 'plough', 'shed', 'shop', 'pangas', 'axes', 'mobile', 'carts', 'sewing']
livestock_2014_2015 = ['cows', 'smallanimals', 'chickens', 'ducks']

# for 2013
asset_val[2013]['Productive'] = asset_val[2013][productive_2013].sum(axis=1)
asset_val[2013]['Livestock'] = asset_val[2013][livestock_2013].sum(axis=1)
    
# add year 
asset_val[2013]['Year'] = '2013'

for year in (2014, 2015):
    # 2013 only has poultry, but 2014 and 2015 have chickens and ducks separately
    asset_val[year]['poultry'] = asset_val[year][ ['chickens', 'ducks'] ].sum(axis = 1)
    asset_n[year]['poultry'] = asset_n[year][ ['chickens', 'ducks'] ].sum(axis = 1)
    
    asset_val[year]['Productive'] = asset_val[year][productive_2014_2015].sum(axis=1)
    asset_val[year]['Livestock'] = asset_val[year][livestock_2014_2015].sum(axis=1)
    
    # add year
    asset_val[year]['Year'] = str(year)

outcomes = ["Total", "Productive", "Livestock"]

for year in (2013, 2014, 2015): 
    asset_val[year]['Total'] = asset_val[year].sum(axis=1)
    
    # set up to make year dummies
    asset_val[year]['2013'] = 0
    asset_val[year]['2014'] = 0
    asset_val[year]['2015'] = 0
    
    # baseline values as 'var'2013
    for var in outcomes: 
        asset_val[year][var+'2013'] = asset_val[2013][var]

# make year dummmies
asset_val[2013]['2013'].replace(to_replace = 0, value = 1, inplace=True)
asset_val[2014]['2014'].replace(to_replace = 0, value = 1, inplace=True)
asset_val[2015]['2015'].replace(to_replace = 0, value = 1, inplace=True)

# add in group dummies from master_assignment.csv
df2 = pd.concat((asset_val[2013], asset_val[2014], asset_val[2015]))
df = df2.join((df_a[ ['Group'] ].join(pd.get_dummies(df_a['Group']))))   

# make interaction terms
df.insert(len(df.columns), 'TUP*2013', df['2013']*df['TUP'])
df.insert(len(df.columns), 'TUP*2014', df['2014']*df['TUP'])
df.insert(len(df.columns), 'TUP*2015', df['2015']*df['TUP'])
df.insert(len(df.columns), 'UCT*2013', df['2013']*df['UCT'])
df.insert(len(df.columns), 'UCT*2014', df['2014']*df['UCT'])
df.insert(len(df.columns), 'UCT*2015', df['2015']*df['UCT'])

models_ols_FE = {}
models_ols_no_FE = {}
d_FE = {}
d_no_FE = {}
baseline_na = True

controls = ['TUP*2014', 'TUP*2015', 'UCT*2014', 'UCT*2015', '2014', '2015', 'TUP', 'UCT']
controls = ['TUP*2014', 'TUP*2015', 'UCT*2014', 'UCT*2015', '2014', '2015']
# from above, outcomes = ["Total", "Productive", "Livestock"]

# remove observations from 2013
df = df[df['Year'] != '2013']
df.index.name = 'idno'
df = df.reset_index().set_index(['idno','Year'])

myX = {}
myY = {}
for outcome in outcomes: 
    temp_df = df[ [outcome, outcome + "2013", 'Control'] + controls]
    temp_df.rename(columns={outcome+"2013":'Baseline value'},inplace=True)
    temp_controls = controls
    
    if baseline_na==True:
        
        # indicator for whether outcome in 2013 is na, and cast it to be an integer
        temp_df["Baseline missing"] = temp_df["Baseline value"].isnull().apply(int)
        
        # code missing values of the baseline variable as 0
        temp_df["Baseline missing"].fillna(0,inplace=True)
        
        temp_controls = temp_controls + ['Baseline missing']
    
    temp_controls = temp_controls + ["Baseline value"]
    temp_df = temp_df.dropna()

    myX[outcome] = temp_df[temp_controls]
    myY[outcome] = temp_df[outcome]

myY = pd.concat(myY)
myX = pd.DataFrame(block_diag(*myX.values()),
                   columns=pd.concat(myX,axis=1).columns,
                   index=myY.index)
    
est = sm.OLS(myY,myX).fit()
b = est.params
se = est.bse

b.index = pd.MultiIndex.from_tuples([tuple(i.split('_')) for i in est.params.index])
se.index = pd.MultiIndex.from_tuples([tuple(i.split('_')) for i in est.params.index])    


b = b.unstack()
b['TUP*2014 - UCT*2015'] = b['TUP*2014'] - b['UCT*2015']
b['TUP*2015 - UCT*2015'] = b['TUP*2015'] - b['UCT*2015']
b = b.T

se = se.unstack()
se['TUP*2014 - UCT*2015'] = np.sqrt(se['TUP*2014']**2 + se['UCT*2015']**2)
se['TUP*2015 - UCT*2015'] = np.sqrt(se['TUP*2015']**2 + se['UCT*2015']**2)

se = se.T

print(df_to_orgtbl(b,sedf=se))

 #+end_src


 #+name: tab:asset_values
 #+caption: Average treatment effects by group-year on total value (in SSP) of all assets measured and of productive assets measured.
 #+attr_latex: :environment longtable :align lrrr
 | Variables                         | Total       | Productive | Livestock  |
 |-----------------------------------+-------------+------------+------------|
 | CTL mean                          | 1634.104    | 301.364    | 189.439    |
 |                                   | (2406.541)  | (580.913)  | (523.819)  |
 |-----------------------------------+-------------+------------+------------|
 | TUP*2014                          | 1617.966*** | 450.392*** | 377.437*** |
 |                                   | (230.986)   | (61.004)   | (52.762)   |
 | TUP*2015                          | 1298.628*** | 341.678*** | 322.614*** |
 |                                   | (230.986)   | (61.004)   | (52.762)   |
 | UCT*2014                          | 292.498     | 90.656     | 25.270     |
 |                                   | (292.493)   | (77.242)   | (66.798)   |
 | UCT*2015                          | -37.562     | 20.476     | 17.767     |
 |                                   | (292.493)   | (77.242)   | (66.798)   |
 | 2014                              | 1291.392*** | 262.936*** | 134.5***   |
 |                                   | (149.060)   | (39.001)   | (33.588)   |
 | 2015                              | 1540.804*** | 293.132*** | 181.163*** |
 |                                   | (149.060)   | (39.001)   | (33.588)   |
 | Bsln Missing                      | 0.000       | 0.000      | 0.000      |
 |                                   | (0.000)     | (0.000)    | (0.000)    |
 | Bsln 2013                         | 0.042***    | 0.015      | 0.021*     |
 |                                   | (0.014)     | (0.012)    | (0.012)    |
 |-----------------------------------+-------------+------------+------------|
 | \(F\)-stat                        | 15.911      | 15.203     | 16.350     |
 | $N$                               | 1510        | 1510       | 1510       |
 |-----------------------------------+-------------+------------+------------|
 | $\beta_{TUP2014}-\beta_{UCT2015}$ | 1655.528*** | 429.916*** | 359.67***  |
 | $\beta_{TUP2015}-\beta_{UCT2015}$ | 1336.19***  | 321.202*** | 304.847*** |

 The final two rows of Table [[tab:asset_values]] considers the difference
 in the average treatment effect between the UCT and TUP treatments.
 Here one needs to be thoughtful about the period of comparison, since
 the UCT treatment was rolled out later than the TUP.  Thus, the first
 row compares the difference in average asset holdings between the TUP
 and UCT groups roughly one year after the program began; here we see
 significantly higher mean TUP asset holdings, in the amount of 1655
 SSP.  The second row instead compares contemporaneous endline asset
 holdings; here we again see significantly higher holdings for the TUP
 group.


*** Savings & Land
**** Discussion of savings accounts                                  :ignore:
The TUP households were strongly encouraged to pay into a savings
account maintained by BRAC at each of their weekly meetings.
Regardless of their motivation, TUP households held savings accounts
at significantly higher rates than both the control and UCT groups
(Table [[tab:nonzero_savings]]); roughly 28% more in 2014, and 18% more
in 2015.  The same is /not/ true of amounts of savings; conditional on
having positive savings, TUP groups held roughly 30% /less/ than the
control group in 2014 (Table [[tab:log_savings]]).  This pattern then
changes dramatically in 2015, when both the TUP and UCT groups
dramatically increase their cash savings relative to the control, with
both groups holding over 50% more than the control (again, conditional
on holding any savings at all).  It is, perhaps, not surprising that
the UCT group held large savings in 2015---after all, giving these
households cash was the point of the treatment, implemented not long
before.  Understanding the increase in cash savings for the TUP group
is more challenging; are their business investments yielding returns
which allow them to save?  There may be some of this, but interpreting
the increase for the TUP groups must also take into account that the
number of TUP households holding positive savings fell from 2014 to
2015 relative to other groups, for which the prevalence of positive
savings increased from one third in 2014 to 39% in 2015.

**** Discussion of food saved                                        :ignore:

It is common in this community (and most in the region) to store non-perishable food
like maize, cassava, or millet as a form of savings.  This makes
particular sense in a high-inflation context, where the price of grain had doubled in the
previous year.  Most households report saving in food (73%) as in cash
(40%).  The prevalence of food savings increased in significantly in 2014 for both TUP
and UCT groups relative to the control.[fn:: Note that food savings was not
measured at baseline, so these controls are omitted.]  However (Table
[[tab:log_savings]]) the /amount/ of food savings conditional on it being
positive was only significant for the TUP group, for whom it increased
by roughly 30% in 2014, one round after treatment; similarly, after
treatment the UCT group saved roughly 20% more in food than either the
control or TUP groups in 2015. 

**** Discussion of transfers                                         :ignore:

Households in the TUP group were roughly 7--8% more likely to both
give and receive transfers from other households in 2014.  The UCT
group showed a similar propensity to /receive/ transfers in 2014, but
did not give significantly more than the control group.  On the other
hand, one cannot reject the hypothesis that the propensity to both
give and receive was equal across both the TUP & UCT groups in 2014.
And for both groups the propensity to make transfers was no different
from that of the control group in 2015.  It's worth noting, however,
that transfers and indeed /all/ the forms of financial activity we
record in Table [[tab:nonzero_savings]] showed marked increases across all
groups in both 2014 and 2015, relative to 2013.

**** Discussion of land ownership & cultivation                      :ignore:

We also examine land ownership and cultivation in each year.  In 2014 both the
TUP and UCT groups are significantly  more likely cultivate land, and
also to own it.  These difference from the control group disappear
by 2015.  But interestingly, the amount of land owned or cultivated
/conditional/ on having some land is not significantly different from
the control for either group in 2014, and land cultivated is
significantly /less/ than the control both groups in 2015 (the same
goes for land ownership, but not at the same level of significance.

As with the savings accounts, one is reminded that eligibility for the
program depended on having access to cultivatable land---one wonders
if the the high initial prevalence followed by a drop in area is a
reallocation of resources for these households in response to the
program, or a reversion to the mean.


**** Recreating savings tables                                       :ignore:
**************** NOTE
 CTL mean lists the mean and standard deviation of the control group 
 in 2015. All of the Ns are the same because NaN values were
 considered as having a value of 0 for that asset (this occurs
 automatically when using .sum(axis=1) to sum values in different
 columns for a given row). The baseline survey did not ask for food
 savings, which is why Baseline2013 and BaselineNA controls are omitted.

 Differences with previous code: 
 -This code does not do any censoring, truncating, or converting of
 values to US dollars
 -s8b_2c_m and  s8b_2d_m for other food savings values are included in
 this code but not the previous
 -in previous code, there was an issue at endline  with how the sum was
 being calculated for 'savings_otherfood_val_e' e.g. 40+NaN=NaN, so
 this new code uses s4b_oth_1b_e and s4b_oth_2b_e instead
 -this code includes columns for 'get transfer' and 'give transfer'
 that were not included in the previous table

 Next steps: 
 -divide the standard deviation of CTL mean by the sqrt of the number of control
 observations to make it comparable with the other point estimates and
 standard errors in the table?
*************** END


 #+name: savings_tables
 #+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no :tangle /tmp/foo.py
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
data_baseline = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_midline  = "../../TUP-data/Midline/TUP_midline.dta"
data_endline = "../../TUP-data/Endline/TUP_endline.dta"

import statsmodels.api as sm
import pandas as pd
import numpy as np
from cfe.df_utils import df_to_orgtbl

df_b = pd.read_stata(data_baseline)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)
df_a = pd.read_csv(data_assignment)

df_b.set_index('idno', inplace=True)
df_m.set_index('idno', inplace=True)
df_e.set_index('idno', inplace=True)
df_a.set_index('RespID', inplace=True)

# merge baseline and midline based on idno index

#format savings_maize_q_m  savings_maize_val_m 
#s8b_2c_m and  s8b_2d_m are for other food savings values
#transfers_get1_m is value of transfers
df = pd.merge(df_b[ ['savings_home_b', 'savings_bank_b', 'savings_BRAC_b', 'savings_NGOs_b', 
                     'savings_other_b', 'land_owncult_b', 'land_rentcult_b', 'land_communitycult_b',
                     'land_ownnocult_b', 'land_ownrent_b', 'transfers_get1_b', 'transfers_get2_b', 
                     'transfers_get3_b', 'transfers_give1_b', 'transfers_give2_b', 'transfers_give3_b'] ], 
              df_m[ ['savings_home_m', 'savings_bank_m', 'savings_BRAC_m', 'savings_NGOs_m', 
                     'savings_other_m', 'savings_maize_val_m', 'savings_sorghum_val_m', 
                     's8b_2c_m', 's8b_2d_m', 'land_owncult_m', 'land_rentcult_m', 'land_communitycult_m',
                     'land_ownnocult_m', 'land_ownrent_m', 'transfers_get1_m', 'transfers_get2_m', 
                     'transfers_get3_m', 'transfers_give1_m', 'transfers_give2_m', 'transfers_give3_m'] ], 
              how='outer', left_index=True, right_index=True)


# # merge baseline/midline and endline based on idno index
# endline issue with how sum being calculated for 'savings_otherfood_val_e' - 40+NaN=NaN, so using s4b_oth_1b_e and s4b_oth_2b_e instead
df = pd.merge(df, 
              df_e[ ['savings_home_e', 'savings_bank_e', 'savings_BRAC_e', 'savings_NGOs_e', 
                     'savings_other_e', 'savings_maize_val_e', 'savings_sorghum_val_e', 
                     's4b_oth_1b_e', 's4b_oth_2b_e', 'land_owncult_e', 'land_rentcult_e', 'land_communitycult_e',
                     'land_ownnocult_e', 'land_ownrent_e', 'transfers_get1_e', 'transfers_get2_e', 
                     'transfers_get3_e', 'transfers_give1_e', 'transfers_give2_e', 'transfers_give3_e'] ], 
              how='outer', left_index=True, right_index=True)


df.rename({'s8b_2c_m': 'savings_otherfood1_val_m',
           's8b_2d_m': 'savings_otherfood2_val_m',
           's4b_oth_1b_e': 'savings_otherfood1_val_e',
           's4b_oth_2b_e': 'savings_otherfood2_val_e'}, axis=1, inplace=True)

# find columns that end with "_b", "_m", or "_e"
# cut off the the last 2 characters "_b", "_m", or "_e"
s2013 = df.filter(regex="_b$").rename(columns=lambda x: x[:-2])
s2014 = df.filter(regex="_m$").rename(columns=lambda x: x[:-2])
s2015 = df.filter(regex="_e$").rename(columns=lambda x: x[:-2])

s2013['Year'] = '2013'
s2014['Year'] = '2014'
s2015['Year'] = '2015'

# make the sum variables
for var in (s2013, s2014, s2015):
    var["Savings"] = var[ ['savings_home', 'savings_bank', 'savings_BRAC', 'savings_NGOs', 'savings_other'] ].sum(axis=1)
    
    # no food savings at baseline (2013)
    if (var['Year'] != '2013').any(): 
        var["Food Savings"] = var[ ['savings_maize_val', 'savings_sorghum_val', 'savings_otherfood1_val', 'savings_otherfood2_val'] ].sum(axis=1)
    
    var["LandCult"] = var[ ['land_owncult', 'land_rentcult', 'land_communitycult'] ].sum(axis=1)
    var["LandOwn"] = var[ ['land_owncult', 'land_ownnocult', 'land_ownrent'] ].sum(axis=1)
    var["Get Transfer"] = var[ ['transfers_get1', 'transfers_get2', 'transfers_get3'] ].sum(axis=1)
    var["Give Transfer"] = var[ ['transfers_give1', 'transfers_give2', 'transfers_give3'] ].sum(axis=1)
    
# LandOwn and LandCult are asked in fedan (unit of area)

outcomes_with_baseline = ['Savings', 'LandCult', 'LandOwn', 'Get Transfer', 'Give Transfer'] #excludes Food Savings

for var in (s2013, s2014, s2015): 
    
    # set up to make year dummies
    var.insert(len(var.columns), '2013', 0)
    var.insert(len(var.columns), '2014', 0)
    var.insert(len(var.columns), '2015', 0)
    
    # baseline values as 'var'2013
    for outcome in outcomes_with_baseline: 
        var[outcome+'2013'] = s2013[outcome]
    
# make year dummmies
s2013['2013'].replace(to_replace = 0, value = 1, inplace=True)
s2014['2014'].replace(to_replace = 0, value = 1, inplace=True)
s2015['2015'].replace(to_replace = 0, value = 1, inplace=True)

df2 = pd.concat((s2013, s2014, s2015))
# add group dummies from master_assignment.csv
df = df2.join((df_a[ ['Group'] ].join(pd.get_dummies(df_a['Group']))))

# make interaction variables
df.insert(len(df.columns), 'TUP*2013', df['2013']*df['TUP'])
df.insert(len(df.columns), 'TUP*2014', df['2014']*df['TUP'])
df.insert(len(df.columns), 'TUP*2015', df['2015']*df['TUP'])
df.insert(len(df.columns), 'UCT*2013', df['2013']*df['UCT'])
df.insert(len(df.columns), 'UCT*2014', df['2014']*df['UCT'])
df.insert(len(df.columns), 'UCT*2015', df['2015']*df['UCT'])

outcomes = ['Savings', 'Food Savings', 'LandCult', 'LandOwn', 'Get Transfer', 'Give Transfer']
nonzero_df = df.copy()

# Drop zero values, take logs.
df[outcomes] = np.log(df[outcomes].replace(0,np.nan))
for outcome in outcomes:
    try:
        df[outcome + '2013'] = np.log(df[outcome+'2013'].replace(0,np.nan))
    except KeyError: # No food savings baseline
        pass

# make positive values = 1, 0 values = NaN, and NaN stay NaN
for i in range(len(nonzero_df)):
    for outcome in outcomes:
         if pd.notna(nonzero_df.iloc[i][outcome]):
            if nonzero_df.iloc[i][outcome] <= 0: 
                nonzero_df.iloc[i, nonzero_df.columns.get_loc(outcome)] = 0
            elif nonzero_df.iloc[i][outcome] > 0:
                nonzero_df.iloc[i, nonzero_df.columns.get_loc(outcome)] = 1
                

regular_models_ols = {}
regular_d = {}
nonzero_models_ols = {}
nonzero_d = {}

baseline_na = True

controls = ['TUP*2014', 'TUP*2015', 'UCT*2014', 'UCT*2015', '2014', '2015']
# from above, outcomes_with_baseline = ['Savings', 'LandCult', 'LandOwn', 'Get Transfer', 'Give Transfer'] #excludes Food Savings

# remove observations from 2013
df = df[df['Year'] != '2013']
nonzero_df = nonzero_df[nonzero_df['Year'] != '2013']

for DF in (df, nonzero_df):
    for outcome in outcomes_with_baseline: 
        temp_df = DF[ [outcome, outcome + "2013", 'Control'] + controls]
        temp_controls = controls
    
        if baseline_na==True:
        
            # indicator for whether outcome in 2013 is na, and cast it to be an integer
            temp_df["Baseline_NAN"] = temp_df[outcome + "2013"].isnull().apply(int)
        
            # code missing values of the baseline variable as 0
            temp_df[outcome + "2013"].fillna(0,inplace=True)
        
            temp_controls = temp_controls + ['Baseline_NAN']
    
        temp_controls = temp_controls + [outcome+"2013"]
        temp_df = temp_df.dropna()
    
        if DF.equals(nonzero_df): 
            models_ols = nonzero_models_ols
        else: 
            models_ols = regular_models_ols
            
        models_ols[outcome] = sm.OLS(temp_df[outcome], temp_df[temp_controls]).fit()
    
        # for computing the mean of the control group in 2015
        controlgroup = temp_df[temp_df['Control'] == 1]

        # for computing the difference in TUP and Cash coefficients
        beta_cash_TUP2014 = models_ols[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["coef"][0]
        pvalue_cash_TUP2014 = models_ols[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
        if pvalue_cash_TUP2014 <= 0.01: 
            beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '***'
        elif pvalue_cash_TUP2014 <= 0.05: 
            beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '**'
        elif pvalue_cash_TUP2014 <= 0.1: 
            beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '*'
        
        beta_cash_TUP2015 = models_ols[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["coef"][0]
        pvalue_cash_TUP2015 = models_ols[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
        if pvalue_cash_TUP2015 <= 0.01: 
            beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '***'
        elif pvalue_cash_TUP2015 <= 0.05: 
            beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '**'
        elif pvalue_cash_TUP2015 <= 0.1: 
            beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '*'
        
        if DF.equals(nonzero_df): 
            d = nonzero_d 
        else: 
            d = regular_d
            
        d[outcome] = [controlgroup[controlgroup['2015'] == 1][outcome].mean(),
                  controlgroup[controlgroup['2015'] == 1][outcome].std(),
                  round(models_ols[outcome].params[0], 3), models_ols[outcome].bse[0],
                  round(models_ols[outcome].params[1], 3), models_ols[outcome].bse[1], 
                  round(models_ols[outcome].params[2], 3), models_ols[outcome].bse[2],
                  round(models_ols[outcome].params[3], 3), models_ols[outcome].bse[3],
                  round(models_ols[outcome].params[4], 3), models_ols[outcome].bse[4],
                  round(models_ols[outcome].params[5], 3), models_ols[outcome].bse[5],
                  round(models_ols[outcome].params[6], 3), models_ols[outcome].bse[6],
                  round(models_ols[outcome].params[7], 3), models_ols[outcome].bse[7],
                  models_ols[outcome].fvalue, models_ols[outcome].nobs,
                  beta_cash_TUP2014, beta_cash_TUP2015]
    
        for x in range(0,8):
            # indexes account for standard errors being in the final table
            # e.g. second coefficient is beneath standard error so add asterisks to d[outcome][2] not d[outcome][1] and same for next coefficients
            if models_ols[outcome].pvalues[x] <= 0.01: 
                d[outcome][x*2 + 2] = str(d[outcome][x*2 + 2]) + '***'
            elif models_ols[outcome].pvalues[x] <= 0.05: 
                d[outcome][x*2 + 2] = str(d[outcome][x*2 + 2]) + '**'
            elif models_ols[outcome].pvalues[x] <= 0.1:
                d[outcome][x*2 + 2] = str(d[outcome][x*2 + 2]) + '*'
            d[outcome][x*2+1] = "(%4.2f)" %  d[outcome][x*2+1] 

        del temp_df   

# for food savings, which had no baseline value
for DF in (df, nonzero_df):

    temp_df = DF[ ["Food Savings", 'Control'] + controls]
    temp_df = temp_df.dropna()
    
    if DF.equals(nonzero_df): 
        models_ols = nonzero_models_ols
    else: 
        models_ols = regular_models_ols
    
    models_ols["Food Savings"] = sm.OLS(temp_df["Food Savings"], temp_df[controls]).fit()
    
    # for computing the mean of the control group in 2015
    controlgroup = temp_df[temp_df['Control'] == 1]
    
    # for computing the difference in TUP and Cash coefficients
    beta_cash_TUP2014 = models_ols["Food Savings"].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP2014 = models_ols["Food Savings"].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP2014 <= 0.01: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '***'
    elif pvalue_cash_TUP2014 <= 0.05: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '**'
    elif pvalue_cash_TUP2014 <= 0.1: 
        beta_cash_TUP2014 = str(round(beta_cash_TUP2014, 3)) + '*'
        
    beta_cash_TUP2015 = models_ols["Food Savings"].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP2015 = models_ols["Food Savings"].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP2015 <= 0.01: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '***'
    elif pvalue_cash_TUP2015 <= 0.05: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '**'
    elif pvalue_cash_TUP2015 <= 0.1: 
        beta_cash_TUP2015 = str(round(beta_cash_TUP2015, 3)) + '*'
    
    if DF.equals(nonzero_df): 
        d = nonzero_d 
    else: 
        d = regular_d
            
    d["Food Savings"] = [controlgroup[controlgroup['2015'] == 1]["Food Savings"].mean(),
                  controlgroup[controlgroup['2015'] == 1]["Food Savings"].std(),
                  round(models_ols["Food Savings"].params[0], 3), models_ols["Food Savings"].bse[0],
                  round(models_ols["Food Savings"].params[1], 3), models_ols["Food Savings"].bse[1], 
                  round(models_ols["Food Savings"].params[2], 3), models_ols["Food Savings"].bse[2],
                  round(models_ols["Food Savings"].params[3], 3), models_ols["Food Savings"].bse[3],
                  round(models_ols["Food Savings"].params[4], 3), models_ols["Food Savings"].bse[4],
                  round(models_ols["Food Savings"].params[5], 3), models_ols["Food Savings"].bse[5],
                     "", " ", "  ", "   ",
                  models_ols["Food Savings"].fvalue, models_ols["Food Savings"].nobs, 
                  beta_cash_TUP2014, beta_cash_TUP2015]
    
    for x in range(0,6):
        # indexes account for standard errors being in the final table
        # e.g. second coefficient is beneath standard error so add asterisks to d[outcome][2] not d[outcome][1] and same for next coefficients
        if models_ols["Food Savings"].pvalues[x] <= 0.01: 
            d["Food Savings"][x*2 + 2] = str(d["Food Savings"][x*2 + 2]) + '***'
        elif models_ols["Food Savings"].pvalues[x] <= 0.05: 
            d["Food Savings"][x*2 + 2] = str(d["Food Savings"][x*2 + 2]) + '**'
        elif models_ols["Food Savings"].pvalues[x] <= 0.1:
            d["Food Savings"][x*2 + 2] = str(d["Food Savings"][x*2 + 2]) + '*'
        d["Food Savings"][x*2+1] = "(%4.2f)" %  d["Food Savings"][x*2+1] 

final = pd.DataFrame(data=regular_d)
nonzero_final = pd.DataFrame(data=nonzero_d)

final.rename(index = {0: "CTL mean", 1: " ", 2: "TUP*2014", 3: "  ", 
                      4: "TUP*2015", 5: "   ", 6: "UCT*2014", 7: "    ",
                      8: "UCT*2015", 9: "     ", 10: "2014", 11: "      ",
                      12: "2015", 13: "       ", 14: "Bsln NA", 15: "        ",
                      16: "Bsln2013", 17: "         ", 18: "F-stat", 19: "N",
                      20: r"$\beta_{TUP2014}-\beta_{UCT2015}$", 21: r"$\beta_{TUP2015}-\beta_{UCT2015}$"}, inplace=True)
nonzero_final.rename(index = {0: "CTL mean", 1: " ", 2: "TUP*2014", 3: "  ", 
                      4: "TUP*2015", 5: "   ", 6: "UCT*2014", 7: "    ",
                      8: "UCT*2015", 9: "     ", 10: "2014", 11: "      ",
                      12: "2015", 13: "       ", 14: "Bsln NA", 15: "        ",
                      16: "Bsln2013", 17: "         ", 18: "F-stat", 19: "N",
                      20: r"$\beta_{TUP2014}-\beta_{UCT2015}$", 21: r"$\beta_{TUP2015}-\beta_{UCT2015}$"}, inplace=True)

newdf_nonzero = df_to_orgtbl(nonzero_final, float_fmt = '%.3f')
print("""#+caption: Average treatment effects by group-year on percentage of households reporting any savings or land access""")
print(newdf_nonzero)

newdf = df_to_orgtbl(final, float_fmt = '%.2f')
print("""#+caption: Total value (in SSP) of all cash and food savings and area (in fedan) of land being cultiviated by the household (including rented or temporary-use) and owned by the household.""")
print(newdf)

 #+end_src

 #+results: savings_tables

#+LATEX: \begin{landscape}
 #+name: tab:nonzero_savings
 #+caption: Average treatment effects by group-year on percentage of households reporting any savings or land access 
 #+attr_latex: :environment longtable :align lrrrrrrr
 |                                   | Savings  | LandCult | LandOwn  | Get Transfer | Give Transfer | Food Savings |
 |-----------------------------------+----------+----------+----------+--------------+---------------+--------------|
 | CTL mean                          | 0.394    | 0.718    | 0.806    | 0.170        | 0.080         | 0.726        |
 |                                   | (0.49)   | (0.45)   | (0.40)   | (0.38)       | (0.27)        | (0.45)       |
 |-----------------------------------+----------+----------+----------+--------------+---------------+--------------|
 | TUP*2014                          | 0.287*** | 0.159*** | 0.181*** | 0.081***     | 0.069***      | 0.127***     |
 |                                   | (0.04)   | (0.04)   | (0.04)   | (0.03)       | (0.03)        | (0.04)       |
 | TUP*2015                          | 0.183*** | 0.013    | -0.004   | -0.006       | 0.029         | 0.001        |
 |                                   | (0.04)   | (0.04)   | (0.04)   | (0.03)       | (0.03)        | (0.04)       |
 | UCT*2014                          | 0.030    | 0.112**  | 0.142*** | 0.08**       | 0.034         | 0.196***     |
 |                                   | (0.05)   | (0.05)   | (0.04)   | (0.04)       | (0.03)        | (0.05)       |
 | UCT*2015                          | 0.006    | -0.026   | -0.031   | -0.026       | -0.006        | -0.038       |
 |                                   | (0.05)   | (0.05)   | (0.04)   | (0.04)       | (0.03)        | (0.05)       |
 | 2014                              | 0.333*** | 0.579*** | 0.58***  | 0.136***     | 0.091***      | 0.636***     |
 |                                   | (0.03)   | (0.02)   | (0.02)   | (0.02)       | (0.02)        | (0.02)       |
 | 2015                              | 0.389*** | 0.701*** | 0.784*** | 0.17***      | 0.075***      | 0.726***     |
 |                                   | (0.03)   | (0.02)   | (0.02)   | (0.02)       | (0.02)        | 0.023        |
 | Bsln NA                           | 0.000    | 0.000    | 0.000    | 0.000        | 0.000         |              |
 |                                   | (0.00)   | (0.00)   | (0.00)   | (0.00)       | (0.00)        |              |
 | Bsln2013                          | 0.000    | 0.017**  | 0.015**  | 0.000        | 0.0**         |              |
 |                                   | 0.000    | 0.008    | 0.006    | 0.000        | 0.000         |              |
 |-----------------------------------+----------+----------+----------+--------------+---------------+--------------|
 | F-stat                            | 13.720   | 5.086    | 10.529   | 1.594        | 3.460         | 4.797        |
 | N                                 | 1500.000 | 1500.000 | 1500.000 | 1500.000     | 1500.000      | 1500.000     |
 |-----------------------------------+----------+----------+----------+--------------+---------------+--------------|
 | $\beta_{TUP2014}-\beta_{UCT2015}$ | 0.28***  | 0.185*** | 0.212*** | 0.107**      | 0.075*        | 0.165***     |
 | $\beta_{TUP2015}-\beta_{UCT2015}$ | 0.176*** | 0.039    | 0.027    | 0.021        | 0.035         | 0.039        |
#+LATEX: \end{landscape}

#+LATEX: \begin{landscape}
 #+name: tab:log_savings
  #+attr_latex: :environment longtable :align lrrrrrrr
 #+caption: Average log value (in SSP) of all cash and food savings and area of land.
 |                                   | Savings   | LandCult | LandOwn  | Get Transfer | Give Transfer | Food Savings |
 |-----------------------------------+-----------+----------+----------+--------------+---------------+--------------|
 | CTL mean                          | 4.61      | 0.95     | 0.85     |         5.08 | 4.56          | 4.22         |
 |                                   | (1.23)    | (2.05)   | (1.82)   |       (0.93) | (0.96)        | (0.96)       |
 |-----------------------------------+-----------+----------+----------+--------------+---------------+--------------|
 | TUP*2014                          | -0.304**  | 0.03     | 0.20     |         0.07 | 0.493*        | 0.3***       |
 |                                   | (0.15)    | (0.16)   | (0.15)   |       (0.22) | (0.27)        | (0.09)       |
 | TUP*2015                          | 0.567***  | -0.273*  | -0.13    |         0.13 | -0.31         | 0.06         |
 |                                   | (0.14)    | (0.16)   | (0.14)   |       (0.22) | (0.31)        | (0.09)       |
 | UCT*2014                          | 0.19      | 0.23     | 0.30     |         0.27 | -0.57         | -0.05        |
 |                                   | (0.21)    | (0.21)   | (0.19)   |       (0.26) | (0.36)        | (0.11)       |
 | UCT*2015                          | 0.513**   | -0.447** | -0.348*  |         0.46 | -0.01         | 0.202*       |
 |                                   | (0.20)    | (0.20)   | (0.18)   |       (0.30) | (0.45)        | (0.12)       |
 | 2014                              | 3.434***  | 0.246**  | 0.332*** |     3.856*** | 3.526***      | 3.626***     |
 |                                   | (0.24)    | (0.12)   | (0.11)   |       (0.46) | (0.56)        | (0.06)       |
 | 2015                              | 4.003***  | 0.96***  | 0.887*** |     4.475*** | 4.525***      | 4.216***     |
 |                                   | (0.24)    | (0.11)   | (0.10)   |       (0.46) | (0.56)        | 0.06         |
 | Bsln NA                           | 0.598***  | -0.03    | -0.06    |         0.61 | 0.01          |              |
 |                                   | (0.23)    | (0.12)   | (0.11)   |       (0.46) | (0.55)        |              |
 | Bsln2013                          | 0.123***  | 0.01     | -0.03    |         0.13 | 0.02          |              |
 |                                   | 0.04      | 0.08     | 0.06     |         0.09 | 0.12          |              |
 |-----------------------------------+-----------+----------+----------+--------------+---------------+--------------|
 | F-stat                            | 20.87     | 4.50     | 2.88     |         3.93 | 3.92          | 20.33        |
 | N                                 | 671.00    | 1042.00  | 1114.00  |       255.00 | 159.00        | 1073.00      |
 |-----------------------------------+-----------+----------+----------+--------------+---------------+--------------|
 | $\beta_{TUP2014}-\beta_{UCT2015}$ | -0.818*** | 0.478*   | 0.553**  |        -0.39 | 0.50          | 0.10         |
 | $\beta_{TUP2015}-\beta_{UCT2015}$ | 0.05      | 0.17     | 0.22     |        -0.33 | -0.30         | -0.14        |
#+LATEX: \end{landscape}



** Income
*** Discussion                                                       :ignore:
Income was reliably measured only in 2015, and so our estimates do not control for
baseline values. The control group in 2015 has a measured income of roughly 4300 SSP
per year, or roughly $540 US (assuming an exchange rate of around 8).
We estimate positive impacts on total income for both the TUP and UCT
groups, but none of these are statistically significant (Table [[tab:income]]).  

*** Recreating income table                                          :ignore:
*************** NOTE
This table uses income data from the endline survey. CTL mean comes from the constant
and its standard error in the regression.

Differences with previous code: 
-This code does not do any type of censoring, truncating, or converting of the
original values to US dollars or anything like that
-This code sets the price to NaN if we cannot figure out the market
price for the crop/livestock type and unit that was harvested (7
observations for crops and 3 observations for livestock). The
reason we would not be able to find out the market price is if no one
in the sample provided a market price for that crop livestock/type and
specific unit
- The original table does not have standard errors under CTL mean

Next steps: 
-make bar graph figure
-consider whether multiplying (reported months of work)*(avg monthly
income) (what is currently being used) is better than using reported
annual income
-run the regressions without the constant? The original table does not
have standard errors under CTL mean
*************** END


#+name: income_table
#+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
data_nonfarm = "../../TUP-data/Endline/sections_8_17.csv"
data_farm = "../../TUP-data/Endline/Agriculture_cleaned.csv"

import statsmodels.api as sm
import pandas as pd
import numpy as np
from cfe.df_utils import df_to_orgtbl

df_a = pd.read_csv(data_assignment)
df_n = pd.read_csv(data_nonfarm)
df_f = pd.read_csv(data_farm)

df_n.set_index('res_id', inplace=True)
df_f.set_index('res_id', inplace=True)
df_a.set_index('RespID', inplace=True)

"""Nonagricultural income"""

df_n = df_n.filter(regex='^s16')

# s16_ic is nonfarm months received for a certain source of income for i = 1, 2, 3, 4, 5
# s16_id is nonfarm average income in each month for a certain source of income for i = 1, 2, 3, 4, 5
# using calculation from reported months and avg monthly income instead of reported annual income

nonfarm_yearly = pd.DataFrame() 
for i in range(1,6):
    nonfarm_yearly['source' + str(i)] = df_n['s16_' + str(i) + 'c']*df_n['s16_' + str(i) + 'd']

nonfarm_yearly['nonfarm_yearly_sum'] = nonfarm_yearly.sum(axis=1)


"""Agricultural income"""

# HARVEST VARIABLE NAMES: harvest_type (crop), harvest_size, harvest_unit (qty and unit harvested)
# harvest_price (market price in SSP), harvest_price_unit (market price unit), harvest_sold (qty sold assuming same unit as harvest_unit)

# in case market price unit and harvested/sold unit don't match, 
# calculate median of reported market prices for that crop (type) and market price unit (harvest_price_unit)
unit_prices = df_f.groupby(["harvest_type", "harvest_price_unit"])["harvest_price"].median()

# the format of unit_prices is the first index is harvest_type, the second index is harvest_price_unit, and
# the value is the median of market price units for that crop in that unit

k = 0
d = 0

df_f.insert(len(df_f.columns), 'adjusted_harvest_price', np.nan)

# iterate through each row
for i in range(len(df_f)):
    
    # if neither unit is NaN
    if pd.notna(df_f.iloc[i]['harvest_unit']):
        if pd.notna(df_f.iloc[i]['harvest_price_unit']):
    
            # if units don't match
            if df_f.iloc[i]['harvest_unit'] != df_f.iloc[i]['harvest_price_unit']:
                d = d + 1
                # get the data frame for the particular type of crop
                find_type = unit_prices[unit_prices.index.get_level_values(0) == df_f.iloc[i]['harvest_type']]
        
                # check if there is no reported market price for that crop and unit, and if so, set adjusted harvest price to missing
                if find_type[find_type.index.get_level_values(1) == df_f.iloc[i]['harvest_unit']].empty: 
                    k = k + 1
                    df_f['adjusted_harvest_price'].iloc[i] = np.nan
    
                else: 
                    # set the adjusted harvest price to be the median price for that type and harvest unit
                    df_f['adjusted_harvest_price'].iloc[i] = find_type[find_type.index.get_level_values(1) == df_f.iloc[i]['harvest_unit']][0]
        
            # if units match
            else: df_f['adjusted_harvest_price'].iloc[i] = df_f.iloc[i]['harvest_price']

#d is the number of rows for which the harvest unit and market price unit don't match (193)
#k is the number of rows for which the harvest unit and crop do not have any market price reported (7)
#print("there are {} rows for which no one reported a market price for that crop and unit".format(k))

farm_yearly = pd.DataFrame() 
df_f["farm_income_yearly"] = df_f["harvest_size"]*df_f["adjusted_harvest_price"]

# there could be  multiple rows for the same household, so we group by respondent ID
farm_yearly['farm_income_yearly_sum'] = df_f[ ['farm_income_yearly'] ].dropna().groupby("res_id")["farm_income_yearly"].sum()


# LIVESTOCK VARIABLE NAMES: livestock_type (animal), livestock_size, livestock_unit (qty and unit produced)
# livestock_price (market price in SSP), livestock_price_unit (market price unit), livestock_sold (qty sold assuming same unit as livestock_unit)

# in case market price unit and produced/sold unit don't match, 
# calculate median of reported market prices for that animal (type) and market price unit (livestock_price_unit)
unit_prices = df_f.groupby(["livestock_type", "livestock_price_unit"])["livestock_price"].median()
# the format of unit_prices is the first index is livestock_type, the second index is livestock_price_unit, and
# the value is the median of market price units for that animal in that unit

k = 0
d = 0

df_f.insert(len(df_f.columns), 'adjusted_livestock_price', np.nan)

# iterate through each row
for i in range(len(df_f)):
    
    # if neither unit is NaN
    if pd.notna(df_f.iloc[i]['livestock_unit']):
        if pd.notna(df_f.iloc[i]['livestock_price_unit']):
    
            # if units don't match
            if df_f.iloc[i]['livestock_unit'] != df_f.iloc[i]['livestock_price_unit']:
                d = d + 1
                # get the data frame for the particular type of animal
                find_type = unit_prices[unit_prices.index.get_level_values(0) == df_f.iloc[i]['livestock_type']]
        
                # check if there is no reported market price for that animal and unit, and if so, set adjusted harvest price to missing
                if find_type[find_type.index.get_level_values(1) == df_f.iloc[i]['livestock_unit']].empty: 
                    k = k + 1
                    df_f['adjusted_livestock_price'].iloc[i] = np.nan
                else: 
                    # set the adjusted livestock price to be the median price for that type and livestock unit
                    df_f['adjusted_livestock_price'].iloc[i] = find_type[find_type.index.get_level_values(1) == df_f.iloc[i]['livestock_unit']][0]
        
            # if units match
            else: df_f['adjusted_livestock_price'].iloc[i] = df_f.iloc[i]['livestock_price']

#d is the number of rows for which the livestock unit and market price unit don't match (9)
#k is the number of rows for which the livestock unit and crop do not have any market price reported (3)
# print("there are {} rows for which no one reported a market price for that animal and unit".format(k))
 
df_f["livestock_income_yearly"] = df_f["livestock_size"]*df_f["adjusted_livestock_price"]

# there could be  multiple rows for the same household, so we group by respondent ID
farm_yearly['livestock_income_yearly_sum'] = df_f[ ['livestock_income_yearly'] ].dropna().groupby("res_id")["livestock_income_yearly"].sum()

df = pd.merge(nonfarm_yearly[ ['nonfarm_yearly_sum'] ], farm_yearly, how='outer', left_index=True, right_index=True)
df = pd.merge(df, df_a[ ['Group'] ].join(pd.get_dummies(df_a['Group'])), how='inner', left_index=True, right_index=True)

df.insert(len(df.columns), 'total', df[['nonfarm_yearly_sum', 'farm_income_yearly_sum', 'livestock_income_yearly_sum']].sum(axis=1))
df.insert(len(df.columns), 'constant', 1)

df.rename(columns = {'farm_income_yearly_sum': "Farm", 'livestock_income_yearly_sum': "Livestock", 
                   'nonfarm_yearly_sum': "Non-Farm", 'total': "Total"}, inplace=True)

outcomes = ["Farm", "Livestock", "Non-Farm", "Total"]
controls = ['constant', 'TUP', 'UCT']
models_ols = {}
d = {}

for outcome in outcomes: 
    temp_df = df[ [outcome] + controls].dropna()
    
    models_ols[outcome] = sm.OLS(temp_df[outcome], temp_df[controls]).fit()
    
    # for computing the difference in TUP and Cash coefficients
    beta_cash_TUP = models_ols[outcome].t_test("TUP - UCT = 0").summary_frame()["coef"][0]
    pvalue_cash_TUP = models_ols[outcome].t_test("TUP - UCT = 0").summary_frame()["P>|t|"][0]
    
    if pvalue_cash_TUP <= 0.01: 
        beta_cash_TUP = str(round(beta_cash_TUP, 3)) + '***'
    elif pvalue_cash_TUP <= 0.05: 
        beta_cash_TUP = str(round(beta_cash_TUP, 3)) + '**'
    elif pvalue_cash_TUP <= 0.1: 
        beta_cash_TUP = str(round(beta_cash_TUP, 3)) + '*'

    d[outcome] = [round(models_ols[outcome].params[0], 3), models_ols[outcome].bse[0],
                  round(models_ols[outcome].params[1], 3), models_ols[outcome].bse[1], 
                  round(models_ols[outcome].params[2], 3), models_ols[outcome].bse[2],
                  models_ols[outcome].fvalue, models_ols[outcome].nobs, 
                  beta_cash_TUP]

    for x in range(0,3):
        # indexes account for standard errors being in the final table
        # e.g. second coefficient is beneath standard error so add asterisks to d[outcome][2] not d[outcome][1] and same for next coefficients
        if models_ols[outcome].pvalues[x] <= 0.01: 
            d[outcome][x*2] = str(d[outcome][x*2]) + '***'
        elif models_ols[outcome].pvalues[x] <= 0.05: 
            d[outcome][x*2] = str(d[outcome][x*2]) + '**'
        elif models_ols[outcome].pvalues[x] <= 0.1:
            d[outcome][x*2] = str(d[outcome][x*2]) + '*'

final = pd.DataFrame(data=d)

# name the rows of the table
final.rename(index = {0: "CTL mean", 1: " ", 2: "TUP", 3: "  ", 
                      4: "UCT", 5: "   ", 6: "F-stat", 7: "N", 8: "beta_TUP-beta_CASH"}, inplace=True)

newdf = df_to_orgtbl(final, float_fmt = '%.3f')
print(newdf)

#+end_src

#+name: tab:income
#+caption: Average treatment effects by group-year on total value (in SSP) of income reported in 2015 by sector.
|                           |       Farm |  Livestock |    Non-Farm |       Total |
|---------------------------+------------+------------+-------------+-------------|
| CTL mean                  | 917.315*** | 924.611*** | 3326.162*** | 4317.337*** |
|                           |     64.809 |    164.390 |     527.282 |     537.231 |
| TUP                       |   -139.780 |    117.096 |     964.323 |    1253.478 |
|                           |    100.918 |    215.596 |     829.082 |     844.725 |
| UCT                       |     48.833 |   -282.164 |     507.132 |     510.939 |
|                           |    133.947 |    298.534 |    1077.298 |    1097.625 |
|---------------------------+------------+------------+-------------+-------------|
| F-stat                    |      1.309 |      0.983 |       0.683 |       1.101 |
| N                         |    532.000 |    305.000 |     690.000 |     690.000 |
|---------------------------+------------+------------+-------------+-------------|
| $\beta_{TUP}-\beta_{UCT}$ |   -188.613 |    399.260 |     457.192 |     742.540 |

** Consumption

The next outcome of interest we consider is household consumption,
defined as the market value of goods or services used by the
household.  A sizable basket of goods were included in the survey
module. These are separated into three categories: Food items (with a
3-day recall window), non-durables (a 30-day recall window), and
durables and large expenditures (a one-year recall window).  We think
that data on consumption provides the most appropriate measure of household
welfare in our survey.

\Tab{consumption} reports on consumption expenditures aggregated
according to the period of recall.  There were large changes in prices
during the course of our experiment, and small changes in the way the
survey elicited item-level expenditures.  Both of these kinds of
changes make comparisons across time difficult to interpret, and might
make comparisons of total consumption misleading.

These data do permit us to answer the central question we're
interested in: what are the effects on consumption, about eight months
later, of enrolling randomly selected households into the TUP program
versus giving them a cash transfer of roughly equal value?  We
measured the effect of cash on a three-day recall of food consumption
12 months after the transfer, and find a statistically significant
increase of 14 SSP per day.  We first measured the effect of the TUP
program on average food consumption 6--9 months after the asset
transfers, and found a slightly larger effect of 18 SSP. The
differences between these two are not significant (see the \(t\)-test
in the row labeled $\beta_{TUP}^{2014}-\beta_{UCT}^{2015}$), and both
result in a 13--17% increase in the value of food expenditures
relative to the control group mean in 2015.

Turning our attention to consumption expenditures involving a monthly
recall period, both programs have positive effects of similar
magnitude, significant at the 10% level.  Differences between the two
are not significant. 

At annual frequencies we discover a significant difference: the effect
of the TUP program on average annual durable expenditures is
significantly greater (at the 10% level) than the effect of the UCT
program, with the average difference equal to 484 SSP, a 46% increase
relative to the average expenditures of the control group in 2015.

#+name: consumption_tables
#+begin_src python :noweb no-export :exports none  :results output table raw :var FE=0 baseline_na=1 :colnames no :tangle consumption_tables.py
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
data_baseline = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_midline  = "../../TUP-data/Midline/TUP_midline.dta"
data_endline = "../../TUP-data/Endline/TUP_endline.dta"

import statsmodels.api as sm
import pandas as pd
import numpy as np
from scipy import stats
from cfe.df_utils import df_to_orgtbl

df_b = pd.read_stata(data_baseline)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)
df_a = pd.read_csv(data_assignment)

# set Respondent ID number as the index
df_b.set_index('idno', inplace=True)
df_m.set_index('idno', inplace=True)
df_e.set_index('idno', inplace=True)
df_a.set_index('RespID', inplace=True)

# missing s20_6m_b and s20_7m_b in monthly for baseline - should be othermonth b
df_b.insert(len(df_b.columns), 'c_othermonth_b', df_b[ ['s20_6m_b', 's20_7m_b'] ].sum(axis=1))

# find variables that start with "c_" and remove "c_" and remove "_b", "_m", or "_e"
c2013 = df_b.filter(regex='^c_').rename(columns=lambda x: x[2:-2])
c2014 = df_m.filter(regex='^c_').rename(columns=lambda x: x[2:-2])
c2015 = df_e.filter(regex='^c_').rename(columns=lambda x: x[2:-2])

c2013['Year'] = '2013'
c2014['Year'] = '2014'
c2015['Year'] = '2015'

# missing durables from monthly for midline in previous code - added it in
# doesn't make sense to ask for clothing and footwear in total and then women's clothes, children's clothes, school uniforms, shoes in midline and endline
# ^ and they don't always sum up - change this?
# #some not asked for in baseline

food = ['cereals', 'maize', 'sorghum', 'millet', 'potato', 'sweetpotato', 'rice', 'bread', 'beans', 'oil', 'salt', 'sugar', 'meat', 'poultry', 'fish', 'egg', 'nuts', 'milk', 'vegetables', 'fruit', 'tea', 'spices', 'alcohol', 'otherfood']
month = ['fuel', 'medicine', 'airtime', 'cosmetics', 'soap', 'transport', 'entertainment', 'childcare', 'tobacco', 'batteries', 'durables', 'church', 'othermonth']    
year = ['clothesfootwear', 'womensclothes', 'childrensclothes', 'shoes', 'homeimprovement', 'utensils', 'furniture', 'textiles', 'ceremonies', 'funerals', 'charities', 'dowry', 'other']    

outcomes = ["Food", "Monthly", "Yearly"]

# not replacing 0s with np.nan; seems to be 4 observations that have legitimate 0s
for var in (c2013, c2014, c2015):
    
    var["Food"] = var.filter(items=food).sum(axis=1)
    var["Monthly"] = var.filter(items=month).sum(axis=1)
    var["Yearly"] = var.filter(items=year).sum(axis=1)
    
    # set up to make year dummies
    var['2013'] = 0
    var['2014'] = 0
    var['2015'] = 0
    
    # set up baseline2013 variables
    for outcome in outcomes: 
        var[outcome+'2013'] = c2013[outcome]

# make year dummies
c2013['2013'].replace(to_replace = 0, value = 1, inplace=True)
c2014['2014'].replace(to_replace = 0, value = 1, inplace=True)
c2015['2015'].replace(to_replace = 0, value = 1, inplace=True)

df2 = pd.concat((c2013, c2014, c2015))
# add group dummies from master_assignment.csv
df = df2.join((df_a[ ['Group'] ].join(pd.get_dummies(df_a['Group']))))   

# make interaction terms
df.insert(len(df.columns), 'TUP*2013', df['2013']*df['TUP'])
df.insert(len(df.columns), 'TUP*2014', df['2014']*df['TUP'])
df.insert(len(df.columns), 'TUP*2015', df['2015']*df['TUP'])
df.insert(len(df.columns), 'UCT*2013', df['2013']*df['UCT'])
df.insert(len(df.columns), 'UCT*2014', df['2014']*df['UCT'])
df.insert(len(df.columns), 'UCT*2015', df['2015']*df['UCT'])

models_ols = {}
b = {}
se = {}

controls = ['TUP*2014', 'TUP*2015', 'UCT*2014', 'UCT*2015', '2014', '2015']

if FE:
    controls += ['TUP', 'UCT']

# remove observations from 2013, since conditioning on baseline
df = df[df['Year'] != '2013']

for outcome in outcomes: 
    temp_df = df[ [outcome, outcome + "2013", 'Control'] + controls]
    temp_controls = controls
    temp_df.rename(columns={outcome + "2013":'Baseline Value'},inplace=True)
    
    if baseline_na==True:
        
        # indicator for whether outcome in 2013 is na, and cast it to be an integer
        temp_df["Baseline missing"] = temp_df['Baseline Value'].isnull().apply(int)
        
        # code missing values of the baseline variable as 0
        temp_df['Baseline Value'].fillna(0,inplace=True)
        
        temp_controls = temp_controls + ['Baseline missing']
    
    temp_controls = temp_controls + ['Baseline Value']
    temp_df = temp_df.dropna()
    
    models_ols[outcome] = sm.OLS(temp_df[outcome], temp_df[temp_controls]).fit()

    # for computing the mean of the control group in 2015
    controlgroup = temp_df[temp_df['Control'] == 1]
    
    """for table with Fixed Effects"""

    b[outcome] = models_ols[outcome].params
    se[outcome] = models_ols[outcome].bse
    
    b[outcome]['Control mean'] = controlgroup[controlgroup['2015'] == 1][outcome].mean()
    se[outcome]['Control mean'] = controlgroup[controlgroup['2015'] == 1][outcome].std()/np.sqrt(controlgroup[controlgroup['2015'] == 1][outcome].count())

    b[outcome][r'$\beta_{TUP}^{2014}-\beta_{UCT}^{2015}$'] = models_ols[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["coef"][0]
    se[outcome][r'$\beta_{TUP}^{2014}-\beta_{UCT}^{2015}$'] = b[outcome][r'$\beta_{TUP}^{2014}-\beta_{UCT}^{2015}$']/models_ols[outcome].t_test("TUP*2014 - UCT*2015 = 0").summary_frame()["t"][0]

    b[outcome][r'$\beta_{TUP}^{2015}-\beta_{UCT}^{2015}$'] = models_ols[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["coef"][0]
    se[outcome][r'$\beta_{TUP}^{2015}-\beta_{UCT}^{2015}$'] = b[outcome][r'$\beta_{TUP}^{2015}-\beta_{UCT}^{2015}$']/models_ols[outcome].t_test("TUP*2015 - UCT*2015 = 0").summary_frame()["t"][0]

b = pd.DataFrame(b)
se = pd.DataFrame(se)

print("#+name: tab:consumption_w_fe")
print("#+caption: Consumption Results")
print(df_to_orgtbl(b, sedf = se, float_fmt = '%.2f'))


#+end_src

#+name: tab:consumption
#+caption: Effect on consumption expenditures with different periods of recall.  Food expenditures are for the past three days, non-durable non-food for the past month, and durables the past year.
|                                         | Food (3-day) | Monthly      | Yearly        |
|-----------------------------------------+--------------+--------------+---------------|
| TUP*2014                                | 18.41^{***}  | 59.23^{*}    | 435.43^{**}   |
|                                         | (5.92)       | (31.20)      | (176.15)      |
| TUP*2015                                | 1.82         | 34.76        | -8.48         |
|                                         | (5.62)       | (29.65)      | (167.41)      |
| UCT*2014                                | -3.80        | 32.96        | -170.68       |
|                                         | (7.34)       | (38.70)      | (218.48)      |
| UCT*2015                                | 14.18^{**}   | 69.81^{*}    | -48.24        |
|                                         | (7.20)       | (37.96)      | (214.33)      |
| 2014                                    | 73.52^{***}  | 259.41^{***} | 808.30^{***}  |
|                                         | (4.87)       | (23.51)      | (125.90)      |
| 2015                                    | 103.00^{***} | 294.59^{***} | 1022.19^{***} |
|                                         | (4.56)       | (21.91)      | (115.48)      |
| Baseline missing                        | 15.72^{**}   | -13.31       | 17.33         |
|                                         | (6.50)       | (32.81)      | (180.06)      |
| Baseline value                          | 0.06^{*}     | -0.05        | 0.04          |
|                                         | (0.03)       | (0.14)       | (0.03)        |
|-----------------------------------------+--------------+--------------+---------------|
| Control mean (2015)                     | 110.34^{***} | 288.60^{***} | 1051.76^{***} |
|                                         | (3.89)       | (16.56)      | (103.24)      |
| $\beta_{TUP2014}-\beta_{UCT2015}$       | 4.23         | -10.58       | 483.67^{*}    |
|                                         | (9.17)       | (48.36)      | (273.11)      |
| $\beta_{TUP2015}-\beta_{UCT2015}$       | -12.36^{*}   | -35.06       | 39.77         |
|                                         | (7.45)       | (39.29)      | (221.80)      |


It is worth noting that the effects of the TUP program on consumption
are not lasting---two years after the distribution of assets (in 2015)
the average consumption expenditures in the TUP group are not
significantly different from that of the control across all types of
consumption.  The delayed roll-out of UCT intervention means that we
can't measure the two-year effects of this program on these measures
of consumption.

** Food Insecurity

Observed changes in total consumption don't translate into significant
decreases in reported food insecurity.  In each year, we ask how often
in a given week the respondent has had experiences indicative of food
insecurity. Included are (from left to right) eating "fewer meals"
during the day than usual; going a whole day without eating, going to
"sleep hungry"; having "no food in the house"; eating only "limited
portions" at mealtimes; going the "whole day without eating"; and
being "worried" about not having enough food.  The rows corresponding
to 2014 and 2015 can be interpreted as the proportion of people who
report experiencing each in a typical week.  While average levels of
these variables indicate very high food insecurity, and an
increase in food insecurity between 2014 and 2015, the estimated
effects of both the TUP and UCT programs are comparatively small and
are generally statistically insignificant.

*** Notes on Recreating food insecurity table                      :noexport:

The table looks at whether respondents experienced going a whole
day without eating, limiting portions, etc. in a typical week.  
The z-score is calculated by taking the sum of binary variables 'worried', 
'portions', 'fewmeals', 'nofood', 'hungry', 'wholeday' for each
individual, separately at baseline, midline, and endline. Call that
'fs_sum_m' for an individual's sum at midline. Then their z-score at
midline  would be (fs_sum_m - fs_sum_m.mean())/fs_sum_m.std(). CTL
mean lists the mean and standard deviation of the control group in 2015.

Differences with previous code: 
-correction for what should be considered "weekly" (this code
includes "Everyday", "3-6 times a week", and "1-2 times a week" as
being considered weekly, whereas the previous code left out "1-2 times a week")
-This code adds 'worried' to the table. It uses 'worried' to
calculate the z-score, which  was also used to calculate
the z-score in the previous table.

Next steps: 
-change how the z-score is being calculated because .sum(axis=1)
equates NaN to 0, so some respondents who had NaN for every measure
will still have a z-score, hence the higher N for 'z-score' than for
the other variables
-divide the standard deviation of CTL mean by the sqrt of the number of control
observations to make it comparable with the other point estimates and
standard errors in the table?
-add another column with f statistic for joint hypothesis that all coefficients are equal to 0

#+name: food_insecurity
#+begin_src python :noweb no-export :exports none  :results output table raw :var FE=1 :colnames no :tangle food_security.py
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
data_baseline = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_midline  = "../../TUP-data/Midline/TUP_midline.dta"
data_endline = "../../TUP-data/Endline/TUP_endline.dta"

import statsmodels.api as sm
import pandas as pd
import numpy as np
from scipy import stats
from scipy.linalg import block_diag
from cfe.df_utils import df_to_orgtbl

df_b = pd.read_stata(data_baseline)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)
df_a = pd.read_csv(data_assignment).rename(columns={'RespID':'idno'})

# set Respondent ID number as the index
df_b.set_index('idno', inplace=True)
df_m.set_index('idno', inplace=True)
df_e.set_index('idno', inplace=True)
df_a.set_index('idno', inplace=True)

df_b['Year'] = 2013
df_m['Year'] = 2014
df_e['Year'] = 2015

# merge baseline and midline based on idno index
df = pd.merge(df_b[ ['Year','fs_worried_b', 'fs_portions_b', 
                     'fs_fewmeals_b', 'fs_nofood_b', 
                     'fs_hungry_b', 'fs_wholeday_b'] ], 
              df_m[ ['Year','fs_worried_m', 'fs_portions_m', 
                      'fs_fewmeals_m', 'fs_nofood_m', 
                      'fs_hungry_m', 'fs_wholeday_m'] ], 
              how='outer', left_index=True, right_index=True)

# merge baseline/midline and endline based on idno index
df = pd.merge(df, 
              df_e[ ['Year','fs_worried_e', 'fs_portions_e', 
                     'fs_fewmeals_e', 'fs_nofood_e', 
                     'fs_hungry_e', 'fs_wholeday_e'] ], 
              how='outer', left_index=True, right_index=True)

# replace words with their corresponding code number from the survey
df.replace({"Everyday": 1,
            "everyday": 1,
            "3-6 times a week": 2,
            "1-2 times a week": 3,
            "Less than once a week": 4,
            "less than once a week": 4,
            "Never": 5,
            "never": 5}, inplace=True)

# for weekly, replace 4 and 5 with 0, and 1, 2, and 3 with 1
df.replace({1: 1, 2: 1, 3: 1, 4: 0, 5: 0}, inplace=True)

# find columns that start with "fs_" and end with "_b", "_m", or "_e"
# cut off the first 3 characters "fs_" and the last 2 "_b", "_m", or "_e"
fs2013 = df.filter(regex="^fs_.*_b").rename(columns=lambda x: x[3:-2])
fs2014 = df.filter(regex="^fs_.*_m").rename(columns=lambda x: x[3:-2])
fs2015 = df.filter(regex="^fs_.*_e").rename(columns=lambda x: x[3:-2])

fs2013['Year'] = '2013'
fs2014['Year'] = '2014'
fs2015['Year'] = '2015'

# used to calculate the z-score
index_vars = ['worried', 'portions', 'fewmeals', 'nofood', 'hungry', 'wholeday']
outcomes = ['z-score'] + index_vars 

for FS in (fs2013, fs2014, fs2015): 
    fs_sum = FS[index_vars].sum(axis=1)
    FS['z-score'] = (fs_sum - fs_sum.mean())/fs_sum.std()
    
    # set up to make year dummies
    FS.insert(len(FS.columns), '2013', 0)
    FS.insert(len(FS.columns), '2014', 0)
    FS.insert(len(FS.columns), '2015', 0)
    
    # baseline values as 'var'2013
    for var in outcomes: 
        FS[var+'2013'] = fs2013[var]
    
# make year dummmies
fs2013['2013'].replace(to_replace = 0, value = 1, inplace=True)
fs2014['2014'].replace(to_replace = 0, value = 1, inplace=True)
fs2015['2015'].replace(to_replace = 0, value = 1, inplace=True)

df2 = pd.concat((fs2013, fs2014, fs2015))
# make group dummies using 'Group' from master_assignment.csv
df = df2.join((df_a[ ['Group'] ].join(pd.get_dummies(df_a['Group']))))

# make interaction variables
df.insert(len(df.columns), 'TUP*2013', df['2013']*df['TUP'])
df.insert(len(df.columns), 'TUP*2014', df['2014']*df['TUP'])
df.insert(len(df.columns), 'TUP*2015', df['2015']*df['TUP'])
df.insert(len(df.columns), 'UCT*2013', df['2013']*df['UCT'])
df.insert(len(df.columns), 'UCT*2014', df['2014']*df['UCT'])
df.insert(len(df.columns), 'UCT*2015', df['2015']*df['UCT'])

baseline_na = True

controls = ['TUP*2014', 'TUP*2015', 'UCT*2014', 'UCT*2015', '2014', '2015']
outcomes = ['z-score', 'wholeday', 'hungry', 'nofood', 'fewmeals', 'portions', 'worried']
outcomes = ['wholeday', 'hungry', 'nofood', 'fewmeals', 'portions', 'worried']

# remove observations from 2013
df = df[df['Year'] != '2013']
df = df.reset_index().set_index(['idno','Year'])

myX = {}
myY = {}
for outcome in outcomes: 
    temp_df = df[ [outcome, outcome + "2013", 'Control'] + controls]
    temp_df.rename(columns={outcome+"2013":'Baseline value'},inplace=True)
    temp_controls = controls
    
    if baseline_na==True:
        
        # indicator for whether outcome in 2013 is na, and cast it to be an integer
        temp_df["Baseline missing"] = temp_df["Baseline value"].isnull().apply(int)
        
        # code missing values of the baseline variable as 0
        temp_df["Baseline missing"].fillna(0,inplace=True)
        
        temp_controls = temp_controls + ['Baseline missing']
    
    temp_controls = temp_controls + ["Baseline value"]
    temp_df = temp_df.dropna()

    myX[outcome] = temp_df[temp_controls]
    myY[outcome] = temp_df[outcome]

myY = pd.concat(myY)
myX = pd.DataFrame(block_diag(*myX.values()),
                   columns=pd.concat(myX,axis=1).columns,
                   index=myY.index)
    
est = sm.OLS(myY,myX).fit()
b = est.params
se = est.bse

b.index = pd.MultiIndex.from_tuples([tuple(i.split('_')) for i in est.params.index])
se.index = pd.MultiIndex.from_tuples([tuple(i.split('_')) for i in est.params.index])    


b = b.unstack()
b['TUP*2014 - UCT*2015'] = b['TUP*2014'] - b['UCT*2015']
b['TUP*2015 - UCT*2015'] = b['TUP*2015'] - b['UCT*2015']
b = b.T
b.rename(columns={'worried':'Worried', 'portions':'Limited portions', 'fewmeals':'Fewer Meals', 'nofood':'No food in house', 'hungry':'Went hungry', 'wholeday':'Whole day without eating'},inplace=True)

se = se.unstack()
se['TUP*2014 - UCT*2015'] = np.sqrt(se['TUP*2014']**2 + se['UCT*2015']**2)
se['TUP*2015 - UCT*2015'] = np.sqrt(se['TUP*2015']**2 + se['UCT*2015']**2)

se = se.T
se.rename(columns={'worried':'Worried', 'portions':'Limited portions', 'fewmeals':'Fewer Meals', 'nofood':'No food in house', 'hungry':'Went hungry', 'wholeday':'Whole day without eating'},inplace=True)

print(df_to_orgtbl(b,sedf=se))
#+end_src

*** food insecurity table                                            :ignore:

#+latex: \begin{landscape}
#+name: tab:food_insecurity
#+caption: Effects of programs on different measures of food insecurity.
|                                     | Fewer Meals     | Went hungry     | No food in house | Limited portions | Whole day without eating | Worried         |
|-------------------------------------+-----------------+-----------------+------------------+------------------+--------------------------+-----------------|
| 2014                                | \(0.716\)^{***} | \(0.370\)^{***} | \(0.470\)^{***}  | \(0.660\)^{***}  | \(0.369\)^{***}          | \(0.758\)^{***} |
|                                     | (\(0.046\))     | (\(0.036\))     | (\(0.040\))      | (\(0.036\))      | (\(0.035\))              | (\(0.050\))     |
| 2015                                | \(0.735\)^{***} | \(0.430\)^{***} | \(0.561\)^{***}  | \(0.795\)^{***}  | \(0.461\)^{***}          | \(0.917\)^{***} |
|                                     | (\(0.044\))     | (\(0.034\))     | (\(0.038\))      | (\(0.034\))      | (\(0.033\))              | (\(0.048\))     |
| Baseline missing                    | \(0.000\)       | \(0.000\)       | \(0.000\)        | \(0.000\)        | \(0.000\)                | \(0.000\)       |
|                                     | (\(0.000\))     | (\(0.000\))     | (\(0.000\))      | (\(0.000\))      | (\(0.000\))              | (\(0.000\))     |
| Baseline value                      | \(0.014\)       | \(0.058\)^{**}  | \(0.050\)        | \(0.027\)        | \(0.010\)                | \(-0.030\)      |
|                                     | (\(0.038\))     | (\(0.028\))     | (\(0.033\))      | (\(0.028\))      | (\(0.028\))              | (\(0.043\))     |
| TUP*2014                            | \(-0.037\)      | \(-0.058\)      | \(-0.085\)^{*}   | \(0.065\)        | \(-0.020\)               | \(0.055\)       |
|                                     | (\(0.044\))     | (\(0.043\))     | (\(0.044\))      | (\(0.044\))      | (\(0.044\))              | (\(0.044\))     |
| TUP*2015                            | \(0.063\)       | \(0.079\)^{*}   | \(0.025\)        | \(-0.014\)       | \(0.013\)                | \(-0.019\)      |
|                                     | (\(0.041\))     | (\(0.041\))     | (\(0.042\))      | (\(0.042\))      | (\(0.042\))              | (\(0.042\))     |
| UCT*2014                            | \(-0.012\)      | \(-0.017\)      | \(0.015\)        | \(0.037\)        | \(-0.017\)               | \(0.052\)       |
|                                     | (\(0.053\))     | (\(0.053\))     | (\(0.054\))      | (\(0.054\))      | (\(0.054\))              | (\(0.053\))     |
| UCT*2015                            | \(0.037\)       | \(0.048\)       | \(0.051\)        | \(-0.009\)       | \(-0.015\)               | \(-0.010\)      |
|                                     | (\(0.052\))     | (\(0.052\))     | (\(0.053\))      | (\(0.053\))      | (\(0.053\))              | (\(0.052\))     |
|-------------------------------------+-----------------+-----------------+------------------+------------------+--------------------------+-----------------|
| $\beta_{TUP2014} - \beta_{UCT2015}$ | \(-0.073\)      | \(-0.105\)      | \(-0.136\)^{**}  | \(0.074\)        | \(-0.004\)               | \(0.065\)       |
|                                     | (\(0.068\))     | (\(0.068\))     | (\(0.068\))      | (\(0.068\))      | (\(0.069\))              | (\(0.068\))     |
| $\beta_{TUP2015} - \beta_{UCT2015}$ | \(0.027\)       | \(0.032\)       | \(-0.026\)       | \(-0.005\)       | \(0.028\)                | \(-0.008\)      |
|                                     | (\(0.067\))     | (\(0.067\))     | (\(0.067\))      | (\(0.067\))      | (\(0.068\))              | (\(0.067\))     |
#+latex: \end{landscape}

** Exposure to Conflict
*** Discussion                                                       :ignore:
In 2014, households were surveyed shortly after the NGO's offices had re-opened in
the wake of the outbreak of widespread armed conflict. Respondents were asked a short
set of questions about whether they were directly affected, and if so, in what way.
There had only been a few incidents of violence near Yei town at that point, and the most
directly involved ethnic groups made up a small portion of the local population. There
is no clear comparison group to which we might compare our sample, and the economic
climate changed over this same period in several ways that were probably not directly
caused by the violence.  We have no clear means of identifying the effect of
the conflict itself on household welfare. Nonetheless, it is interesting to consider
correlates with self-reported exposure to the conflict, and to see if program
assignment had any effect on households' reported exposure or response.

In Table [[tab:conflict]], the UCT group is included with controls
because the cash transfers weren't given out until immediately after
the midline survey, and this table uses data from the midline
survey. All variables are binary variables. CTL mean comes from the constant
and its standard error in the regression.

Our main outcomes of interest are whether individuals say they were
"/Worried/" or "directly /Affected/" by the violence, unable to invest
(/NoInvest/) in
a farm or business as a result, /Migrated/ as a cautionary measure, or
did something else to /ProtectLives/ of family members. A final
question among those who took no cautionary measures was whether this
because they did not have the means (/NoMeans/). TUP participants
are 24% (13 pp.) less likely to report having been "Affected" by the
conflict, and 38% (7.6 pp.) less likely to report that they were
affected specifically by being unable to plant crops or invest in
their business. This was the second most common way in which
households reported being affected behind "needed to relocate or
migrate", where TUP respondents are not clearly different. Finally,
TUP respondents were significantly less likely (10.9 pp) to report
that they took no action to help others because they lacked the means
(/NoMeans/) to do so.

Overall, nearly all households were worried (93%), and nearly 40%
migrated to avoid violence; 31% also did something else to protect the
lives of their family members.  In these reactions TUP households were
not detectably different from the control group.  However, TUP
households were significantly (13 pp.) less likely to report that they were
directly affected by violence; also 8 pp. less likely to report that
their ability to invest in land or business was affected by the
conflict; and 11 pp. less likely to report that any /lack/ of action
was due to a lack of means.  All these significant differences
seem to indicate that the TUP program had some mitigating effect on
the effects of conflict on the household.

*** Recreating the exposure to conflict table                        :ignore:

*************** NOTE
Affected and Worried are whether or not respondents answered that they
were "directly affected by any of the violence" or "worried that the
conflict would affect you since December," respectively. NoInvest is
whether they said that one way they were directly affected is that
they could not invest in business. NoMeans is whether they said they
did not do anything to protect the lives of family members or to
protect their assets, and listed the reason as "didn't have the
means." ProtectLives is whether the respondent did anything (including
migration) besides "nothing" to protect the lives of their family
members - this could be migrating to stay with friend or family,
migrating and finding own accommodation, looking for protection with
NGO, or looking for protection with government or military. Migrated
is if the respondent listed migration as one of the ways they were
directly affected, and/or if they said they migrated to protect the
lives of their family.

Differences with previous code: 
-this code adds relevant variables ('s22_4d_m' and 's22_4e_m': other
ways you were directly affected by conflict) which were not renamed to
include 'conflict'
-N for NoMeans in this table is lower than in the previous table
because the question was: if they answered no to the previous two
questions, 'why did you do nothing to protect yourself and assets?'
So only a subset of people who said they didn't do anything to protect 
the lives of their family members AND didn't do anything to protect 
their assets would be asked this question and therefore able to answer 
'didn't have the means' to the question. Therefore, N for NoMeans in 
this table is lower than in the previous table because it takes into 
account that the NaNs should not be changed to 0s because people who 
had NaNs either did something to protect the lives of their family 
members or to protect their assets

Next steps: 
-recreate bar graph figure
*************** END


#+name: conflict
#+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
data_midline  = "../../TUP-data/Midline/TUP_midline.dta"
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
import statsmodels.api as sm
import pandas as pd
import numpy as np
from cfe.df_utils import df_to_orgtbl

df_m = pd.read_stata(data_midline)
df_a = pd.read_csv(data_assignment)

# change 'Cash' to 'Control' because the cash transfers weren't given out until immediately after midline survey
df_a.replace(to_replace="Cash", value="Control", inplace=True)

df = pd.merge(df_m, df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")

df.set_index(['idno', 'Group'], inplace=True)

# add relevant variables which were not renamed to include 'conflict'
named_without_conflict = df.filter(regex="^s22_.")

# make a dataframe of variables with 'conflict' in them and join it with the other relevant variables
df = df.filter(like='conflict')
df = df.join(named_without_conflict)

# Affected and Worried are the binary variables of whether they responded yes (1) or no (0) to being affected and worried
df.rename(columns={"conflict_affected_m": "Affected", "conflict_worried_m": "Worried"}, inplace = True)

# NoMeans variable is whether they answered 'didn't have the means' to question 'why did you do nothing to protect yourself and assets?'
# NaNs should not be counted as 0s because some people were not asked this question, so the people who were asked this question would be a smaller subset 
df.replace({"Didn't have the means": 1, 
 "Not necessary/Didn't fee in danger": 0, 
 "Didn't have the suport from othes": 0, 
 "Felt in danger but decided to rist it": 0}, inplace=True)

df.rename(columns={'conflict_whynotprotect_m': 'NoMeans'}, inplace = True)

# no invest is only if directly affected by could not invest in business
affected_business = ['conflict_affected1_m', 'conflict_affected2_m', 'conflict_affected3_m', 's22_4d_m', 's22_4e_m']
df_business = df[affected_business]

df_business.replace({"Could not plant crop or invest in business": 1, 
 "Death in Familuy": 0, 
 "Needed to elocate or migrate": 0, 
 "Loss of assets": 0, 
 "HH memebr went to fight": 0,
 "Business was harmed": 0}, inplace=True)

df_business.insert(len(df_business.columns), 'NoInvest', df_business[affected_business].sum(axis=1))

df = df.join(df_business['NoInvest'])
del df_business

# migrated is only if directly affected by migration or migrated to protect lives 
affected_migration = ['conflict_affected1_m', 'conflict_affected2_m', 'conflict_affected3_m', 's22_4d_m', 's22_4e_m', 'conflict_protectlives_m']
df_migration = df[affected_migration]

df_migration.replace({"Could not plant crop or invest in business": 0, 
 "Death in Familuy": 0, 
 "Needed to elocate or migrate": 1, 
 "Loss of assets": 0, 
 "HH memebr went to fight": 0,
 "Business was harmed": 0,
 "Migrate to stay with friend/family": 1,
 "Migrated and found new accommodation": 1,                     
 "Nothing": 0,                     
 "Looked for protection with Govt. Military": 0,
 "Looked for protection with NGO": 0}, inplace=True)

df_migration.insert(len(df_migration.columns), 'Migrated', df_migration[affected_migration].sum(axis=1))
# replace 2s with 1s because the 2s just double count migration
df_migration.replace(to_replace=2, value=1, inplace=True)

df = df.join(df_migration['Migrated'])
del df_migration

# protect lives is anything (including migration) besides nothing 
df['conflict_protectlives_m'].replace({"Nothing": 0, 
                                       "Migrate to stay with friend/family": 1, 
                                       "Migrated and found new accommodation": 1, 
                                       "Looked for protection with Govt. Military": 1,
                                       "Looked for protection with NGO": 1}, inplace = True)

df.rename(columns={'conflict_protectlives_m': 'ProtectLives'}, inplace = True)

outcomes = ["Affected", "Migrated", "NoInvest", "NoMeans", "ProtectLives", "Worried"]

df.insert(len(df.columns), 'constant', 1)

# make 'Group' index a column
df.reset_index(level=['Group'], inplace=True)

df = df.join(pd.get_dummies(df['Group']))
controls = ["constant", "TUP"]

models_ols = {}
d={}

for outcome in outcomes: 
    
    temp_df = df[controls + [outcome]].dropna()
    models_ols[outcome] = sm.OLS(temp_df[outcome], temp_df[controls]).fit()
    
    # make a dictionary with the coefficients, standard errors, F-Stats and Ns
    d[outcome] = [round(models_ols[outcome].params[0], 3), models_ols[outcome].bse[0],
                  round(models_ols[outcome].params[1], 3), models_ols[outcome].bse[1], 
                  models_ols[outcome].fvalue, models_ols[outcome].nobs]
   
    if models_ols[outcome].pvalues[0] <= 0.01: 
        d[outcome][0] = str(d[outcome][0]) + '***'
    elif models_ols[outcome].pvalues[0] <= 0.05: 
        d[outcome][0] = str(d[outcome][0]) + '**'
    elif models_ols[outcome].pvalues[0] <= 0.1:
        d[outcome][0] = str(d[outcome][0]) + '*'
        
    # second coefficient is beneath standard error so add asterisks to d[outcome][2] not d[outcome][1]
    if models_ols[outcome].pvalues[1] <= 0.01: 
        d[outcome][2] = str(d[outcome][2]) + '***'
    elif models_ols[outcome].pvalues[1] <= 0.05: 
        d[outcome][2] = str(d[outcome][2]) + '**'
    elif models_ols[outcome].pvalues[1] <= 0.1:
        d[outcome][2] = str(d[outcome][2]) + '*'
    
    del temp_df

final = pd.DataFrame(data=d)
final.rename(index = {0: "CTL mean", 1: "  ", 2: "TUP", 3: "   ", 4: "F-stat", 5: "N"}, inplace=True)
newdf1 = df_to_orgtbl(final.loc[ ["CTL mean", "  ", "TUP", "   "] ], float_fmt = '%.3f')
newdf2 = df_to_orgtbl(final.loc[ ["F-stat", "N"] ], float_fmt = '%.3f')
print("|-\n" + newdf1 + "|-\n" + newdf2 + "|-\n")

#+end_src

#+RESULTS: exposure_to_conflict

#+name: tab:conflict
#+caption: Average treatment effects by group-year on the probability of having been affected in a significant way by the outbreak of violence in late 2013
#+attr_latex: :environment longtable :align lrrrrrr
|----------+-----------+----------+----------+----------+--------------+----------|
|          |  Affected | Migrated | NoInvest |  NoMeans | ProtectLives |  Worried |
|----------+-----------+----------+----------+----------+--------------+----------|
| CTL mean |  0.532*** | 0.373*** | 0.177*** | 0.538*** |     0.311*** | 0.928*** |
|          |     0.026 |    0.025 |    0.018 |    0.031 |        0.025 |    0.014 |
| TUP      | -0.127*** |   -0.005 | -0.076** | -0.109** |        0.040 |   -0.016 |
|          |     0.042 |    0.041 |    0.030 |    0.052 |        0.040 |    0.023 |
|----------+-----------+----------+----------+----------+--------------+----------|
|          |  Affected | Migrated | NoInvest |  NoMeans | ProtectLives |  Worried |
|----------+-----------+----------+----------+----------+--------------+----------|
| F-stat   |     9.203 |    0.013 |    6.609 |    4.381 |        1.003 |    0.486 |
| N        |   601.000 |  606.000 |  606.000 |  402.000 |      585.000 |  603.000 |
|----------+-----------+----------+----------+----------+--------------+----------|

* Concluding Remarks                                               :noexport:
  
BRAC's South Sudan pilot of the TUP program represents the only such test of the
ultra-poor graduation framework conducted in an area of significant political and
economic instability. It also represents one of the only direct comparisons of this
model to a similarly expensive unconditional cash transfer, arguably the most
sensible benchmark for success.  Though our study does not generalize to
contexts with high-functioning cash economies and relative political
stability, within its context it provides some clues about the
benefits of both TUP and UCT programs.

Cash transfers appear to increase consumption and possibly shift
investment from agriculture to non-farm activities, without a related
increase in wealth or income.  Conversely, the TUP program increased
wealth and directly shifted work from agriculture to livestock, with
increased consumption in the short run. We also find that having
received asset transfers dampened the negative investment effects
following the outbreak of violence.  It is impossible to say whether a
cash transfer would have had a similar mitigating effect.  We
tentatively conclude that targeted asset transfers can play a
constructive role in helping poor, self-employed households when they
face economic uncertainty. And while cash increases household
consumption, the goal of improving income or wealth is well served by
the additional services that the ultra-poor graduation framework
offer.
  
* References
   \renewcommand{\refname}{}
   \printbibliography


#+latex: \appendix

* Balance & Selection
** Balance on Observables
*** Discussion                                                       :ignore:
We start by checking whether either treatment arm appears significantly different
from the control group in terms of average baseline observable characteristics.
Table [[tab:balance_check]] presents summary statistics by group on a range of factors
related to consumption, asset holdings, and household
characteristics.  Of all of these baseline characteristics, the only
cases in which we can reject the null of equality with the control
group are for the value of motorcycles and textiles.  The mean value
depends on both /whether/ a household possesses these assets, and
conditional on possession, the value of the asset.  Particularly for
the case of motorcycles one wonders whether a small difference in
rates of ownership might account for the difference, rather than the
value of the asset conditional on ownership.

*** Creation of master random assignment file                        :ignore:

#+name: master_random_assignment
#+begin_src python
"""For making the master treatment groups .csv file

 This code creates a file which contains all of the assignments to
 treatment groups, regardless of whether a respondent was found at
 one or all of baseline, midline, and endline. The file reconciles
 differences between asset_assign.csv and locations.csv. We consider
 asset_assign.csv to be the assignment at baseline, and locations.csv
 to be the assignment that was created at endline when some
 respondents who were not found at baseline were still surveyed at
 endline.

 There are 6 households that were interviewed only at endline and
 were not included in asset_assign.csv or locations.csv. These
 households are manually added as Control households in 
 master_assignment.csv, like the other households for which a 
 group was not assigned at baseline and therefore are labeled as 
 Control at endline.
"""

data_locations = "../../TUP-data/csv/locations.csv"
data_asset_assign = "../../TUP-data/csv/asset_assign.csv"
import pandas as pd

df_l = pd.read_csv(data_locations) # assignments at endline
df_a = pd.read_csv(data_asset_assign) # assignments at baseline

# take the union of locations.csv and asset_assign.csv, rename groups accordingly
df = pd.merge(df_l, df_a[ ['respid', 'group' ] ], how='outer', left_on='RespID', right_on='respid')
df.replace(to_replace='Gift', value='Control', inplace=True)
df.replace(to_replace='TUP-high asset', value='TUP', inplace=True)
df.replace(to_replace='Cash', value='UCT', inplace=True)

# asset_assign.csv assignments are listed under 'group'
# locations.csv assignments are listed under 'Group'

for i in range(len(df)): 

    # if a group is not assigned at baseline (in asset_assign 'group'), it should be 'Control' at endline (in locations 'Group') (takes care of observation idno 1157)
    if pd.isnull(df.iloc[i, df.columns.get_loc('group')]):
        df.iloc[i, df.columns.get_loc('Group')]='Control'
    
    # if a group is assigned at baseline (in asset_assign 'group'), it should be the same at endline (in locations 'Group') (takes care of observation idno 2131)
    if pd.notna(df.iloc[i, df.columns.get_loc('group')]) and (df.iloc[i, df.columns.get_loc('group')] != df.iloc[i, df.columns.get_loc('Group')]):
        df.iloc[i, df.columns.get_loc('Group')] = df.iloc[i, df.columns.get_loc('group')]

# now all the assignments in 'Group' should be consistent with 'group' so 'group' is a subset of 'Group' and we can drop it
df = df.drop(columns=['group', 'respid'])

# manually add the 6 households which were only interviewed at endline as 'Control'
df = df.append({'RespID': 1359,  'Group': 'Control'}, ignore_index=True)
df = df.append({'RespID': 1484,  'Group': 'Control'}, ignore_index=True)
df = df.append({'RespID': 1553,  'Group': 'Control'}, ignore_index=True)
df = df.append({'RespID': 1960,  'Group': 'Control'}, ignore_index=True)
df = df.append({'RespID': 2142,  'Group': 'Control'}, ignore_index=True)
df = df.append({'RespID': 2174,  'Group': 'Control'}, ignore_index=True)

df.to_csv('master_assignment.csv')
#+end_src

#+RESULTS: master_random_assignment


*** Recreating Elliott's balance table (without some variables which are sums of other variables) :ignore:

# N represents the number of nonzero values

#+name: mean_balance
#+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
# <<load_data>> can use this!!
data_baseline  = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
import pandas as pd
from scipy import stats
from cfe.df_utils import df_to_orgtbl
import matplotlib. pyplot as plt

df_b = pd.read_stata(data_baseline)
df_a = pd.read_csv(data_assignment)

df = pd.merge(df_b, df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")

df_3_days = df[ ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b'] ].transform(lambda x: x/3)

df_month = df[ ['c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b'] ].transform(lambda x: x/30)

df_year = df[ ['c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b'] ].transform(lambda x: x/365)

df_assets = df.get(['asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b',
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b',
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b',
             'asset_val_netITN_b',
             'asset_n_house_b', 'in_business_b', 'child_total_b', 'hh_size_b',
             'Group'])

df_1 = df_3_days.join(df_month.join(df_year.join(df_assets)))

means = df_1.groupby(['Group']).mean()
means = means.T

means.insert(len(means.columns), '$\Delta$ TUP', means['TUP'] - means['Control'])
means.insert(len(means.columns), '$\Delta$ UCT', means['UCT'] - means['Control'])

d = {'Control': means['Control'], '$\Delta$ TUP': means['TUP'] - means['Control'], '$\Delta$ UCT': means['UCT'] - means['Control']}
means = pd.DataFrame(data=d)

# so we can add * later
means = means.round(decimals = 3)
means = means.astype(str)
means.insert(len(means.columns), 'N', 0)

column_list = ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b', \
              'c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b', \
               'c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b', \
               'asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b', \
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b', \
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b', \
             'asset_val_netITN_b', \
             'asset_n_house_b', 'in_business_b', 'child_total_b', 'hh_size_b']

# some are off by 0.001
for column in column_list:
    
    pvalue_tup = stats.ttest_ind(df_1[df_1['Group'] == 'TUP'][column], df_1[df_1['Group'] == 'Control'][column], nan_policy='omit').pvalue
    
    if pvalue_tup <= 0.01: 
        means.at[column,'$\Delta$ TUP'] = means.at[column,'$\Delta$ TUP'] + '***'
    elif pvalue_tup <= 0.05: 
        means.at[column,'$\Delta$ TUP'] = means.at[column,'$\Delta$ TUP'] + '**'
    elif pvalue_tup <= 0.1: 
        means.at[column,'$\Delta$ TUP'] = means.at[column,'$\Delta$ TUP'] + '*'
        
    pvalue_cash = stats.ttest_ind(df_1[df_1['Group'] == 'UCT'][column], df_1[df_1['Group'] == 'Control'][column], nan_policy='omit').pvalue
    
    if pvalue_cash <= 0.01: 
        means.at[column,'$\Delta$ UCT'] = means.at[column,'$\Delta$ UCT'] + '***'
    elif pvalue_cash <= 0.05: 
        means.at[column,'$\Delta$ UCT'] = means.at[column,'$\Delta$ UCT'] + '**'
    elif pvalue_cash <= 0.1: 
        means.at[column,'$\Delta$ UCT'] = means.at[column,'$\Delta$ UCT'] + '*'
    
    # as it is in the table, N is number of nonzero entries
    means.at[column, 'N'] = df_1[df_1[column] > 0][column].count()

# label the rows using "Better Var Name" in TUP-data/ssudan_survey.org,
# but without (last three days) or (last month) since variables are transformed (i.e. divided by 365 for yearly)
means.rename({'c_meat_b': ' Meat', 'c_fish_b': 'Fish', 
            'c_cereals_b': 'Cereals', 'c_sugar_b': 'Sugar', 
            'c_egg_b': 'Egg', 'c_oil_b': 'Oil', 
            'c_beans_b': 'Beans', 'c_fruit_b': 'Fruit', 
            'c_salt_b': 'Salt', 'c_vegetables_b': 'Vegetables', 
            'c_milk_b': 'Milk', 'c_spices_b': 'Spices', 
            'c_alcohol_b': 'Alcohol', 'c_otherfood_b': 'Other food', 
            'c_fuel_b': 'Cooking fuel', 'c_soap_b': 'Toiletries', 
            'c_transport_b': 'Transportation', 'c_cosmetics_b': 'Cosmetics', 
            'c_entertainment_b': 'Entertainment', 'c_clothesfootwear_b': 'Clothing, footwear', 
            'c_charities_b': 'Charities', 'c_ceremonies_b': 'Rituals/ceremonies', 
            'c_textiles_b': 'Textiles', 'c_utensils_b': 'Utensils',
            'c_dowry_b': 'Dowry', 'c_furniture_b': 'Furniture', 
            'c_other_b': 'Other non-food', 'asset_val_cows_b': 'Large livestock (cows)', 
            'asset_val_smallanimals_b': 'Small livestock', 'asset_val_bicycle_b': 'Bicycle', 
            'asset_val_radio_b': 'Radio', 'asset_val_motorcycle_b': 'Motorcycle', 
            'asset_val_net_b': 'Mosquito net', 'asset_val_poultry_b': 'Poultry', 
            'asset_val_bed_b': 'Bed', 'asset_val_chairtables_b': 'Chair/table', 
            'asset_val_mobile_b': 'Mobile phone', 'asset_val_netITN_b': 'Mosquito net ITN', 
            'asset_n_house_b': '# Houses', 'in_business_b': 'In business', 
            'child_total_b': '# Child', 'hh_size_b':'Household size'}, inplace=True)

newdf = df_to_orgtbl(means, float_fmt = '%.3f')
print(newdf)

df_1.boxplot(column = 'c_textiles_b', by = 'Group')
plt.savefig("textiles_Elliott.png")

df_1.boxplot(column = 'asset_val_motorcycle_b', by = 'Group')
plt.savefig("motorcycle_Elliott.png")

#+end_src


#+name: tab:balance_check
#+caption: Means of some analysis variables at baseline.  Asterisks indicate p<.1, .05, and .01 respectively
#+attr_latex: :environment longtable :align lrrrr
| Baseline Characteristics | Control | $\Delta$ TUP | $\Delta$ UCT | $N$ |
|--------------------------+---------+--------------+--------------+-----|
| Household size           |   7.232 |       -0.175 |          0.3 | 648 |
| # Child                  |   3.263 |        0.118 |        0.108 | 594 |
| In business              |   0.398 |        0.038 |        0.017 | 265 |
|--------------------------+---------+--------------+--------------+-----|
| Baseline Assets          |         |              |              |     |
|--------------------------+---------+--------------+--------------+-----|
| Furniture                |   0.196 |       -0.014 |        0.044 | 368 |
| Large livestock (cows)   |  253.31 |     -140.605 |      -99.681 |  35 |
| Small livestock          | 236.601 |      -86.069 |     -123.134 | 123 |
| Bicycle                  | 109.075 |      -12.554 |      -11.413 | 171 |
| Radio                    |  58.448 |       -5.969 |      -16.529 | 260 |
| Motorcycle               | 341.737 |      192.956 |    353.836** |  93 |
| Mosquito net             |  19.164 |        0.668 |        0.248 | 423 |
| Poultry                  |  42.402 |       -3.365 |       -8.894 | 161 |
| Bed                      |  241.27 |        7.992 |       32.762 | 521 |
| Chair/table              | 206.786 |      -29.368 |        3.617 | 531 |
| Mobile phone             |  97.537 |       12.627 |       -4.199 | 414 |
| Mosquito net ITN         |   7.822 |        1.215 |        1.178 | 181 |
| # Houses                 |   2.829 |         0.03 |        0.118 | 543 |
|--------------------------+---------+--------------+--------------+-----|
| Baseline Consumption     |         |              |              |     |
|--------------------------+---------+--------------+--------------+-----|
| Alcohol                  |   0.043 |        0.006 |       -0.029 |  18 |
| Beans                    |   0.696 |        0.231 |        0.226 | 192 |
| Rituals/ceremonies       |   0.132 |        0.007 |        0.026 | 152 |
| Charities                |    0.03 |       -0.006 |         -0.0 | 134 |
| Clothing, footwear       |   0.658 |       -0.026 |        0.033 | 595 |
| Cosmetics                |   0.682 |        0.027 |       -0.125 | 468 |
| Dowry                    |   1.256 |        -0.04 |        0.028 | 126 |
| Egg                      |   1.096 |       -0.091 |        0.038 | 276 |
| Entertainment            |   0.087 |       -0.024 |        -0.02 | 145 |
| Fish                     |   2.505 |       -0.154 |       -0.156 | 474 |
| Fruit                    |    0.69 |       -0.089 |          0.0 | 272 |
| Cooking fuel             |   0.762 |       -0.039 |       -0.072 | 456 |
| Meat                     |   4.205 |       -0.568 |       -0.052 | 378 |
| Milk                     |   1.284 |       -0.237 |       -0.232 | 114 |
| Oil                      |   1.364 |       -0.131 |       -0.141 | 613 |
| Salt                     |   0.447 |       -0.026 |        0.007 | 617 |
| Toiletries               |   0.483 |       -0.009 |       -0.026 | 536 |
| Spices                   |    0.23 |        0.024 |       -0.045 | 158 |
| Sugar                    |   1.713 |       -0.078 |       -0.189 | 604 |
| Textiles                 |   0.154 |       -0.005 |       0.055* | 376 |
| Transportation           |   0.176 |       -0.033 |        0.002 | 193 |
| Utensils                 |   0.246 |       -0.008 |        0.008 | 442 |
| Vegetables               |   1.543 |       -0.165 |        -0.18 | 471 |
| Cereals                  |   9.187 |       -0.947 |         0.27 | 605 |

*** Creating a new balance table with only nonzero values counting toward the mean :ignore:

To explore this, Table [[tab:mean_log_balance]] reports the means of log
expenditures, with expenditures of zero treated as missing data; thus
these figures are conditional on ownership.  We see from this table
that there are no significant differences between the control and
treatment groups; thus, rejections in the previous table can be
attributed to differences in rates of ownership.

# N represents the number of nonzero values, and only nonzero values
# are considered in calculations of the means

#+name: mean_log_balance
#+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
data_baseline  = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
import pandas as pd
from scipy import stats
from cfe.df_utils import df_to_orgtbl
import matplotlib. pyplot as plt
import numpy as np

df_b = pd.read_stata(data_baseline)
df_a = pd.read_csv(data_assignment)

df = pd.merge(df_b, df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")


df_3_days = df[ ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b'] ]

df_month = df[ ['c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b'] ]

df_year = df[ ['c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b'] ]

df_assets = df.get(['asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b',
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b',
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b',
             'asset_val_netITN_b',
             'asset_n_house_b', 'child_total_b', 'hh_size_b'])

df_1 = df_3_days.join(df_month.join(df_year.join(df_assets)))

df_1.replace(to_replace = 0, value = float("NaN") , inplace = True)

# take the logs of expenditure for all variables except dummy variables
df_1 = df_1.transform(lambda x: np.log(x))

# add in group assignment and the dummy variables that need 0 as one of their values
df_1 = df_1.join(df[ [ 'in_business_b', 'Group'] ])

means = df_1.groupby(['Group']).mean()
means = means.T

means.insert(len(means.columns), '$\Delta$ TUP', means['TUP'] - means['Control'])
means.insert(len(means.columns), '$\Delta$ UCT', means['UCT'] - means['Control'])

d = {'Control': means['Control'], '$\Delta$ TUP': means['TUP'] - means['Control'], '$\Delta$ UCT': means['UCT'] - means['Control']}
means = pd.DataFrame(data=d)

# so we can add * later
means = means.round(decimals = 3)
means = means.astype(str)
means.insert(len(means.columns), 'N', 0)

column_list = ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b', \
              'c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b', \
               'c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b', \
               'asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b', \
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b', \
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b', \
             'asset_val_netITN_b', \
             'asset_n_house_b', 'in_business_b', 'child_total_b', 'hh_size_b']

# some are off by 0.001
for column in column_list:
    
    pvalue_tup = stats.ttest_ind(df_1[df_1['Group'] == 'TUP'][column], df_1[df_1['Group'] == 'Control'][column], nan_policy='omit').pvalue
    
    if pvalue_tup <= 0.01: 
        means.at[column,'$\Delta$ TUP'] = means.at[column,'$\Delta$ TUP'] + '***'
    elif pvalue_tup <= 0.05: 
        means.at[column,'$\Delta$ TUP'] = means.at[column,'$\Delta$ TUP'] + '**'
    elif pvalue_tup <= 0.1: 
        means.at[column,'$\Delta$ TUP'] = means.at[column,'$\Delta$ TUP'] + '*'
        
    pvalue_cash = stats.ttest_ind(df_1[df_1['Group'] == 'UCT'][column], df_1[df_1['Group'] == 'Control'][column], nan_policy='omit').pvalue
    
    if pvalue_cash <= 0.01: 
        means.at[column,'$\Delta$ UCT'] = means.at[column,'$\Delta$ UCT'] + '***'
    elif pvalue_cash <= 0.05: 
        means.at[column,'$\Delta$ UCT'] = means.at[column,'$\Delta$ UCT'] + '**'
    elif pvalue_cash <= 0.1: 
        means.at[column,'$\Delta$ UCT'] = means.at[column,'$\Delta$ UCT'] + '*'
    
   # N is number of nonzero entries
    means.at[column, 'N'] = df_1[column].count()

# label the rows using "Better Var Name" in TUP-data/ssudan_survey.org
means.rename({'c_meat_b': ' Meat (last three days)', 'c_fish_b': 'Fish (last three days)', 
            'c_cereals_b': 'Cereals (last three days)', 'c_sugar_b': 'Sugar (last three days)', 
            'c_egg_b': 'Egg (last three days)', 'c_oil_b': 'Oil (last three days)', 
            'c_beans_b': 'Beans (last three days)', 'c_fruit_b': 'Fruit (last three days)', 
            'c_salt_b': 'Salt (last three days)', 'c_vegetables_b': 'Vegetables (last three days)', 
            'c_milk_b': 'Milk (last three days)', 'c_spices_b': 'Spices (last three days)', 
            'c_alcohol_b': 'Alcohol (last three days)', 'c_otherfood_b': 'Other food (last three days)', 
            'c_fuel_b': 'Cooking fuel (last month)', 'c_soap_b': 'Toiletries (last month)', 
            'c_transport_b': 'Transportation (last month)', 'c_cosmetics_b': 'Cosmetics (last month)', 
            'c_entertainment_b': 'Entertainment (last month)', 'c_clothesfootwear_b': 'Clothing, footwear (last year)', 
            'c_charities_b': 'Charities (last year)', 'c_ceremonies_b': 'Rituals/ceremonies (last year)', 
            'c_textiles_b': 'Textiles (last year)', 'c_utensils_b': 'Utensils (last year)',
            'c_dowry_b': 'Dowry (last year)', 'c_furniture_b': 'Furniture (last year)', 
            'c_other_b': 'Other non-food (last year)', 'asset_val_cows_b': 'Large livestock (cows)', 
            'asset_val_smallanimals_b': 'Small livestock', 'asset_val_bicycle_b': 'Bicycle', 
            'asset_val_radio_b': 'Radio', 'asset_val_motorcycle_b': 'Motorcycle', 
            'asset_val_net_b': 'Mosquito net', 'asset_val_poultry_b': 'Poultry', 
            'asset_val_bed_b': 'Bed', 'asset_val_chairtables_b': 'Chair/table', 
            'asset_val_mobile_b': 'Mobile phone', 'asset_val_netITN_b': 'Mosquito net ITN', 
            'asset_n_house_b': '# Houses', 'in_business_b': 'In business', 
            'child_total_b': '# Child', 'hh_size_b':'Household size'}, inplace=True)

newdf = df_to_orgtbl(means, float_fmt = '%.3f')
print(newdf)

# box and whisker plots of log positive observations
df_1['c_textiles_b'] = df_1['c_textiles_b']
df_1.boxplot(column = 'c_textiles_b', by = 'Group')
plt.savefig("textiles_nonzero.png")

df_1['asset_val_mobile_b'] = df_1['asset_val_mobile_b']
df_1.boxplot(column = 'asset_val_mobile_b', by = 'Group')
plt.savefig("mobile_nonzero.png")

#+end_src

#+name: tab:mean_log_balance
#+caption: Means of logs of some analysis variables at baseline.  Asterisks indicate p<.1, .05, and .01 respectively.  Zeros are treated as missing.
#+attr_latex: :environment longtable :align lrrrr
| Variable                       | Control | $\Delta$ TUP | $\Delta$ UCT | $N$ |
|--------------------------------+---------+--------------+--------------+-----|
| Meat (last three days)         |   2.845 |        0.018 |        0.033 | 378 |
| Fish (last three days)         |   2.058 |        0.041 |       -0.022 | 474 |
| Cereals (last three days)      |   3.033 |       -0.071 |       -0.019 | 605 |
| Sugar (last three days)        |   1.346 |        -0.06 |       -0.031 | 604 |
| Egg (last three days)          |   1.811 |        0.011 |       -0.077 | 276 |
| Oil (last three days)          |   0.897 |        0.014 |       -0.035 | 613 |
| Beans (last three days)        |   1.669 |        0.135 |        0.127 | 192 |
| Fruit (last three days)        |   1.329 |        0.007 |        0.027 | 272 |
| Salt (last three days)         |   0.201 |       -0.054 |        0.001 | 617 |
| Vegetables (last three days)   |   1.532 |       -0.026 |       -0.047 | 471 |
| Milk (last three days)         |   2.514 |       -0.015 |       -0.217 | 114 |
| Spices (last three days)       |   0.858 |       -0.069 |        -0.21 | 158 |
| Alcohol (last three days)      |   1.304 |        0.104 |        0.305 |  18 |
| Cooking fuel (last month)      |   2.941 |        -0.01 |       -0.127 | 456 |
| Toiletries (last month)        |   2.328 |       -0.051 |       -0.172 | 536 |
| Transportation (last month)    |   2.395 |       -0.003 |        0.127 | 193 |
| Cosmetics (last month)         |   2.659 |        0.084 |       -0.054 | 468 |
| Entertainment (last month)     |   2.088 |        -0.13 |       -0.064 | 145 |
| Clothing, footwear (last year) |   4.989 |       -0.005 |       -0.074 | 595 |
| Charities (last year)          |   3.348 |       -0.229 |       -0.121 | 134 |
| Rituals/ceremonies (last year) |   4.422 |        0.261 |        0.162 | 152 |
| Textiles (last year)           |   4.096 |        0.052 |        0.112 | 376 |
| Utensils (last year)           |   4.352 |        -0.01 |       -0.003 | 442 |
| Dowry (last year)              |   5.934 |        0.433 |        0.178 | 126 |
| Furniture (last year)          |   4.224 |        0.033 |        0.185 | 368 |
| Large livestock (cows)         |   7.461 |       -0.072 |       -0.068 |  35 |
| Small livestock                |   6.425 |       -0.195 |       -0.364 | 123 |
| Bicycle                        |   5.601 |        0.051 |         0.14 | 171 |
| Radio                          |     4.3 |        0.067 |       -0.071 | 260 |
| Motorcycle                     |   6.762 |        0.676 |        0.778 |  93 |
| Mosquito net                   |   3.089 |        0.024 |       -0.008 | 423 |
| Poultry                        |   4.616 |        0.002 |       -0.177 | 161 |
| Bed                            |   5.306 |       -0.001 |        0.116 | 521 |
| Chair/table                    |   4.889 |        0.021 |        0.025 | 531 |
| Mobile phone                   |   4.811 |        0.119 |        0.031 | 414 |
| Mosquito net ITN               |   3.147 |       -0.047 |         0.13 | 181 |
| # Houses                       |   1.064 |       -0.027 |         -0.0 | 543 |
| # Child                        |   1.118 |        0.043 |        0.039 | 594 |
| Household size                 |   1.917 |       -0.021 |         0.05 | 648 |
| In business                    |   0.398 |        0.038 |        0.017 | 638 |

*** Box and whisker plots for the new balance table                  :ignore:noexport:

#+Caption: Box and whisker plot of log(positive textiles) for nonzero values
#+NAME: fig:box_whisker_textiles_nonzero
[[../documents/textiles_nonzero.png]]

#+Caption: Box and whisker plot of log(positive mobile) for nonzero values
#+NAME: fig:box_whisker_mobile_nonzero
[[../documents/mobile_nonzero.png]]


** Attrition & Selection
*** Discussion of attrition                                          :ignore:
Our census found 755 eligible households, and made efforts to
interview a respondent from every one of these households.  The degree
to which we succeeded is documented in \Tab{attrition_count}.
In the baseline survey we successfully interviewed
649 of these households; in the midline 606, and in the endline 700.

*** Recreating the attrition tables                         :ignore:
*************** NOTE
The results are slightly different (in the 2015 column) than in the original attrition
tables due to the creation of master_assignment.csv, which includes
everyone who was assigned to any group no matter whether they were
found at baseline, midline, or endline. It added 6 households which
were found only at endline and not accounted for in the original
attrition tables.
*************** END


# using master_assignment.csv
#+name: master_assignment_attrition 
#+begin_src python :exports none  :results output table raw labels=[] :colnames no :tangle master_assignment_attrition.py
data_baseline  = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
data_midline = "../../TUP-data/Midline/TUP_midline.dta"
data_endline = "../../TUP-data/Endline/TUP_endline.dta"
import pandas as pd
import numpy as np
from cfe.df_utils import df_to_orgtbl

df_b = pd.read_stata(data_baseline) 
df_a = pd.read_csv(data_assignment)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)


print("""#+caption: Total number of households in sample by group and round.""")
# Initial assignment (from Census)
dassign = df_a.Group.value_counts(ascending=True)
# change "Cash" to "UCT"
dassign.rename(index={'Cash':'UCT'}, inplace=True)
dassign['All'] = dassign.sum()

# baseline (2013)
# merge baseline data id number and group column from master assignment csv based on respondent id/id number
df = pd.merge(df_b[ ['idno'] ], df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")

# change "Gift" to "Control"
df.replace(to_replace='Gift', value='Control', inplace=True)

# change "Cash" to "UCT"
df.replace(to_replace='Cash', value='UCT', inplace=True)

# count the number in each treatment group
bygroupdf = df.groupby(['Group']).count()
d2013 = {'UCT': bygroupdf.at['UCT','idno'],'Control': bygroupdf.at['Control','idno'],'TUP': bygroupdf.at['TUP','idno'],'All': df.Group.notna().sum()}

df_b = df

# midline (2014)
# merge midline data id number and group column from random assignment data based on respondent id/id number
df = pd.merge(df_m[ ['idno'] ], df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")

# change "Gift" to "Control"
df.replace(to_replace='Gift', value='Control', inplace=True)

# change "Cash" to "UCT"
df.replace(to_replace='Cash', value='UCT', inplace=True)

# count the number in each treatment group
bygroupdf = df.groupby(['Group']).count()
d2014 = {'UCT': bygroupdf.at['UCT','idno'],'Control': bygroupdf.at['Control','idno'],'TUP': bygroupdf.at['TUP','idno'],'All': df.Group.notna().sum()}

df_m = df

# endline (2015)
# merge endline data id number and group column from random assignment data based on respondent id/id number
df = pd.merge(df_e[ ['idno'] ], df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")

# change "Gift" to "Control"
df.replace(to_replace='Gift', value='Control', inplace=True)

# change "Cash" to "UCT"
df.replace(to_replace='Cash', value='UCT', inplace=True)

# count the number in each treatment group
bygroupdf = df.groupby(['Group']).count()
d2015 = {'UCT': bygroupdf.at['UCT','idno'],'Control': bygroupdf.at['Control','idno'],'TUP': bygroupdf.at['TUP','idno'],'All': df.Group.notna().sum()}

df_e = df

# make the table for total number of households in sample by group and round
d = {'Assigned': dassign, '2013': d2013, '2014': d2014, '2015': d2015}
table = pd.DataFrame(data = d) 

newdf = df_to_orgtbl(table, float_fmt = '%d')
print(newdf)


print("""#+caption: Number of households in sample with baseline survey by group and round""")


# find the intersection of people who had surveys at both baseline and midline
df_b_m = pd.merge(df_b[ ['idno'] ], df_m[ ['idno', 'Group'] ], how='inner', left_on="idno", right_on="idno")
bygroupdf_b_m = df_b_m.groupby(['Group']).count()

# find the intersection of people who had surveys at both baseline and endline
df_b_e = pd.merge(df_b[ ['idno'] ], df_e[ ['idno', 'Group'] ], how='inner', left_on="idno", right_on="idno")
bygroupdf_b_e = df_b_e.groupby(['Group']).count()

# count the number in each treatment group
dd2013 = d2013
dd2014 = {'UCT': bygroupdf_b_m.at['UCT','idno'],'Control': bygroupdf_b_m.at['Control','idno'],'TUP': bygroupdf_b_m.at['TUP','idno'],'All': df_b_m.Group.notna().sum()}
dd2015 = {'UCT': bygroupdf_b_e.at['UCT','idno'],'Control': bygroupdf_b_e.at['Control','idno'],'TUP': bygroupdf_b_e.at['TUP','idno'],'All': df_b_e.Group.notna().sum()}

# make the table for number of households in sample with baseline survey by group and round
dd = {'Assigned': dassign, '2013': dd2013, '2014': dd2014, '2015': dd2015}
table2 = pd.DataFrame(data = dd) 

newdf2 = df_to_orgtbl(table2, float_fmt = '%d')
print(newdf2)

#+end_src


#+name: tab:attrition_count
#+caption: Numbers of surveyed households, by group and round.  Parenthetical numbers are households surveyed conditional on also being surveyed at baseline.
|         | Assigned | 2013 | 2014      | 2015      |
|---------+----------+------+-----------+-----------|
| All     |      755 |  649 | 606 (554) | 700 (603) |
| Control |      380 |  281 | 265 (219) | 353 (262) |
| TUP     |      250 |  244 | 228 (223) | 236 (231) |
| UCT     |      125 |  124 | 113 (112) | 111 (110) |

*** Interview Probability Table                                      :ignore:
#+name: interview_probabilities
#+begin_src python :exports none  :results output table raw :dir ../ :colnames no :tangle selection_regression.py
import pandas as pd
import numpy as np
from cfe.df_utils import df_to_orgtbl
from cfe.estimation import ols

data_baseline  = "../TUP-data/data/Baseline/TUP_baseline.dta"
data_assignment  = "../TUP-report/documents/master_assignment.csv"
data_midline = "../TUP-data/Midline/TUP_midline.dta"
data_endline = "../TUP-data/Endline/TUP_endline.dta"

df_b = pd.read_stata(data_baseline) 
df_a = pd.read_csv(data_assignment)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)

DF = {}
# baseline (2013)
# merge baseline data id number and group column from master assignment csv based on respondent id/id number
df = pd.merge(df_b[ ['idno'] ], df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")

# change "Gift" to "Control"
df.replace(to_replace='Gift', value='Control', inplace=True)

# change "Cash" to "UCT"
df.replace(to_replace='Cash', value='UCT', inplace=True)

df.set_index('idno', inplace=True)

DF[2013] = df['Group']

# midline (2014)
# merge midline data id number and group column from random assignment data based on respondent id/id number
df = pd.merge(df_m[ ['idno'] ], df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")

# change "Gift" to "Control"
df.replace(to_replace='Gift', value='Control', inplace=True)

# change "Cash" to "UCT"
df.replace(to_replace='Cash', value='UCT', inplace=True)

df.set_index('idno', inplace=True)

DF[2014] = df['Group']

# endline (2015)
# merge endline data id number and group column from random assignment data based on respondent id/id number
df = pd.merge(df_e[ ['idno'] ], df_a[ ['RespID', 'Group'] ], how='inner', left_on="idno", right_on="RespID")

# change "Gift" to "Control"
df.replace(to_replace='Gift', value='Control', inplace=True)

# change "Cash" to "UCT"
df.replace(to_replace='Cash', value='UCT', inplace=True)

df.set_index('idno', inplace=True)

DF[2015] = df['Group']

df = (~pd.DataFrame(DF).isnull() + 0)
df.columns.name = 'Round'

df = df.stack()
df.name = 'Interviewed'

group = df_a.set_index('RespID')['Group']
group.index.name = 'idno'

df = pd.DataFrame(df).join(group,how='outer')

p = df.groupby(['Round','Group']).mean().unstack('Round')
p.columns = p.columns.droplevel(0)

print(df_to_orgtbl(p,float_fmt='\(%4.2f\)'))
#+end_src


#+name: tab:interview_probabilities
#+caption: Probability of completed interview, by round and treatment group.
| Group   | 2013     | 2014     | 2015     |
|---------+----------+----------+----------|
| Control | \(0.75\) | \(0.70\) | \(0.94\) |
| TUP     | \(0.98\) | \(0.92\) | \(0.95\) |
| UCT     | \(0.99\) | \(0.90\) | \(0.89\) |

*** Interview Probability Discussion                                 :ignore:

 One important question is whether there's /differential/ selection by
 treatment group.  Table [[tab:interview_probabilities]] shows the
 proportion of /assigned/ households actually interviewed by both round
 and group.  There are three things to note about this table.  The first
 is that at baseline and midline the proportion of households
 interviewed from the control group was significantly lower than for
 the TUP and UCT groups.  The second is that challenges in the field
 led to a lower probability of a successful interview at midline.  The
 third is that members of the UCT were slightly (but not
 significantly) less likely to be interviewed at endline. 

 The relatively low proportion of control households being interviewed
 in the first two rounds can be attributed to the fact that these
 households did not have the same consistent contact with BRAC that
 the TUP & UCT groups had, and this consistent contact was important
 for locating people in a rural area lacking certain infrastructure (phones,
 addresses) which might otherwise have made finding households
 easier, while the lower proportion of households interviewed at midline
 may be attributable to an early harvest that year.

 We explore this further, examining whether a random lack of balance in observable
 household characteristics at baseline is related to subsequent
 successful interviews.  \Tab{attrition_balance} reports the average level of
 various characteristics in 2013. Then we report the difference in means between
 households that were in and out of the midline or endline surveys.
 Note that the sample here consists not of all eligible households,
 but all households interviewed at baseline.  There is one important
 relationship that emerges: larger households & households with more
 children were more likely to be interviewed at midline. 

**** Recreating attrition balance table                            :ignore:
*************** NOTES
        - Elicitation for "Cereals" disaggregated after baseline, so not comparable.
*************** END

 #+name: attrition_balance
 #+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
data_baseline  = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_midline  = "/Users/beckycardinali/Desktop/TUP-data/Midline/TUP_midline.dta"
data_endline = "/Users/beckycardinali/Desktop/TUP-data/Endline/TUP_endline.dta"
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
import pandas as pd
from scipy import stats
from cfe.df_utils import df_to_orgtbl

df_b = pd.read_stata(data_baseline)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)
df_a = pd.read_csv(data_assignment)

df_m.insert(len(df_m.columns), 'in_midline', 1)
df_e.insert(len(df_e.columns), 'in_endline', 1)

df = pd.merge(df_b, df_a[ ['RespID', 'Group'] ], how='outer', left_on="idno", right_on="RespID")

# merge in midline and endline
df = pd.merge(df, df_m[ ['idno', 'in_midline'] ], how='outer', left_on="idno", right_on="idno")
df = pd.merge(df, df_e[ ['idno', 'in_endline'] ], how='outer', left_on='idno', right_on='idno')

df['in_midline'].fillna(value=0, inplace=True)
df['in_endline'].fillna(value=0, inplace=True)

df_3_days = df[ ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b'] ].transform(lambda x: x/3)

df_month = df[ ['c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b'] ].transform(lambda x: x/30)

df_year = df[ ['c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b'] ].transform(lambda x: x/365)

df_assets = df.get(['asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b',
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b',
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b',
             'asset_val_netITN_b',
             'asset_n_house_b', 'in_business_b', 'child_total_b', 'hh_size_b',
             'Group', 'in_midline', 'in_endline'])

df_1 = df_3_days.join(df_month.join(df_year.join(df_assets)))

means_b = df_1.mean()
means_m = df_1.groupby(['in_midline']).mean()
means_e = df_1.groupby(['in_endline']).mean()
means_b = means_b.T
means_m = means_m.T
means_e = means_e.T

# some are off by 0.001, clothesfootwear numbers differ
d = {'mean_baseline': means_b, 'beta_mid': means_m[1.0] - means_m[0.0], 'beta_end': means_e[1.0] - means_e[0.0]}
means = pd.DataFrame(data=d)

#default axis=0 for dropping rows
means.drop(['in_midline', 'in_endline'], inplace=True)

# so we can add * later
means = means.round(decimals = 3)
means = means.astype(str)

column_list = ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b', \
              'c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b', \
               'c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b', \
              'asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b', \
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b', \
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b', \
             'asset_val_netITN_b', \
             'asset_n_house_b', 'in_business_b', 'child_total_b', 'hh_size_b']

for column in column_list:
    
    pvalue_mid = stats.ttest_ind(df_1[df_1['in_midline'] == 1][column], df_1[df_1['in_midline'] == 0][column], nan_policy='omit').pvalue
    
    if pvalue_mid <= 0.01: 
        means.at[column,'beta_mid'] = means.at[column,'beta_mid'] + '***'
    elif pvalue_mid <= 0.05: 
        means.at[column,'beta_mid'] = means.at[column,'beta_mid'] + '**'
    elif pvalue_mid <= 0.1: 
        means.at[column,'beta_mid'] = means.at[column,'beta_mid'] + '*'
        
    pvalue_end = stats.ttest_ind(df_1[df_1['in_endline'] == 1][column], df_1[df_1['in_endline'] == 0][column], nan_policy='omit').pvalue
    
    if pvalue_end <= 0.01: 
        means.at[column,'beta_end'] = means.at[column,'beta_end'] + '***'
    elif pvalue_end <= 0.05: 
        means.at[column,'beta_end'] = means.at[column,'beta_end'] + '**'
    elif pvalue_end <= 0.1: 
        means.at[column,'beta_end'] = means.at[column,'beta_end'] + '*'

# label the rows using "Better Var Name" in TUP-data/ssudan_survey.org,
# but without (last three days) or (last month) since variables are transformed (i.e. divided by 365 for yearly)
means.rename({'c_meat_b': ' Meat', 'c_fish_b': 'Fish', 
            'c_cereals_b': 'Cereals', 'c_sugar_b': 'Sugar', 
            'c_egg_b': 'Egg', 'c_oil_b': 'Oil', 
            'c_beans_b': 'Beans', 'c_fruit_b': 'Fruit', 
            'c_salt_b': 'Salt', 'c_vegetables_b': 'Vegetables', 
            'c_milk_b': 'Milk', 'c_spices_b': 'Spices', 
            'c_alcohol_b': 'Alcohol', 'c_otherfood_b': 'Other food', 
            'c_fuel_b': 'Cooking fuel', 'c_soap_b': 'Toiletries', 
            'c_transport_b': 'Transportation', 'c_cosmetics_b': 'Cosmetics', 
            'c_entertainment_b': 'Entertainment', 'c_clothesfootwear_b': 'Clothing, footwear', 
            'c_charities_b': 'Charities', 'c_ceremonies_b': 'Rituals/ceremonies', 
            'c_textiles_b': 'Textiles', 'c_utensils_b': 'Utensils',
            'c_dowry_b': 'Dowry', 'c_furniture_b': 'Furniture', 
            'c_other_b': 'Other non-food', 'asset_val_cows_b': 'Large livestock (cows)', 
            'asset_val_smallanimals_b': 'Small livestock', 'asset_val_bicycle_b': 'Bicycle', 
            'asset_val_radio_b': 'Radio', 'asset_val_motorcycle_b': 'Motorcycle', 
            'asset_val_net_b': 'Mosquito net', 'asset_val_poultry_b': 'Poultry', 
            'asset_val_bed_b': 'Bed', 'asset_val_chairtables_b': 'Chair/table', 
            'asset_val_mobile_b': 'Mobile phone', 'asset_val_netITN_b': 'Mosquito net ITN', 
            'asset_n_house_b': '# Houses', 'in_business_b': 'In business', 
            'child_total_b': '# Child', 'hh_size_b':'Household size'}, inplace=True)

newdf = df_to_orgtbl(means, float_fmt = '%.3f')
print(newdf)


 #+end_src


 #+caption: Means of household baseline characteristics and regression coefficients for whether they were ultimately found at baseline or endline. (Note that this does not consider households found only in 2014 or 2015).
 #+attr_latex: :environment longtable :align lrrrrr
 #+name: tab:attrition_balance
 |--------------------------+----------------------+----------------+----------------|
 | Baseline Characteristics | $\mbox{Mean}_{2013}$ | $\beta_{2014}$ | $\beta_{2015}$ |
 |--------------------------+----------------------+----------------+----------------|
 | Household size           |                7.224 |        0.595** |          0.428 |
 | # Children               |                3.328 |       0.656*** |          0.423 |
 | In business              |                0.415 |          0.038 |          0.007 |
 |--------------------------+----------------------+----------------+----------------|
 | Baseline Assets          |                      |                |                |
 |--------------------------+----------------------+----------------+----------------|
 | # Houses                 |                2.863 |          0.305 |          0.367 |
 | Bed                      |              250.535 |         12.649 |        -51.133 |
 | Bicycle                  |              102.174 |         11.179 |          4.212 |
 | Chair/table              |              196.436 |         -0.303 |        -37.177 |
 | Furniture                |                0.199 |         -0.023 |           0.02 |
 | Large livestock (cows)   |              181.402 |         67.862 |        -89.273 |
 | Mobile phone             |              101.482 |          6.336 |        -13.028 |
 | Motorcycle               |              481.886 |        213.002 |       -241.819 |
 | Mosquito net ITN         |                8.504 |         -1.777 |          0.449 |
 | Mosquito net             |               19.462 |          0.332 |          2.814 |
 | Poultry                  |               39.438 |        23.634* |         -2.243 |
 | Radio                    |               53.046 |         -6.333 |       -35.093* |
 | Small livestock          |              180.716 |         18.966 |        -79.014 |
 |--------------------------+----------------------+----------------+----------------|
 | Baseline Consumption     |                      |                |                |
 |--------------------------+----------------------+----------------+----------------|
 | Alcohol                  |                 0.04 |          0.005 |         -0.004 |
 | Beans                    |                0.826 |          0.269 |         -0.382 |
 | Rituals/ceremonies       |                0.139 |         -0.019 |         -0.038 |
 | Charities                |                0.027 |          0.007 |         -0.001 |
 | Clothing, footwear       |                0.655 |         0.177* |         -0.203 |
 | Cosmetics                |                0.668 |          0.005 |          0.229 |
 | Dowry                    |                1.247 |          0.745 |         -0.394 |
 | Egg                      |                1.069 |         -0.005 |          0.106 |
 | Entertainment            |                0.074 |          0.021 |         0.051* |
 | Fish                     |                2.418 |         -0.132 |          0.036 |
 | Fruit                    |                0.657 |          0.009 |         -0.151 |
 | Cooking fuel             |                0.733 |          0.105 |         -0.049 |
 | Meat                     |                3.982 |          0.254 |            0.3 |
 | Milk                     |                 1.15 |          0.283 |          0.239 |
 | Oil                      |                1.288 |          0.037 |        -0.532* |
 | Salt                     |                0.439 |       -0.14*** |         -0.043 |
 | Toiletries               |                0.475 |        -0.181* |          0.047 |
 | Spices                   |                0.231 |          0.024 |         -0.048 |
 | Sugar                    |                1.648 |         -0.285 |          -0.02 |
 | Textiles                 |                0.163 |           0.01 |          0.011 |
 | Transportation           |                0.164 |          0.004 |          0.018 |
 | Utensils                 |                0.245 |          0.061 |         -0.023 |
 | Vegetables               |                1.447 |          0.096 |         -0.151 |


**** Creating a new attrition balance table with only nonzero values counting toward the mean :noexport:

 The following table shows the means of log expenditures. 

 #+name: attrition_balance_nonzero
 #+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
data_baseline  = "../../TUP-data/data/Baseline/TUP_baseline.dta"
data_midline  = "/Users/beckycardinali/Desktop/TUP-data/Midline/TUP_midline.dta"
data_endline = "/Users/beckycardinali/Desktop/TUP-data/Endline/TUP_endline.dta"
data_assignment  = "../../TUP-report/documents/master_assignment.csv"
import pandas as pd
import numpy as np
from scipy import stats
from cfe.df_utils import df_to_orgtbl

df_b = pd.read_stata(data_baseline)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)
df_a = pd.read_csv(data_assignment)

df_m.insert(len(df_m.columns), 'in_midline', 1)
df_e.insert(len(df_e.columns), 'in_endline', 1)

df = pd.merge(df_b, df_a[ ['RespID', 'Group'] ], how='outer', left_on="idno", right_on="RespID")

# merge in midline and endline
df = pd.merge(df, df_m[ ['idno', 'in_midline'] ], how='outer', left_on="idno", right_on="idno")
df = pd.merge(df, df_e[ ['idno', 'in_endline'] ], how='outer', left_on='idno', right_on='idno')

df['in_midline'].fillna(value=0, inplace=True)
df['in_endline'].fillna(value=0, inplace=True)

df_3_days = df[ ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b'] ]

df_month = df[ ['c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b'] ]

df_year = df[ ['c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b'] ]

df_assets = df.get(['asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b',
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b',
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b',
             'asset_val_netITN_b',
             'asset_n_house_b', 'child_total_b', 'hh_size_b'])

df_1 = df_3_days.join(df_month.join(df_year.join(df_assets)))
df_1.replace(to_replace = 0, value = float("NaN") , inplace = True)

# take the logs of expenditure for all variables except dummy variables
df_1 = df_1.transform(lambda x: np.log(x))

# add in group assignment and the dummy variables that need 0 as one of their values
df_1 = df_1.join(df[ ['in_midline', 'in_endline', 'in_business_b', 'Group'] ])

means_b = df_1.mean()
means_m = df_1.groupby(['in_midline']).mean()
means_e = df_1.groupby(['in_endline']).mean()
means_b = means_b.T
means_m = means_m.T
means_e = means_e.T

d = {'mean_baseline': means_b, 'beta_mid': means_m[1.0] - means_m[0.0], 'beta_end': means_e[1.0] - means_e[0.0]}
means = pd.DataFrame(data=d)

#default axis=0 for dropping rows
means.drop(['in_midline', 'in_endline'], inplace=True)

# so we can add * later
means = means.round(decimals = 3)
means = means.astype(str)

column_list = ['c_meat_b', 'c_fish_b', 'c_cereals_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b', 'c_beans_b', \
                    'c_fruit_b', 'c_salt_b', 'c_vegetables_b', \
                    'c_milk_b', 'c_spices_b', 'c_alcohol_b', 'c_otherfood_b', \
              'c_fuel_b', 'c_soap_b', 'c_transport_b', 'c_cosmetics_b', 'c_entertainment_b', \
               'c_clothesfootwear_b', 'c_charities_b', 'c_ceremonies_b', 'c_textiles_b', 'c_utensils_b',
                  'c_dowry_b', 'c_furniture_b', 'c_other_b', \
              'asset_val_cows_b', 'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b', \
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b', \
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b', \
             'asset_val_netITN_b', \
             'asset_n_house_b', 'in_business_b', 'child_total_b', 'hh_size_b']

for column in column_list:
    
    pvalue_mid = stats.ttest_ind(df_1[df_1['in_midline'] == 1][column], df_1[df_1['in_midline'] == 0][column], nan_policy='omit').pvalue
    
    if pvalue_mid <= 0.01: 
        means.at[column,'beta_mid'] = means.at[column,'beta_mid'] + '***'
    elif pvalue_mid <= 0.05: 
        means.at[column,'beta_mid'] = means.at[column,'beta_mid'] + '**'
    elif pvalue_mid <= 0.1: 
        means.at[column,'beta_mid'] = means.at[column,'beta_mid'] + '*'
        
    pvalue_end = stats.ttest_ind(df_1[df_1['in_endline'] == 1][column], df_1[df_1['in_endline'] == 0][column], nan_policy='omit').pvalue
    
    if pvalue_end <= 0.01: 
        means.at[column,'beta_end'] = means.at[column,'beta_end'] + '***'
    elif pvalue_end <= 0.05: 
        means.at[column,'beta_end'] = means.at[column,'beta_end'] + '**'
    elif pvalue_end <= 0.1: 
        means.at[column,'beta_end'] = means.at[column,'beta_end'] + '*'

# label the rows using "Better Var Name" in TUP-data/ssudan_survey.org
means.rename({'c_meat_b': ' Meat (last three days)', 'c_fish_b': 'Fish (last three days)', 
            'c_cereals_b': 'Cereals (last three days)', 'c_sugar_b': 'Sugar (last three days)', 
            'c_egg_b': 'Egg (last three days)', 'c_oil_b': 'Oil (last three days)', 
            'c_beans_b': 'Beans (last three days)', 'c_fruit_b': 'Fruit (last three days)', 
            'c_salt_b': 'Salt (last three days)', 'c_vegetables_b': 'Vegetables (last three days)', 
            'c_milk_b': 'Milk (last three days)', 'c_spices_b': 'Spices (last three days)', 
            'c_alcohol_b': 'Alcohol (last three days)', 'c_otherfood_b': 'Other food (last three days)', 
            'c_fuel_b': 'Cooking fuel (last month)', 'c_soap_b': 'Toiletries (last month)', 
            'c_transport_b': 'Transportation (last month)', 'c_cosmetics_b': 'Cosmetics (last month)', 
            'c_entertainment_b': 'Entertainment (last month)', 'c_clothesfootwear_b': 'Clothing, footwear (last year)', 
            'c_charities_b': 'Charities (last year)', 'c_ceremonies_b': 'Rituals/ceremonies (last year)', 
            'c_textiles_b': 'Textiles (last year)', 'c_utensils_b': 'Utensils (last year)',
            'c_dowry_b': 'Dowry (last year)', 'c_furniture_b': 'Furniture (last year)', 
            'c_other_b': 'Other non-food (last year)', 'asset_val_cows_b': 'Large livestock (cows)', 
            'asset_val_smallanimals_b': 'Small livestock', 'asset_val_bicycle_b': 'Bicycle', 
            'asset_val_radio_b': 'Radio', 'asset_val_motorcycle_b': 'Motorcycle', 
            'asset_val_net_b': 'Mosquito net', 'asset_val_poultry_b': 'Poultry', 
            'asset_val_bed_b': 'Bed', 'asset_val_chairtables_b': 'Chair/table', 
            'asset_val_mobile_b': 'Mobile phone', 'asset_val_netITN_b': 'Mosquito net ITN', 
            'asset_n_house_b': '# Houses', 'in_business_b': 'In business', 
            'child_total_b': '# Child', 'hh_size_b':'Household size'}, inplace=True)

newdf = df_to_orgtbl(means, float_fmt = '%.3f')

print(newdf)


 #+end_src

 #+RESULTS: attrition_balance_nonzero
 |                                | mean_baseline |  beta_mid | beta_end |
 |--------------------------------+---------------+-----------+----------|
 | # Houses                       |         1.054 |     0.083 |    0.071 |
 | Bed                            |         5.328 |     0.028 | -0.408** |
 | Bicycle                        |         5.642 |     0.106 |    0.015 |
 | Chair/table                    |         4.902 |     0.179 |   -0.224 |
 | Large livestock (cows)         |         7.428 |     0.686 |    0.133 |
 | Mobile phone                   |         4.862 |     0.075 |   -0.174 |
 | Motorcycle                     |         7.223 |     0.918 |    0.787 |
 | Mosquito net ITN               |         3.151 |     0.034 |   -0.324 |
 | Mosquito net                   |         3.096 |     0.009 |    0.056 |
 | Poultry                        |         4.582 |     0.259 |    0.111 |
 | Radio                          |         4.314 |    -0.036 |   -0.354 |
 | Small livestock                |          6.29 |     0.351 |    0.081 |
 | Alcohol (last three days)      |         1.367 |    -0.272 |   0.633* |
 | Beans (last three days)        |         1.746 |      0.3* |   -0.272 |
 | Cereals (last three days)      |         3.003 |    -0.052 |  -0.255* |
 | Rituals/ceremonies (last year) |          4.54 |    -0.139 |   -0.085 |
 | Charities (last year)          |         3.237 |    -0.241 |    0.076 |
 | Clothing, footwear (last year) |         4.973 |    0.234* |   -0.169 |
 | Cosmetics (last month)         |          2.68 |     -0.03 |    0.062 |
 | Dowry (last year)              |         6.103 |     0.596 |    0.122 |
 | Egg (last three days)          |         1.797 |    -0.081 |    0.076 |
 | Entertainment (last month)     |         2.033 |    -0.116 |    0.418 |
 | Fish (last three days)         |         2.069 |       0.0 |    0.028 |
 | Fruit (last three days)        |         1.337 |     0.094 |   -0.114 |
 | Cooking fuel (last month)      |         2.912 |     0.033 |   -0.257 |
 | Furniture (last year)          |         4.273 |     0.041 |   -0.338 |
 | Meat (last three days)         |         2.859 |     0.133 |    0.105 |
 | Milk (last three days)         |         2.461 |     0.193 |    0.175 |
 | Oil (last three days)          |         0.896 |    -0.054 |   -0.216 |
 | Other non-food (last year)     |           nan |       nan |      nan |
 | Other food (last three days)   |           nan |       nan |      nan |
 | Salt (last three days)         |          0.18 | -0.154*** |   -0.053 |
 | Toiletries (last month)        |         2.276 |     -0.17 |   -0.086 |
 | Spices (last three days)       |         0.792 |     -0.19 |   -0.156 |
 | Sugar (last three days)        |         1.317 |    -0.092 |   -0.116 |
 | Textiles (last year)           |         4.137 |     0.099 |   -0.109 |
 | Transportation (last month)    |         2.417 |    -0.132 |    0.022 |
 | Utensils (last year)           |         4.347 |      0.21 |    -0.24 |
 | Vegetables (last three days)   |         1.514 |     0.095 |   -0.047 |
 | # Child                        |         1.142 |  0.219*** |   0.173* |
 | Household size                 |         1.919 |   0.081** |    0.062 |
 | In business                    |         0.415 |     0.038 |    0.007 |


* COMMENT Extra Analysis

** Good-level analysis

Next, \ref{tab:consumption_full} sets aside these aggregated measures to look more
carefully at potential changes in the composition of consumption in each group. Given
the large number of zeros, we use a linear model to consider first the 
frequency of non-zero consumption of each good among treatment and control
households, then look at levels of consumption among households with non-zero
consumption. \Tab{consumption_full} presents point estimates.

A few changes in the composition of consumption are interesting. TUP households appear to consume 17% less
sorghum (often considered an inferior good in Yei) and more on rice, which is
considered a higher-quality staple. While almost everyone reports some health
spending over the past month, both treatment groups spent more, though only
statistically significant in the cash group, which saw a 50% increase over the
control group. The cash group was also 30% (14 pp) more likely to have spent money
on funerals, though they did not spend more on average.

#+name: consumption_disaggreate_results
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
<<consumption_analysis>>
from matplotlib import pyplot as plt
#~ Only keep disaggregate items
Goods = C.filter(regex="^c_").rename(columns=lambda col: col[2:] if col.startswith("c_") else col)
#~ 1 if coded as >0, 0 if zero, keep NaN's missing, since those are only from missing surveys.
Nonzero = Goods.applymap(lambda i: float(i>0) if not np.isnan(i) else np.nan)
#~ Restrict Goods df to positive responses.
Goods = Goods.replace(0,np.nan)
too_many_zeros = 30
many_zeros = [item for item in Goods if Goods[item].notnull().sum()<too_many_zeros]
Nonzero = Nonzero.drop(many_zeros,1)
Goods = Goods.drop(many_zeros,1)

#~ Merge in Control Vars
controls = ["cons","TUP","CSH"]
Goods = Goods.join(C[controls],how="left")
Nonzero = Nonzero.join(C[controls],how="left")
Items = [item[:-2] for item in Goods if item.endswith("_e")]
CTL = Goods[controls].sum(axis=1)==1 #~ i.e. only constant ==1, TUP & UCT ==0
Goods_ctl_mean =   Goods.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Zeros_ctl_mean = Nonzero.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])

Zero, Zero_se = reg_table(regressions(Nonzero,"_e", outcomes=Items, Baseline="_b"), resultdf=True, Transpose=True)
Good, Good_se = reg_table(regressions(Goods,"_e", outcomes=Items, Baseline="_b"  ), resultdf=True, Transpose=True)
#~ Make full table of Standard errors
SE = Zero_se[["TUP","CSH"]].join(Good_se[["TUP","CSH"]], lsuffix=" (%>0)", rsuffix=" (Amt.)")

#~ Make full table of point estimates and control-group means
FullTable = pd.DataFrame({"Mean (CTL)":Goods_ctl_mean, "% >0 (CTL)":Zeros_ctl_mean}).join(Zero[["TUP","CSH"]])
FullTable = FullTable.join(Good[["TUP","CSH","N"]], lsuffix=" (%>0)", rsuffix=" (Amt.)")

#~ Make % change graph
fig, ax = plt.subplots(2,1, figsize=(6,9))
for i, group in enumerate(("TUP","CSH")):
    pct_change = FullTable[group+" (Amt.)"]/FullTable["Mean (CTL)"]
    se_change  = SE[group+" (Amt.)"]/FullTable["Mean (CTL)"]
    pct_change.sort()
    pct_change.plot(kind="bar", yerr=SE[group+" (Amt.)"], ax=ax[i])
    ax[i].set_title(group, fontsize=6)
fig.savefig("../figures/Consumption.png")

FullTable = df_to_orgtbl(FullTable, sedf=SE)
return FullTable
#+end_src

#+name: tab:consumption_full
#+caption: Control group means and estimated treatment effects for percent consuming any and total amounts consumed.
#+attr_latex: :environment longtable :align lrrrrrrr
 #+RESULTS: consumption_disaggreate_results

** Disaggregate Asset Results 

#+name: asset_disaggreate_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<assets_disaggreate_analysis>>
return tab
#+end_src

#+name: tab:asset_disaggregate
#+caption: Control means and treatment effects for Assets owned by >40 households
#+RESULTS: asset_disaggreate_results
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|
|              | # own  |              |             | Value     |                |                |          |
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|
| Item         | CTL    | TUP          | UCT        | CTL       | TUP            | UCT           | N        |
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|
| Pangas       | $1.06$ | $0.01$       | $0.13^{**}$ | $11.86$   | $1.66^{**}$    | $0.04$         | $410.00$ |
| Chickens     | $3.79$ | $0.70$       | $-0.32$     | $154.35$  | $23.31$        | $0.80$         | $162.00$ |
| Mobile       | $1.88$ | $-0.09$      | $0.08$      | $113.96$  | $2.62$         | $1.70$         | $569.00$ |
| Radio        | $1.62$ | $0.84$       | $-0.40$     | $57.25$   | $4.98$         | $5.10$         | $333.00$ |
| Shed         | $1.22$ | $-0.14$      | $-0.22$     | $48.81$   | $37.57$        | $6.81$         | $53.00$  |
| Stoves       | $1.44$ | $0.34$       | $0.31$      | $20.32$   | $18.19^{**}$   | $8.31$         | $84.00$  |
| Potspans     | $4.46$ | $-0.06$      | $-0.23$     | $102.73$  | $15.90$        | $-15.40$       | $582.00$ |
| Ducks        | $5.72$ | $4.26^{***}$ | $-0.16$     | $230.93$  | $109.99^{***}$ | $-19.34$       | $223.00$ |
| Motorcycle   | $1.51$ | $-0.48$      | $0.12$      | $2288.48$ | $300.46$       | $-196.32$      | $66.00$  |
| Chairtables  | $5.02$ | $0.25$       | $0.39$      | $167.62$  | $19.00$        | $-24.73$       | $638.00$ |
| Net          | $3.07$ | $0.03$       | $-0.08$     | $24.49$   | $0.66$         | $-3.81$        | $382.00$ |
| Axes         | $1.02$ | $0.03$       | $-0.02$     | $17.74$   | $0.02$         | $-3.94^{**}$   | $218.00$ |
| Smallanimals | $3.39$ | $0.29$       | $-0.90$     | $767.26$  | $-151.35$      | $-311.05^{**}$ | $155.00$ |
| Charcoal     | $2.20$ | $-0.26$      | $-0.83$     | $35.81$   | $-1.43$        | $-4.65$        | $176.00$ |
| Bicycle      | $6.34$ | $-5.46$      | $-5.52$     | $272.90$  | $-31.50$       | $-42.67$       | $135.00$ |
| Bed          | $3.17$ | $-0.23$      | $-0.40$     | $300.64$  | $19.32$        | $-57.78^{*}$   | $628.00$ |
| Tv           | $1.48$ | $-0.36$      | $-0.26$     | $380.45$  | $121.95$       | $348.23^{**}$  | $45.00$  |
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|


* COMMENT Code appendix
  
** Food Security

 #+name: foodsecure_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle foodsecurity.py
 #~ DevLunch_analysis.py
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
 D = full_data(DIR = DATADIR,balance=[])
 WEEKLY = True

 codes = {"1-2 times a week": 3,
         "3-6 times a week": 2,
         "Everyday": 1,
         "everyday": 1,
         "Less than once a week": 4,
         "less than once a week": 4,
         "Never": 5,
         "never": 5}

 recode = lambda x: codes.setdefault(x,x)

 Aval2013 = D.filter(regex="^fs_.*_b").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Aval2014 = D.filter(regex="^fs_.*_m").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Aval2015 = D.filter(regex="^fs_.*_e").rename(columns=lambda x: x[3:-2]).applymap(recode)

 if WEEKLY:
     weekly = lambda i: float(i<3) if pd.notnull(i) else np.nan
     Aval2013 = Aval2013.applymap(weekly)
     Aval2014 = Aval2014.applymap(weekly)      
     Aval2015 = Aval2015.applymap(weekly)
 
 index_vars = "worried,portions,fewmeals,nofood,hungry,wholeday".split(",")
 Outcomes = index_vars+["z-score"]
 #~ Creates Year dummies, z-scores and baseline values as `var'2013
 for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
     Aval["Year"]=Year
     if not weekly:
        for var in index_vars:
            Aval[index_vars] = (Aval[index_vars]-Aval[index_vars].mean())/Aval[index_vars].std()
     FS_sum = Aval[index_vars].sum(axis=1)
     Aval["z-score"] = (FS_sum-FS_sum.mean())/FS_sum.std()
     for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
    
 Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
 Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
 Vals = Vals.join(D[["TUP","CSH"]])

 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

 Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 Vals=Vals.loc[2014:2015]
 regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = Vals["TUP"]+Vals["CSH"] ==0
 CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+end_src

** Consumption

 #+name: lambda_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/SS-lambdas.py

 import numpy as np
 import pandas as pd
 import cfe.estimation as nd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append("../../data")
 from TUP import full_data, consumption_data, regressions, reg_table, df_to_orgtbl, mobile_data
 food =  ['cereals', 'maize', 'sorghum', 'millet', 'potato', 'sweetpotato', 'rice', 'bread', 'beans', 'oil', 'salt', 'sugar', 'meat', 'livestock', 'poultry', 'fish', 'egg', 'nuts', 'milk', 'vegetables', 'fruit', 'tea', 'spices', 'alcohol', 'otherfood']
 month = ['fuel', 'medicine', 'airtime', 'cosmetics', 'soap', 'transport', 'entertainment', 'childcare', 'tobacco', 'batteries', 'church', 'othermonth']    
 ConsumptionItems = food+['airtime','fuel']
 mobile=True

 D = full_data(DIR=DATADIR)
 C, HH, T = consumption_data(D,WRITE=False,include2016=False)
 HH['log HHSIZE'] = HH["hh_size"].apply(np.log)
 HH = HH.drop("hh_size",1)
 y,z = C.replace(0,np.nan).apply(np.log).sort_index(level=[0,1,2])[ConsumptionItems].copy(),HH.sort_index(level=[0,1,2]).copy()
 y.index.names, z.index.names = ['j','t','mkt'], ['j','t','mkt']
 keep = pd.notnull(y.index.get_level_values("mkt"))
 y,z = y.loc[keep,:].align(z,join="left",axis=0)
 b,ce,d,sed= nd.estimate_reduced_form(y,z,return_se=True,VERBOSE=True)
 ce = ce.dropna(how='all')
 print("Getting Loglambdas")
 bphi,logL=nd.get_loglambdas(ce,TEST="warn")
 try:
    xrange
    logL.to_pickle(DATADIR + "ss-lambdas.df")
 except NameError: logL.to_pickle(DATADIR + "ss-lambdas3.df")
 
 if mobile:
     M,Mc,Mhh = mobile_data(use_dates=True,DIR = DATADIR+"Mobile/")
     y = Mc.replace(0,np.nan).apply(np.log).sort_index(level=[0,1,2]).filter(items=ConsumptionItems).copy()
     z = Mhh.sort_index(level=[0,1,2]).copy()
     y.index.names, z.index.names = ['j','t','mkt'], ['j','t','mkt']
     keep = pd.notnull(y.index.get_level_values("mkt"))
     y,z = y.loc[keep,:].align(z,join="left",axis=0)
     b,ce,d,sed= nd.estimate_reduced_form(y,z,return_se=True,VERBOSE=True)
     ce = ce.dropna(how='all')
     print("Getting Loglambdas")
     Mbphi,MlogL=nd.get_loglambdas(ce,TEST="warn")
     MlogL -= MlogL.mean()
     MlogL /= MlogL.std()
     MlogL = MlogL.unstack('t').drop('4February',1).stack()
     try:
       xrange
       MlogL.to_pickle(DATADIR + "ss-lambdas_mobile.df")
     except NameError: MlogL.to_pickle(DATADIR + "ss-lambdas_mobile3.df")

 #+end_src 

 #+name: consumption_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/SS-consumption.py
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 from TUP import full_data, consumption_data, regressions, reg_table, df_to_orgtbl, mobile_data
 
 USE_LAMBDAS = False #~ Whether to use the output of SS-lambdas.py as an outcome (You may have to run it first.)
 TGROUP_FE   = True  #~ Whether to use TUP & CSH dummy variables as controls (I'm pretty sure we should be)

 food = ['cereals', 'maize', 'sorghum', 'millet', 'potato', 'sweetpotato', 'rice', 'bread', 'beans', 'oil', 'salt', 'sugar', 'meat', 'livestock', 'poultry', 'fish', 'egg', 'nuts', 'milk', 'vegetables', 'fruit', 'tea', 'spices', 'alcohol', 'otherfood']
 month = ['fuel', 'medicine', 'airtime', 'cosmetics', 'soap', 'transport', 'entertainment', 'childcare', 'tobacco', 'batteries', 'church', 'othermonth']    
 year = ['clothesfootwear', 'womensclothes', 'childrensclothes', 'shoes', 'homeimprovement', 'utensils', 'furniture', 'textiles', 'ceremonies', 'funerals', 'charities', 'dowry', 'other']    


 D = full_data(DIR=DATADIR)
 C, HH, T = consumption_data(D,WRITE=True) #"csv")
 if USE_LAMBDAS:
    logL = pd.read_pickle(DATADIR + "ss-lambdas.df")
    logL.index.names=["HH","Year","Location"]
    C = C.join(logL,how="left").rename(columns={"loglambda":"$\log\lambda_{it}$"})
 C = C.reorder_levels([1,2,0])#.sort_index(level=0)
 keep = pd.notnull(C.index.get_level_values("Location"))
 C = C.loc[keep,:]

 # Make aggregate variables
 C["Food"]   = C.filter(items=food).sum(axis=1).replace(0,np.nan)
 C["Month"]   = C.filter(items=food).sum(axis=1)
 C["Year"]   = C.filter(items=food).sum(axis=1)
 C["Tot"]    = C[["Food","Month","Year"]].sum(axis=1).replace(0,np.nan)

 def align_indices(df1,df2):
    """
    Reorder levels of df2 to match that of df1
    Must have same index.names
    """
    I1, I2 = df1.index, df2.index
    try: assert(not set(I1.names).difference(I2.names))
    except AssertionError: raise ValueError("Index names must be the same")
    new_order = []
    for lvl in I1.names: new_order.append(I2.names.index(lvl))
    df2 = df2.reorder_levels(new_order)
    return df1, df2

 def winsorize(Series, **kwargs):
    """
    Need to implement two-sided censoring as well.
    WARNING: if Top<0, all zeros will be changed to Top
    """
    percent    = kwargs.setdefault("percent",99)
    stdev      = kwargs.setdefault("stdev",False)
    drop       = kwargs.setdefault("drop",False)
    drop_zeros = kwargs.setdefault("drop_zeros",True)
    twoway     = kwargs.setdefault("twoway",False)

    if drop_zeros: S = Series.replace(0,np.nan).dropna()
    else: S = Series.dropna()
    N_OBS = S.notnull().sum()
    if N_OBS<10: return S

    if percent: Top = np.percentile(S, percent)
    if stdev:   
        Top =  S.dropna().mean()
        Top += stdev*S.dropna().std()
    try: assert((not drop_zeros) or Top>0)
    except AssertionError: raise ValueError("Top < 0 but zeros excluded")
    if drop: replace_with = np.nan
    else:    replace_with = Top

    _Series = Series.copy()
    _Series[_Series>Top]=replace_with

    if not twoway: return _Series
    else:
        kwargs['twoway'] = False
        return -1*winsorize(-1*_Series, **kwargs)

 def USD_conversion(Exp,exchange_rate=1.,PPP=1.,inflation=1.,time='Year'):
    """
    Convert nominal local currency into price- and inflation-adjusted USD

    Exp - A numeric or pd.Series object 
    exchange_rate - Taken as LCU/USD. 
    PPP - Taken as $Real/$nominal
    inflation - Taken as % inflation compared to some baseline.
    time - If a list is passed, `time' indicates the name or position of the time level in Exp.index
        NOTE: This has to be a cumulative number, so if inflation is 20% for two straight years, that year should be divided by (1+.2)**2
    Final calculation will basically be Exp_usdppp = Exp*(exchange_rate*PPP)/inflation

    if pd.Series are passed for any kwarg, index name needs to be in the multi-index of Exp.
    """
    if type(inflation)==list: inflation=[1./i for i in inflation]
    else: inflation = 1/inflation
    if type(exchange_rate)==list: exchange_rate=[1./i for i in exchange_rate]
    else: exchange_rate = 1/exchange_rate
    
    _Exp = Exp.copy()
    VARS = (exchange_rate, PPP,inflation)
    if list in map(type,VARS):
        if time in _Exp.index.names: time=_Exp.index.names.index(time)
        time = _Exp.index.levels[time]
    for var in VARS:
        if type(var)==list: var=pd.Series(var,index=time)
        try: _Exp = _Exp.mul(var)
        except ValueError: #~ If Series index doesn't have a name, try this...
            var.index.name = var.name
            _Exp = _Exp.mul(var)
    return _Exp

 def percapita_conversion(Exp,HH,children=["boys","girls"],adult_equivalent=1.,minus_children='hh_size'):
    """
    Returns household per-capita expenditures given:
        `Exp'- Total household expenditures
        `HH' - Total number of individuals in the household
            If HH is a pd.DataFrame, Exp is divided by HH.sum(1)
            if `children' is the name of a column or a list of column names, 
            those first get divided by the factor adult_equivalent
    """
    try: HH.columns #~ If HH is a series, just divide though
    except AttributeError: return Exp.div(HH)
    _HH = HH.copy()
    if type(children)==str: children=[children]
    children = _HH.columns.intersection(children).tolist()
    if minus_children: _HH[minus_children] -= _HH[children].sum(1)
    if children: _HH[children] *= adult_equivalent
    Exp,_HH = align_indices(Exp,_HH)
    return Exp.div(_HH.sum(1).replace(0,1))

 #~ Source: http://data.worldbank.org/indicator/PA.NUS.PRVT.PP?locations=SS&name_desc=false
 xrate = [ 2.161, 2.162, 3.293] #~ Using PPP adjusted xrate and just setting PPP=1.
 PPP = 1.
 inflation= 1. #~ Bank data uses international $, which is inflation adjusted.
 C["Exp_usd"] = winsorize(USD_conversion(C["Tot"],exchange_rate=xrate,PPP=PPP,inflation=inflation))
 C["Tot_pc"] = percapita_conversion(C["Exp_usd"],HH,adult_equivalent=.5)
 #C["Exp_usdpc_tc"] = winsorize(C["Exp_usdpc"])

 #C["z-score"]  = (C["Tot"]-C["Tot"].mean())/C["Tot"].std()
 C["FoodShr"]= C["Food"].div(C["Tot"]) #$\approx$ FoodShare variable
 C["logTot"] = C["Tot"].apply(np.log)
 C = C.join(T, how="left",lsuffix="_")

 if USE_LAMBDAS: Outcomes = ["Tot","logTot","FoodShr", "Food", "$\log\lambda_{it}$"]
 else: Outcomes = ["Tot","logTot","FoodShr", "Food"]

 #$\approx$ Make Baseline variable
 for var in Outcomes: 
     Bl = C.loc[2013,var].reset_index("Location",drop=True)
     #if var in mC: mC = mC.join(Bl,rsuffix="2013", how="left")
     C = C.join(Bl,rsuffix="2013", how="left")


 C["Y"]=np.nan
 for yr in (2013, 2014, 2015): C.loc[yr,"Y"]=str(int(yr))

 C = C.join(pd.get_dummies(C["Y"]), how="left",lsuffix="_")
 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         interaction = C[group]*C[year]
         if interaction.sum()>0: C["{}*{}".format(group,year)] = interaction

 if TGROUP_FE: Controls = ["2014","2015", 'TUP*2014', 'CSH*2014', 'TUP*2015', 'CSH*2015', 'TUP','CSH']
 else:         Controls = ["2014","2015", 'TUP*2014', 'CSH*2014', 'TUP*2015', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 C = C.loc[2014:2015]
 regs  = regressions(C, outcomes=Outcomes,  controls=Controls,  Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = C["TUP"]+C["CSH"] ==0
 CTLmean = {var: C[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: C[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+END_SRC
   
 #+name: mobile_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/SS-mobile.py
 import sys
 DATADIR = "../../data/"
 sys.path.append("../../data")
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 from TUP import full_data, consumption_data, regressions, reg_table, df_to_orgtbl, mobile_data
 ITEMS = ["beans", "sugar", "fish", "nuts", "vegetables", "airtime", "fuel"]

 D = full_data(DIR=DATADIR)
 HH, T = consumption_data(D,WRITE=True)[1:] #"csv")
 M, C,mHH= mobile_data(DIR = DATADIR+"Mobile/")
 try: logL = pd.read_pickle(DATADIR+"ss-lambdas_mobile.df")
 except EnvironmentError: raise IOError("Need to run SS-lambdas.py")
 logL.index.names=["HH","Year","Location"]
 logL.name       =["loglambda"]
 C    = C.join(logL,how="left").rename(columns={"loglambda":"$\log\lambda_{it}$"})
 C    = C.reorder_levels([1,0,2]).sortlevel()
 keep = pd.notnull(C.index.get_level_values("Location"))
 C    = C.loc[keep,:]
 # Make aggregate variables
 C["Tot"]    = C.filter(ITEMS).sum(axis=1).replace(0,np.nan)
 C["logTot"] = C["Tot"].apply(np.log)
 C           = C.join(T, how="left",lsuffix="_")
 C['const']  = 1.

 Outcomes =["Tot",  "logTot", "$\log\lambda_{it}$"]
 Controls= ['const', 'TUP', 'CSH']

 regs = regressions(C,outcomes=Outcomes, controls=Controls, Baseline=2013)
 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])
 CTL = C["TUP"]+C["CSH"] ==0
 CTLmean = {var: C.loc[CTL,var].mean() for var in Outcomes}
 CTLsd = {var: C.loc[CTL,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest= regs[var].t_test("TUP - CSH = 0").summary_frame()
     diff.loc[   r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest["coef"][0]
     diff_se.loc[r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest["std err"][0]

 results = results.append(diff)
 SE = SE.append(diff_se)
 mtab = df_to_orgtbl(results, sedf=SE)

 #+end_src
   
** Assets
   
#+name: asset_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/asset_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
from matplotlib import pyplot as plt
import sys
sys.path.append("../../data")
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

TGROUP_FE = True

D = full_data(DIR="../../data/")

Outcomes = ["Total", "Productive","Livestock"]
Aval2013 = asset_vars(D,year=2013)[0][Outcomes]
Aval2014 = asset_vars(D,year=2014)[0][Outcomes]
Aval2015 = asset_vars(D,year=2015)[0][Outcomes]

#$\approx$ Creates Year dummies and baseline values as `var'2013
for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
    Aval["Year"]=Year
    for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
Vals = Vals.join(D[["TUP","CSH"]])
Vals["CTL"] = (Vals["TUP"]+Vals["CSH"] ==0).apply(int)

for group in ("TUP", "CSH"):
    for year in ("2013", "2014", "2015"):
        Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

#~ Make graph of distribution
stringify = lambda var: Vals[var].apply(lambda x: var if x else "")
Vals["Group"] = stringify("TUP")+stringify("CSH")+stringify("CTL")


amean = Vals.groupby(["Year","Group"]).mean()[["Total","Productive","Livestock"]]
aN = Vals.groupby(["Year","Group"]).count()[["Total","Productive","Livestock"]]
astd = Vals.groupby(["Year","Group"]).std()[["Total","Productive","Livestock"]]
ase = astd/np.sqrt(aN)
asset_pctchange = (amean/amean.ix[2013]).unstack("Year") - 1

for var in ("Total","Productive","Livestock"):
   #~ fig,ax = plt.subplots(1,2)
   #~ for i,yr in enumerate((2014,2015)):
   #~     Vals.ix[yr].dropna(subset=[[var,"TUP","CSH","CTL"]]).groupby("Group")[var].plot(kind="kde",ax=ax[i])
   #~     ax[i].set_title("{} Asset Value in {}".format(var,yr))
   #~     ax[i].legend()
   #~     #~ ax[i].set_aspect(1)
   #~     ax[i].set_xlim(left=0)
   #~ plt.savefig("../figures/Asset{}_kde.png".format(var))
   #~ plt.clf()
   amean.unstack("Group")["Total"].plot(kind="bar",yerr=ase.unstack("Group")["Total"].values)
   plt.tight_layout()
   plt.xticks(rotation=45)
   plt.savefig("../figures/Asset{}_groupyear.png".format(var))
   plt.clf()

if TGROUP_FE: Controls = ["2014","2015", 'TUP*2014', 'CSH*2014', 'TUP*2015', 'CSH*2015', 'TUP','CSH']
else:         Controls = ["2014","2015", 'TUP*2014', 'CSH*2014', 'TUP*2015', 'CSH*2015']


#$\approx$ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
Vals=Vals.loc[2014:2015]
regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

CTL = Vals["TUP"]+Vals["CSH"] ==0
CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

for var in Outcomes:
    ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


results = results.append(diff)
SE = SE.append(diff_se)

tab = df_to_orgtbl(results, sedf=SE)
#+end_src

#+name: assets_disaggreate_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
import numpy as np
import pandas as pd
import sys
sys.path.append("../../data")
from TUP import full_data, regressions, asset_vars, reg_table, df_to_orgtbl

D = full_data(balance=[])
D["cons"] = 1.
Count = D.filter(regex="^asset_n_").rename(columns=lambda col: col[8:])
Vals = D.filter(regex="^asset_val_").rename(columns=lambda col: col[10:])
#~ 1 if coded as >0, 0 if zero, keep NaN's missing, since those are only from missing surveys.
Nonzero = Count.applymap(lambda i: float(i>0) if not np.isnan(i) else np.nan)
#~ Restrict both df's to nonzero responses.
Count = Count.replace(0,np.nan)
Vals  =  Vals.replace(0,np.nan)
too_many_zeros = 30
many_zeros = [item for item in Vals if Vals[item].notnull().sum()<too_many_zeros]
for df in (Nonzero, Count, Vals): df.drop(many_zeros,1, inplace=True)

#~ Merge in Control Vars
controls = ["cons","TUP","UCT"]
Nonzero  = Nonzero.join(D[controls],how="left")
Count    =   Count.join(D[controls],how="left")
Vals     =    Vals.join(D[controls],how="left")

Items = [item[:-2] for item in Vals if item.endswith("_e")]
CTL = Vals[controls].sum(axis=1)==1 #~ i.e. only constant ==1, TUP & UCT ==0
Zeros_ctl_mean = Nonzero.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Count_ctl_mean =   Count.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Vals_ctl_mean  =    Vals.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])

ZeroTable       = reg_table(regressions(Nonzero,"_e", outcomes=Items, controls = ["cons",'UCT','TUP'], Baseline="_b"), orgtbl=True, Transpose=True)
Count, Count_se = reg_table(regressions(Count,"_e",   outcomes=Items, controls = ["cons",'UCT','TUP'], Baseline="_b"), resultdf=True, Transpose=True)
Vals, Vals_se   = reg_table(regressions(Vals,"_e",    outcomes=Items, controls = ["cons",'UCT','TUP'], Baseline="_b"), resultdf=True, Transpose=True)

#~ Make full table of Standard errors-- MAKE SURE YOU HAVE THE SUFFIXES RIGHT.
SE = Count_se[["TUP","UCT"]].join(Vals_se[["TUP","UCT"]], rsuffix=" (SSP)", lsuffix=" (# own)")

#~ Make full table of point estimates and control-group means
FullTable = pd.DataFrame({"# own (CTL)":Count_ctl_mean, "Value (CTL)":Vals_ctl_mean}).join(Count[["TUP","UCT"]])
FullTable = FullTable.join(Vals[["TUP","UCT","N"]], rsuffix=" (SSP)", lsuffix=" (# own)")
FullTable = df_to_orgtbl(FullTable, sedf=SE)
AllTables = FullTable+"\n\n"+ZeroTable
return AllTables
#+end_src

** Savings

#+name: savings_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/savings_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
import sys
sys.path.append("../../data")
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl

def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

D = full_data(balance=[])

Sav = pd.DataFrame(index=D.index) #~ Set up empty DataFrame to fill

years = [("_b",2013), ("_m",2014), ("_e", 2015)]
for suff,year in years: #~ Make Aggregate savings and land holding variables
    Sav["Savings{}".format(year)]  = D.filter(regex="^savings_(home|bank|BRAC|NGOs|other){}".format(suff)).sum(1)
    Sav["Food Sav{}".format(year)] = D.filter(regex="^savings_(maize|sorghum|otherfood)_val{}".format(suff)).sum(1)

    Sav["LandCult{}".format(year)] = D.filter(regex="^land_(owncult|rentcult|communitycult){}".format(suff)).sum(1)
    Sav["LandOwn{}".format(year)]  = D.filter(regex="^land_own.*{}".format(suff)).sum(1)

    Sav["Get Trans{}".format(year)]  = D.filter(regex="^transfers_get.*{}".format(suff)).sum(1)
    Sav["Give Trans{}".format(year)] = D.filter(regex="^transfers_give.*{}".format(suff)).sum(1)

Outcomes = ["Savings","Food Sav","LandCult","LandOwn", "Get Trans", "Give Trans"] #~ Loans give/received omitted

by_year = []
for yr in ("2013","2014","2015"): #~ Provide Baseline Values & put in long format
    S_Year = Sav.filter(like=yr).rename(columns=lambda x:x[:-4])
    for var in Outcomes: 
        if var+"2013" in Sav: S_Year[var+"2013"] = Sav[var+"2013"]
    #~ Note that adding "Year" has to come after topcode, which switches strings to Nan...
    S_Year["Year"] = yr
    by_year.append(S_Year)

#~ In long format with Year,HH index
Sav = pd.concat(by_year).reset_index().set_index(["Year", "HH"], drop=False)
#~ Make Year Dummies for fixed effects


Sav = Sav.join(pd.get_dummies(Sav["Year"]).rename(columns=lambda col: str(int(col))), how="left")
Sav = Sav.drop(["HH","Year"],1)
Sav = Sav.join(D[["TUP","CSH"]])

for group in ("TUP", "CSH"): #~ Make Treatment-by-year interactions
    for year in ("2013", "2014", "2015"):
        Sav["{}*{}".format(group,year)] = Sav[group]*Sav[year]

def isPositive(i):
    #~ Returns 1 if number is positive, 0 if number<=0, nan if already nan, and self if string.
    #~ Note that it's safe to run dummy variables through..
    try:
        if np.isnan(i): return i
        else: return float(i>0)
    except TypeError: return i

too_many_null = 30
Nonzero = Sav.applymap(isPositive)

#~ Save DataFrame with zeros
Savings = Sav.copy()
#~ Naturally, only do this after Creating Nonzero dataframe.
for var in Outcomes: #~ Set zeros to missing And topcode among non-zero values
    for outcome in (var,var+"2013"):
       if outcome in Sav:
           Sav[outcome] = Sav[outcome].replace(0,np.nan)
           Sav[outcome] = Sav.groupby(level="Year")[outcome].apply(topcode) #~ (Untested)

many_null = [item for item in Sav if Sav[item].count()<too_many_null]
many_null2 =[item for item in Savings if Savings[item].count()<too_many_null]
Sav = Sav.drop(many_null,1).copy()
Savings = Savings.drop(many_null,1).copy()

Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

Sav = Sav.loc["2014":"2015"]
Savings = Savings.loc["2014":"2015"]
Nonzero = Nonzero.loc["2014":"2015"]
Sav_regs = regressions(Sav,     outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)
Zer_regs = regressions(Nonzero, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)
Amt_regs = regressions(Savings, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

Sav_results, Sav_SE  = reg_table(Sav_regs,  resultdf=True,table_info=["N","F-stat"])
Zer_results, Zer_SE  = reg_table(Zer_regs,  resultdf=True,table_info=["N","F-stat"])
Amt_results, Amt_SE  = reg_table(Amt_regs,  resultdf=True,table_info=["N","F-stat"])

CTL  = Sav["TUP"]+Sav["CSH"] ==0
CTL2 = Savings["TUP"]+Savings["CSH"] ==0
#~ Get control group means and standard deviations
Sav_CTLmean = {var: Sav[CTL].loc["2015",var].mean() for var in Outcomes}
Zer_CTLmean = {var: Nonzero[CTL].loc["2015",var].mean() for var in Outcomes}
Amt_CTLmean = {var: Savings[CTL2].loc["2015",var].mean() for var in Outcomes}

Sav_CTLsd = {var: Sav[CTL].loc["2015",var].std() for var in Outcomes}
Zer_CTLsd = {var: Nonzero[CTL].loc["2015",var].std() for var in Outcomes}
Amt_CTLsd = {var: Savings[CTL2].loc["2015",var].std() for var in Outcomes}

Sav_diff, Sav_diff_se = pd.DataFrame(Sav_CTLmean,index=["CTL mean"]), pd.DataFrame(Sav_CTLsd,index=["CTL mean"])
Zer_diff, Zer_diff_se = pd.DataFrame(Zer_CTLmean,index=["CTL mean"]), pd.DataFrame(Zer_CTLsd,index=["CTL mean"])
Amt_diff, Amt_diff_se = pd.DataFrame(Amt_CTLmean,index=["CTL mean"]), pd.DataFrame(Amt_CTLsd,index=["CTL mean"])

for var in Outcomes:
    #~ Savings regressions first
    ttest1= Sav_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Sav_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Sav_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Sav_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Sav_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Sav_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

    #~ Nonzero regressions second
    ttest1= Zer_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Zer_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Zer_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Zer_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Zer_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Zer_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

    #~ Savings regressions third
    ttest1= Amt_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Amt_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Amt_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Amt_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Amt_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Amt_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

Save_results = Sav_results.append(Sav_diff)
Zero_results = Zer_results.append(Zer_diff)
Amnt_results = Amt_results.append(Amt_diff)
Save_SE = Sav_SE.append(Sav_diff_se)
Zero_SE = Zer_SE.append(Zer_diff_se)
Amnt_SE = Amt_SE.append(Amt_diff_se)

#~ Land = ["LandCult","LandOwn"] 
#~ Savings = ["Savings","Food Sav", "Get Trans", "Give Trans"] 
#~ Land_results = Sav_results[Land]
#~ zLan_results = Zer_results[Land]
#~ Land_SE = Sav_SE[Land]
#~ zLan_SE = Zer_SE[Land]
#~ 
#~ Sav_results = Sav_results[Savings]
#~ Zer_results = Zer_results[Savings]
#~ Sav_SE =           Sav_SE[Savings]
#~ Zer_SE =           Zer_SE[Savings]

Save_tab = df_to_orgtbl(Save_results, sedf=Sav_SE)
Zero_tab = df_to_orgtbl(Zero_results, sedf=Zer_SE)
Amnt_tab = df_to_orgtbl(Amnt_results, sedf=Amt_SE)

Table = Zero_tab +"\n"+ Save_tab + "\n"+ Amnt_tab

#+end_src

** Income

#+name: income_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
from pandas.io import stata
import statsmodels.api as sm
from matplotlib import pyplot as plt
import sys
sys.path.append("../../data")
from TUP import full_data, regressions, reg_table, df_to_orgtbl
"""
Note that topcoding has a large effect on the distribution here, and we see only a small (presumably non-random) portion of actual income for each household.
"""

# Top-Code or censor outliers?
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

D = full_data(balance=[])
keep = D.index

I_file = '../../data/Endline/sections_8_17.csv'
I = pd.read_csv(I_file).rename(columns={"id":"HH"}).set_index("HH", drop=True).ix[keep]

#~Getting non-agriculture income data is easy
I = I.filter(regex="^s16")
Imonths    = I.filter(regex="s16_\dc").rename(columns=lambda x: x[:-1])
Ipermonth  = I.filter(regex="s16_\dd").rename(columns=lambda x: x[:-1])
Income_12m = Imonths.mul(Ipermonth).sum(axis=1)
Iyear      = I.filter(regex="s16_\de").rename(columns=lambda x: x[:-1]).sum(axis=1)

A_file = "../../data/Endline/Agriculture_cleaned.csv"
A = pd.read_csv(A_file).rename(columns={"id":"HH"}).set_index("HH",drop=False).ix[keep]
unit_prices = A.groupby(["harvest_type", "harvest_price_unit"])["harvest_price"].median()
prices = unit_prices.loc[zip(A["harvest_type"],A["harvest_price_unit"])]
A["price"]=list(prices)

A["harvest_unit_match"] = A["harvest_price_unit"] == A["harvest_unit"]
A["price"] = A["harvest_unit_match"]*A["harvest_price"] + (1-A["harvest_unit_match"])*A["price"]

A["income_farm_year"] = A["harvest_size"]*A["price"]
Ayear = A.groupby("HH")["income_farm_year"].sum()

unit_prices = A.groupby(["livestock_type", "livestock_price_unit"])["livestock_price"].median()
prices = unit_prices.loc[zip(A["livestock_type"],A["livestock_price_unit"])]
A["price"]=list(prices)
A["livestock_unit_match"] = A["livestock_price_unit"] == A["livestock_unit"]
A["price"] = A["livestock_unit_match"]*A["livestock_price"] + (1-A["livestock_unit_match"])*A["price"]

A["income_livestock_year"] = A["livestock_size"]*A["price"]
Lyear = A.groupby("HH")["income_livestock_year"].sum()

Outcomes = ["Total", "Non-Farm", "Farm",  "Livestock"]
Controls = ["cons", "TUP","CSH"]
Vals = pd.DataFrame({"Non-Farm": Income_12m, "Farm":Ayear, "Livestock":Lyear})
Vals = Vals.apply(topcode)

Vals["Total"] = Vals.sum(axis=1)
Vals["cons"] = 1.

Vals = Vals.join(D[["TUP","CSH"]],how="left")
Vals["CTL"] = (Vals["TUP"]+Vals["CSH"] ==0).apply(int)

#~ Make graph of distribution
stringify = lambda var: Vals[var].apply(lambda x: var if x else "")
Vals["Group"] = stringify("TUP")+stringify("CSH")+stringify("CTL")
Vals.dropna(subset=[["Total","TUP","CSH","CTL"]]).groupby("Group")["Total"].plot(kind="kde")
plt.title("Total Income Distribution by Group")
plt.savefig("../figures/IncomeDistribution.png")
plt.clf()
#~ Make bar graphs
Imean  = Vals.groupby("Group").mean()[Outcomes]
Icount = Vals.groupby("Group").count()[Outcomes]
Istd = Vals.groupby("Group").std()[Outcomes]
Ise=Istd/np.sqrt(Icount)
Imean.T.plot(kind="bar",yerr=Ise.T)
plt.tight_layout()
plt.xticks(rotation=45)
plt.savefig('../figures/Income_group.png')
plt.clf()

regs = {var: sm.OLS(Vals[var], Vals[Controls], missing="drop").fit() for var in Outcomes}
results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

CTL = Vals["CTL"] 
CTLmean = {var: Vals.query("CTL==1")[var].mean() for var in Outcomes}
CTLsd = {var: Vals.query("CTL==1").std() for var in Outcomes}
diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

for var in Outcomes:
    ttest1= regs[var].t_test("TUP - CSH = 0").summary_frame()

    diff.loc[   r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest1["coef"][0]
    diff_se.loc[r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest1["std err"][0]

results = results.append(diff)
SE = SE.append(diff_se)

tab = df_to_orgtbl(results, sedf=SE)

#+end_src

#+RESULTS: income_analysis
: None

** Conflict Exposure
#+name: conflict_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
import sys
sys.path.append("../../data")
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl

# Top-Code or censor outliers?
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

#~ Read in data
D = full_data("../../data/TUP_full.dta", balance=[])
D = D[D.merge_midline != 1]
C = D.filter(like="conflict").rename(columns = lambda x: x[:-2]) #~ Set up empty DataFrame to fill

#~ Make Outcome variables
#~ NOTE: Not looking at whether they protected assets as only 50 #~ said they did, and all said "Migrated to stay with other family"
C["Bsln_NaN"] = D["merge_midline"] == 2
C["Worried"]  = C["conflict_worried"]
C["Affected"] = C["conflict_affected"]
protect_lives_codes = lambda x: {"Nothing": 0, "Migrate to stay with friend/family": 1, "Migrated and found new accommodation": 1,
                                 "Looked for protection with Govt. Military": 2, "Looked for protection with NGO": 2}.get(x)
C["ProtectLives"] = C["conflict_protectlives"].apply(protect_lives_codes)
C["Migrated"] = (C["ProtectLives"]==1) + \
                (C["conflict_affected1"]=="Needed to elocate or migrate")
C["NoMeans"] = C["conflict_whynotprotect"]=="Didn't have the means"
C["NoInvest"]= C.filter(like="affected").applymap(lambda x: x=="Could not plant crop or invest in business").sum(axis=1)
C = C.drop([var for var in C if var.startswith("conflict_")], 1)
Outcomes = ["ProtectLives", "Worried", "Affected", "Migrated", "NoMeans", "NoInvest"]

#~ Bring in Treatment variables
C["TUP"] = D["TUP"]
C["cons"] = 1.
C = C.applymap(lambda x: int(x) if not np.isnan(x) else x)
Controls = ["cons", "TUP"] #~, "Bsln_NaN"]

#~ Plot
byT=C.groupby("TUP")
Cbar=byT.mean().drop(["Bsln_NaN",'cons'],1)
Cstd=byT.std().drop(["Bsln_NaN",'cons'],1)
Cn=byT.count().drop(["Bsln_NaN",'cons'],1)
for df in (Cbar,Cstd,Cn): 
        df.index=["CTL","TUP"]
        df.index.name="Group"
Cse=Cstd/np.sqrt(Cn)
Cbar.T.plot(kind="bar",yerr=Cse.T)
plt.tight_layout()
plt.xticks(rotation=45)
plt.savefig("../figures/conflict_exposure.png")
plt.clf()


C_regs = regressions(C, outcomes=Outcomes, controls=Controls, Baseline=False, baseline_na=False)
C_results, C_SE  = reg_table(C_regs,  resultdf=True,table_info=["N","F-stat"])

#~ Get control group means and standard deviations and add to regression table
CTLmean = {var: C.query("TUP==0")[var].mean() for var in Outcomes}
CTLsdv  = {var: C.query("TUP==0")[var].std()  for var in Outcomes}
CTLmean, CTLsdv = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsdv,index=["CTL mean"])
C_results = C_results.append(CTLmean)
C_SE      = C_SE.append(CTLsdv)
C_results.drop('cons',inplace=True)
C_SE.drop('cons',inplace=True)

Table = df_to_orgtbl(C_results, sedf=C_SE)
return Table

#+end_src

** Confidence & Autonomy

#+name: decision_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
 D = full_data(DIR = DATADIR,balance=[])

 Decide14 = D.filter(regex="^decide_.*_m").rename(columns=lambda x: x[:-2].split("_")[-1])#~.applymap(recode)
 Decide15 = D.filter(regex="^decide_.*_e").rename(columns=lambda x: x[:-2].split("_")[-1])#~.applymap(recode)

{'decide_arguments',
 'decide_fearful',

 'decide_pressuretospend'}

 who_decides = ['dailypurchase', 'familyvisits', 'healthchild', 'healthown', 'majorpurchase', 'moneyown']
 who_recode  = lambda x: float("Herself" in x) if pd.notnull(x) else x

 if WEEKLY:
     weekly = lambda i: float(i<3) if pd.notnull(i) else np.nan
     Aval2013 = Aval2013.applymap(weekly)
     Aval2014 = Aval2014.applymap(weekly)      
     Aval2015 = Aval2015.applymap(weekly)
 
 index_vars = "worried,portions,fewmeals,nofood,hungry,wholeday".split(",")
 Outcomes = index_vars+["z-score"]
 #~ Creates Year dummies, z-scores and baseline values as `var'2013
 for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
     Aval["Year"]=Year
     if not weekly:
        for var in index_vars:
            Aval[index_vars] = (Aval[index_vars]-Aval[index_vars].mean())/Aval[index_vars].std()
     FS_sum = Aval[index_vars].sum(axis=1)
     Aval["z-score"] = (FS_sum-FS_sum.mean())/FS_sum.std()
     for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
    
 Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
 Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
 Vals = Vals.join(D[["TUP","CSH"]])

 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

 Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 Vals=Vals.loc[2014:2015]
 regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = Vals["TUP"]+Vals["CSH"] ==0
 CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+end_src

#+name: confidence_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
 D = full_data(DIR = DATADIR,balance=[])

 autonomy_codes = {'go_NGO',
                   'go_church',
                   'go_healthcenter',
                   'go_housenonrelative',
                   'go_houserelative',
                   'go_market',
                   'go_school',
                   'go_water'}

 codes = {"1-2 times a week": 3,
         "3-6 times a week": 2,
         "Everyday": 1,
         "everyday": 1,
         "Less than once a week": 4,
         "less than once a week": 4,
         "Never": 5,
         "never": 5}

 recode = lambda x: codes.setdefault(x,x)

 Autnmy14 = D.filter(regex="^cango_.*_m").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Autnmy15 = D.filter(regex="^cango_.*_e").rename(columns=lambda x: x[3:-2]).applymap(recode)

{'decide_arguments',
 'decide_fearful',

 'decide_dailypurchase',
 'decide_familyvisits',
 'decide_healthchild',
 'decide_healthown',
 'decide_majorpurchase',
 'decide_moneyown',
 'decide_pressuretospend'}

 Decide14 = D.filter(regex="^decide_.*_m").rename(columns=lambda x: x[:-2])#~.applymap(recode)
 Decide15 = D.filter(regex="^decide_.*_e").rename(columns=lambda x: x[:-2])#~.applymap(recode)

 Confid14 = D.filter(regex="^conf_.*_m").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Confid15 = D.filter(regex="^conf_.*_e").rename(columns=lambda x: x[3:-2]).applymap(recode)

 if WEEKLY:
     weekly = lambda i: float(i<3) if pd.notnull(i) else np.nan
     Aval2013 = Aval2013.applymap(weekly)
     Aval2014 = Aval2014.applymap(weekly)      
     Aval2015 = Aval2015.applymap(weekly)
 
 index_vars = "worried,portions,fewmeals,nofood,hungry,wholeday".split(",")
 Outcomes = index_vars+["z-score"]
 #~ Creates Year dummies, z-scores and baseline values as `var'2013
 for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
     Aval["Year"]=Year
     if not weekly:
        for var in index_vars:
            Aval[index_vars] = (Aval[index_vars]-Aval[index_vars].mean())/Aval[index_vars].std()
     FS_sum = Aval[index_vars].sum(axis=1)
     Aval["z-score"] = (FS_sum-FS_sum.mean())/FS_sum.std()
     for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
    
 Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
 Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
 Vals = Vals.join(D[["TUP","CSH"]])

 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

 Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 Vals=Vals.loc[2014:2015]
 regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = Vals["TUP"]+Vals["CSH"] ==0
 CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+end_src

** Extras
#+name: get_loglambdas
#+BEGIN_SRC python :noweb no-export :results silent

"""
This code does not yet work in this file. Come back to it...
"""
  df = TUP.process_data(C, HH, T, year = year) #~ Process_data() builds consumption data if not given as an argument
  df['Constant']=1
  df["CTL"] = 1-df["TUP"] #~ Code the cash group as controls since they're not in the midline analysis

  explist=[s[2:-2] for s in df.columns[[s.startswith('c_') and s.endswith(year[1]) for s in df.columns]]]
  df = df.rename(columns= lambda x: x[:-2] if x.endswith(year[1]) else x)

  bothdf=[]
  xvars=['hh_size_b','child_total_b','Loc']
  for x in explist:
      if 'c_'+x+r'_b' not in df:
          #~ When you take out the baseline controls in favor of repeated cross-sections, this is where to start...
          print(x+" has no baseline data or had too few non-zero responses at baseline. Skipping.")
          continue
      ydf=pd.DataFrame(df[['c_'+x]].rename(columns={'c_'+x:x.capitalize()}).stack())
      rdict=dict(zip(xvars+['c_'+x+r'_b'],["%s_%s" % (s,x.capitalize()) for s in xvars]+['Baseline_%s' % x.capitalize()]))
      xdf=pd.DataFrame(df[xvars+['c_'+x+r'_b']])
      xdf.index=pd.MultiIndex.from_tuples([(i,x.capitalize()) for i in xdf.index])
      locations=pd.get_dummies(xdf['Loc'],prefix='Loc_%s' % x.capitalize())
      del xdf['Loc']
      xdf.rename(columns=rdict,inplace=True)
      xdf=xdf.join(locations)
      xdf.replace(to_replace=np.NaN,value=0,inplace=True)

      # Add row to restrict location dummies to sum to one
      ydf=pd.concat([ydf,pd.DataFrame([0],index=[(0,x.capitalize())])])
      xdf=pd.concat([xdf,pd.DataFrame([s.startswith('Loc_')+0. for s in xdf.columns],index=xdf.columns,columns=[(0,x.capitalize())]).T]) 

      xdf[0]=ydf
      xdf.dropna(how='any',inplace=True)
      bothdf.append(xdf)

  #~ Are this fillna() call and the xdf.replace call above a problem? It seems necessary for the block-diagonal ols function
  #~ we're using, but aren't we coding zeros as missing and calculating residuals for only those positive consumption? Wouldn't
  #~ replacing them to zero insert some non-zero residual for households that never consume a given good?
  #~ And isn't this the motivation behind svd_missing?
  mydf=pd.concat(bothdf).fillna(value=0)

  X=mydf.iloc[:,1:]

  y=mydf[[0]]

  x=np.exp(y.unstack().iloc[1:,:]) # Expenditures (in levels)
  xshares=x.divide(x.sum(axis=1),axis=0).fillna(value=0).mean() # Expenditure shares (taking missing as zero)
  xshares.index=xshares.index.droplevel(0)

  b,se=ols(X,y)

  ## betahat=b[['Constant_%s' % s.capitalize() for s in explist]]
  ## betahat.rename(columns=dict(zip(betahat.columns,[s.capitalize() for s in explist])),inplace=True)

  e=y-X.dot(b.T)

  e.rename(columns={0:'Resid'},inplace=True)
  e.index.names=['HH','Good']

  testdf=pd.merge(df[['TUP','CTL']].reset_index(),e.reset_index(),how='outer',on=['HH'])
  testdf.set_index(['HH','Good'],inplace=True)

  TUP=testdf['TUP'].mul(testdf['Resid']).dropna().unstack()
  CTL=testdf['CTL'].mul(testdf['Resid']).dropna().unstack()

  e=(e-e.mean()).unstack()

  # Test of significant differences between treatment and control:
  # Weighting matrix:
  A=np.matrix((TUP-CTL).cov().as_matrix()).I
  g=np.matrix((TUP-CTL).mean())
  J=e.shape[0]*g*A*g.T # Chi2 statistic

  p=1-chi2.cdf(J,e.shape[1])

  chi2test="Chi2 test: %f (%f)" % (J,p)

  N=pd.Series([d.shape[0]-1 for d in bothdf],index=[d.index.levels[1][0] for d in bothdf])

  resultdf=pd.DataFrame({'TUP':TUP.mean(),'CTL':CTL.mean(),'$N$':N})
  sedf=pd.DataFrame({'TUP':TUP.std()/np.sqrt(resultdf['$N$']),'CTL':CTL.std()/np.sqrt(resultdf['$N$'])})
  resultdf['Diff.']=resultdf['TUP']-resultdf['CTL']
  sedf['Diff.']=np.sqrt((sedf['TUP']**2) + (sedf['CTL']**2))

  # Use svd (with missing data) to construct beta & log lambda

  myb,myl = get_loglambdas(e,TEST=True)

  myb.index=myb.index.droplevel(0)

  # Normalize log lambdas
  l=myl/myl.std()
#+END_SRC

#+name: residuals_by_group
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
def residuals_by_group(models, groups, outcomes=[], kind="kde", figure_dir="../figures/", seriesname="Treat", blanks_to = "Control"):
    """
     Takes a set of statsmodels regression results and,
     for each outcome, produces a plot comparing the
     distribution of residuals by group.

     models:
         A dictionary of the form {variable name: sm.RegressionResults}. Empty defaults to all available.
     groups:
         A list, pd.Series, or pd.DataFrame with variables
         (A later version could contain an arbitrary set of categorical and give groups for every combination.)
     outcomes:
         A list specifying which variables in models to make plots for.
     kind:
         kde (or density) or histogram (or hist)
         Density plots are on a single axis. Histograms are stacked by group.
     figure_dir:
         The directory into which the figures get saved. If doesn't exist, throws error (future version might make that directory on the fly.)
     Seriesname:
         If a series or list is passed without a name, defaults to `seriesname'
     blanks_to:
         Observations with no treatment status from "groups" gets renamed to `blanks_to'
    """

    #~ Make outcomes a list. If empty, defaults to all variables in models
    if type(outcomes)==str: outcomes=[outcomes]
    if not outcomes: outcomes = sorted(models.keys())

    #~ Make data frame and make "Group" categorical
    df = pd.DataFrame(groups).rename(columns={0:seriesname})
    for var in df: df[var] = df[var].applymap(lambda x: var if x else "")
    df["Group"] = df.sum(axis=1).replace("",blanks_to)

    #~ Make residuals
    for var in outcomes:
        #~ Make column of residual values
        resid_var = "resid_{}".format(var)
        df[resid_var] = models[var].resid
        #~ Groupby object
        groups = df.dropna(subset=[resid_var]).groupby("Group")[resid_var]

        #~ Plot density by group
        if kind in ("kde", "density"):
            fig, ax = plt.subplots()
            groups.plot(kind=kind, ax=ax, legend=True)
            fig.savefig(figure_dir+resid_var+".png")

        #~ Plot histograms by group
        elif kind in ("hist", "histogram"):
            i=0
            fig, ax = plt.subplots(len(set(df["Group"])),1,sharex=True)
            for group, data in grps[var]:
                ax[i].hist(data.values, bins=20)
                ax[i].set_title(group)
                i+=1
            i=0
            fig.savefig(figure_dir+resid_var+".png")
        print(resid_var+".png created.")


#+end_src
   



 


