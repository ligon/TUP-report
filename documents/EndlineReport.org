#+TITLE: Comparing Cash and Asset Transfers to Low-Income Households in South Sudan
#+AUTHOR: Reajul Chowdhury, Elliott Collins, Ethan Ligon, Munshi Sulaiman
#+DATE: \today
#+OPTIONS: texht:t toc:nil num:nil ':t
#+LATEX_CLASS_OPTIONS: [12pt,article]
#+LATEX_HEADER:       \newcommand{\Eq}[1]{(\ref{eq:#1})}
#+LATEX_HEADER_EXTRA: \newcommand{\Fig}[1]{Figure \ref{fig:#1}} \newcommand{\Tab}[1]{Table \ref{tab:#1}}
#+LaTeX_HEADER:       \addbibresource{./prospectus.bib}
#+LaTeX_HEADER:       \addbibresource{main.bib}
#+LATEX_HEADER_EXTRA: \usepackage{stringstrings} \renewcommand{\cite}[1]{\caselower[q]{#1}\citet{\thestring}}
#+LaTeX_HEADER:       \usepackage[T1]{fontenc}
#+LaTeX_HEADER:       %\usepackage[backend=biber,style=authoryear,natbib=true]{biblatex}

#+begin_abstract
Several previous studies have found that the ``graduation'' or
``Transfers to the Ultra-Poor'' (TUP) framework is an effective
approach to alleviating the constraints that prevent extremely poor
households from increasing their productivity.  The framework consists
of a sizable transfer of productive physical capital, coupled with
training and continuous support over the course of one or two years.
A second and related literature has found some evidence that
unconditional cash transfers (UCT's) may also improve household
productivity and welfare with fewer fixed costs.  Our field experiment
provides a comparison of these two approaches to transferring wealth to
low-income households during the first two years of BRAC's TUP pilot
in South Sudan.  We consider the effect of each on consumption,
income, asset holdings, and a number of intangible outcomes. We also
consider the TUP program's effect on households' responses to the
outbreak of violence in 2014. We find evidence that both types of
transfer have positive effects on consumption, but only in the
short-run.  We find a persistent increase in asset stocks, but only from
the TUP. We also elicit suggestive evidence that BRAC's support may
have helped TUP beneficiaries cope with the short-term economic
effects of the outbreak of violence in 2014. We tentatively conclude
that in this economic context cash can increase household consumption,
but the goal of improving income or wealth is aided by the additional
services that the ultra-poor graduation framework offer.
#+end_abstract
\newpage


* Introduction
 
Poor rural households often earn money from low-return activities like
small-scale cultivation or casual day labor, and face both financial and human
capital constraints, keeping them from investing and expanding into more lucrative
activities. Experience and research over many years has led many to believe that
households facing particularly acute poverty are unable to solve this problem through
the small, high-interest loans typically marketed to them. It was these
considerations that led to the development of the initial ``Targeting the
Ultra-Poor'' (TUP) program in Bangladesh as a supplement or precursor to credit
services. First implemented by BRAC in 2007, the program aims to simultaneously
alleviate physical and human capital constraints by providing households with a
significant transfer of food and productive assets, followed by two years of training
and support by extension officers. (\cite{bandiera2017}) The general framework[fn:: Known as the
``graduation framework'' pointing to the original ambition to move households into an
activity where they are able to finance further income growth without costly
transfers.] has since expanded to a wide range of countries, with a general pattern
of success in increasing aggregate investment, labor supply, and aggregate
consumption. \citep{banerjee2015}

A second, older literature that has gained new interest in recent years studies the
effect of offering poor households direct unconditional cash transfers (UCT's).
\citep{haushofer-shapiro2013} \citep{blattman2014} \citep{blattman2013} While this
and the TUP framework are both direct capital transfer interventions, they are very
different in their approach, with TUP programs guiding and constraining the use of
capital towards productive investment while UCT's allow households to invest, save,
and consume as they see fit. The natural question that arises is how these additional
features and constraints in the TUP framework change how households use their capital
transfers.

To provide some insight into this question, we worked with BRAC's South Sudanese
branch office in Yei county to randomize selection for their pilot TUP program. After
a census identified 745 eligible households, 250 were enrolled in the pilot TUP
program, and another 125 were later given cash roughly equal to the value of the
physical assets provided to TUP households. Another 274 households were assigned to a
control group. While a full body of research exists on each of these two approaches
to wealth transfers, this study provides one of the most direct side-by-side
experimental evaluations to date. 

* The Program
   
The pilot program itself was similar to the other TUP programs completed by BRAC. It
consisted of four phases: targeting and selection, training and enterprise selection,
asset transfers, and monitoring. 

*** Targeting, Selection, & Training

The fist phase of the program was to complete a census of households in the area
around BRAC's office in the town of Yei in Western Equitoria. This census contained
questions to assess eligibility for the program. First, households were excluded if
they had a salaried worker in the household, were participating in another NGO
program, or had no access to cultivable land (which was in some cases necessary for
the program's model). Households were then deemed eligible if they fit at least three
criteria in a list of five poverty indicators:

1) The household head was working as a day laborer (generally an occupation with poverty wages),
2) The household had two or more children,
3) At least one child in the household was working,
4) The household had fewer than three rooms, or
5) A woman in the household had not completed secondary school.

The census was completed in April of 2013 and 745 were identified as eligible. Of
these, 649 were identified in a baseline survey. These households were stratified on
employment, asset ownership, and size and selected into treatment groups. 250 were
enrolled in the TUP program, 125 in the UCT group, and the final 274 in a pure
control group.

*** Asset Transfers & Monitoring

The second phase of the program was training and enterprise selection. Unlike most
programs of this type, the number of households given each kind of asset was set in
advance, with 75 enrolled in agricultural activities (vegetable cultivation), 85 in
duck rearing, 45 in goat rearing, and the rest in small trade businesses. While the
staff tried to map housheolds' asset types to their respective preferences and
skills, a disproportionate number stated preferences for goats and small trade.
Households then attended training sessions. The first of these were for general
business skills around literacy, numeracy, and financial management. The next were
sector specific and focused on how to properly raise livestock or gardens. 

Once training is completed, asset transfers began in late 2013 and continued through
the first few months of 2014. The productive assets related to each enterprise were
valued at around $240 per household, with a random subset recieving an additional $60
in assets later in 2014. Shortly thereafter, households started to attend weekly or
semi-weekly meetings with other nearby participants to discuss with each other and a
BRAC extension officer the details of their businesses. These meetings also included
food transfers for a while, which were designed to help get households to the point
of receiving revenue from their assets without having to sell them.

*** Cash Transfers 

In all, the market value of these food transfers were valued at $110, bringing the
total value of all transfers to $350-$410. The 125 households in the UCT group were
randomly divided in half to receive cash in these two amounts. Unfortunately, political
instability disrupted NGO operations throughtout South Sudan, preventing the
simultaneous disbursal of the cash and asset transfers. Instead, a second survey was
conducted in June of 2014, with the cash transfers being disbursed immediately
thereafter. This resulted in a timing difference of 3 to 6 months between the two
(since the asset transfers were rolled out over around 12 weeks).
This naturally complicates the comparison of treatment effects between the two
treatments. We will describe our empirical strategy in light of these challenges in
the Empirical Strategy section.

Cash transfers were delivered by field officers in person and in local
currency. That they were delivered in cash distinguishes it from
several other cash transfer programs.[fn:: For example,
cite:haushofer-shapiro16 deposited funds in a bank or mobile money
account, while \cite{fafchamps-etal14} used money transfers.]
Experimental studies of the behavioral implications of savings
accounts citep:dupas-robinson13 would lead one to expect that the
method of disbursal might affect the propensity to save a cash
transfer.  In all of these experiments transfers were denominated in
the local currency.  However, our context is unusual in that the local
economy was facing significant inflation at the time, with
year-on-year estimates ranging between 80% and 100%.  This would
naturally make holding cash a less appealing option relative to
consumption or investing in some real asset.

Local community leaders were approached in advance of talking to
individual households to reassure recipients of BRAC's intentions. It
was made clear that the transfers were there for their benefit, that
while they were encouraged to put them towards some productive use,
their use was unrestricted, and that under no circunstances would they
receive a second transfer in the future. BRAC was a well-established
institution in the greater Yei community, so these assurances were
taken as credible and no refused to accept the money.

** The Data

The census was conducted in April of 2013 in the area around BRAC's offices in Yei
County to identify women eligible for participation. A baseline survey was conducted
that Summer, which successfully interviewed 649 of these women and randomly selected
them into the TUP, UCT, and control groups. Half of each beneficiary group was
randomly selected to receive additional "top-up" transfers with market value of $60
(around 20% of the original transfers).

In response to the outbreak of violence in late 2013 and subsequent closing of the
offices in Yei, a midline survey was conducted in June 2014 to try to separate pre-
and post-conflict changes in outcomes. For lack of a valid comparison group, we will
not speak with any authority about the effect of the conflict on economic conditions
in Yei, though we will report estimates of treatment effects on the severity or
likelihood of having been effected exposure to the conflict. Some of the original
asset transfers were done before the office closure, which may affect estimates of
the difference between programs if rates of return changed in the few intervening
months. Finally, an endline survey was conducted in mid-2015 to estimate the effect
of program participation on households' financial situation and overall welfare. The
key here is that the survey conducted in mid-2014 provides us with /short-term/
treatment effects of the TUP program within 6 months of the asset transfers, while
providing a second baseline for the Cash transfers. Likewise, the 2015 survey
allows us to estimate treatment effects one year after the cash transfers, and 15-18
months after the asset transfers.

This unfortunately left us without data past one year for the cash transfer effects.
To get some point estimates on household welfare for this group in the slightly
longer term, we conducted a series of five short surveys on a monthly basis from
November of 2015 to March of 2016. These collected only a subset of the full
consumption modules and a few questions tracking major transactions and shocks. The
short length of the survey allowed them to be administered via the mobile network,
reducing cost and improving response rate. These data provide a slightly longer
estimate of treatment effects for both cash transfers and the TUP program, but will
be especially valuable in assessing whether the cash transfers had a long-term impact
on consumption.

** Empirical Strategy 

For the main panel (excluding the high-frequency mobile surveys), we estimate a
single model using interactions between time effects and group assignment, as well as
baseline values of the outcome variable where available.

\begin{equation*}
Y_{it} =\sum_{t=2014}^{2015}\delta_{t}+\beta_{t}^{Cash}I_{t}*Cash_{it}+\beta_{t}^{TUP}I_{t}*TUP_{it}+\gamma Y_{i,2013}+\epsilon_{i}
\end{equation*}

\noindent
where $\delta_{t}$ are time fixed effects and $I_{t}$ is an indicator if the year
/t/, and $Y_{it}$ is an outcome of interest for household /i/ in year /t/. We take
the interactions of TUP assignment with 2014 and 2015 indicators as the treatment
effects at 6-8 and 15-17 months respectively. The analagous interactions with the
Cash group offer a second baseline and a 12-month treatment effect, respectively.
Since those transfers happened after the midline survey, its interaction with /2014/
acts as a placebo; there is no /ex ante/ reason to expect that they were different
from the rest of the control group at that point. Given the slight difference in
timing, we report a t-test of the hypothesis \(\beta_{TUP,t}-\beta_{Cash,2015}=0\) for
both \(t \in {2014,2015}\). Since the difference in timing is smaller, we consider
\(\beta_{TUP,2015}-\beta_{Cash,2015}=0\) to be the preferred hypothesis, but examine
and report \(\beta_{TUP,2014}-\beta_{Cash,2015}=0\) as well.
 
For the supplementary analysis of the high-frequency panel, we estimate a separate
model, since the underlying data is so different. A constant parameter takes the
place of the fixed effects. We include 2013 levels as a covariate where possible.
Since we collect expenditures on only ten consumption items, we report not only the
total value of spending on those goods, but also a more theoretically grounded
measure described in \cite{collins-ligon17}, which uses the composition of
expenditures to derive the marginal utility of expenditures for each household. We
chose ten relatively demand-elastic items specifically for this purpose, as those
will tend to be the most responsive to changes in welfare. 

******* TODO
        Include a scatter plot of df[10items].sum(1) vs df[AllItems].sum(1) to speak
        to whether that basket tracks "total" spending.
******* 

#+begin_src python :results output
import matplotlib. pyplot as plt
import pandas as pd

# need to get the correct data for 10items and allitems from mobile survey
data = [[1,2], [3,4], [5,6], [2, 12], [5, -4]]
tenitems = pd.DataFrame(data, columns = ['Respondent 1', 'Respondent 2'])
data = [[6,5], [4,3], [2,1], [3, 1], [6, -3]]
AllItems = pd.DataFrame(data, columns = ['Respondent 1', 'Respondent 2'])

X = tenitems.sum(1)
Y = AllItems.sum(1)

X = X.values.tolist()
Y = Y.values.tolist()

plt.scatter(X, Y)
plt.savefig("scatter.png")
plt.show() # can take this out later

#+end_src

#+RESULTS:

#+Caption: Scatter plot of basket vs total spending
#+NAME: fig:Income_group
[[../documents/scatter.png]]

* Results
** Balance on Obervables

We start by checking whether either treatment arm appears significantly different
from the control group in terms of average baseline observable characteristics.
\Tab{balance_check} presents summary statistics by group on a range of factors
related to consumption, asset holdings, and household characteristics.

#+name: balance_check
#+BEGIN_SRC python :dir ../analysis :results value table :exports none
import check_balance
return check_balance.tables
#+END_SRC

#+name: tab:balance_check
#+caption: Means of some analysis variables at baseline.  Asterisks indicate p<.1, .05, and .01 respectively
#+attr_latex: :environment longtable :align lrrrrr
|-----------------+---------+--------------+--------------+-----|
| Consumption     |     CTL | $\Delta$ TUP | $\Delta$ CSH | $N$ |
|-----------------+---------+--------------+--------------+-----|
| Meat            |    4.21 |       -0.568 |       -0.052 | 378 |
| Fuel            |    0.76 |       -0.039 |       -0.072 | 456 |
| Clothesfootwear |    0.67 |       -0.026 |        0.033 | 595 |
| Soap            |    0.48 |       -0.008 |       -0.026 | 536 |
| Fish            |    2.50 |       -0.154 |       -0.156 | 474 |
| Charities       |    0.03 |       -0.006 |          0.0 | 134 |
| Cereals         |    9.19 |       -0.947 |         0.27 | 605 |
| Transport       |    0.18 |       -0.033 |        0.002 | 193 |
| Cosmetics       |    0.68 |        0.027 |       -0.125 | 468 |
| Sugar           |    1.71 |       -0.078 |       -0.189 | 604 |
| Egg             |    1.10 |       -0.091 |        0.038 | 276 |
| Oil             |    1.36 |        -0.13 |       -0.141 | 613 |
| Ceremonies      |    0.13 |        0.006 |        0.026 | 152 |
| Beans           |    0.70 |        0.232 |        0.226 | 192 |
| Fruit           |    0.69 |       -0.089 |        0.001 | 272 |
| Textiles        |    0.16 |       -0.004 |  $0.056^{*}$ | 376 |
| Utensils        |    0.25 |       -0.009 |        0.008 | 442 |
| Dowry           |    1.27 |       -0.041 |        0.028 | 126 |
| Furniture       |    0.20 |       -0.014 |        0.045 | 368 |
| Salt            |    0.45 |       -0.026 |        0.007 | 617 |
| Vegetables      |    1.54 |       -0.165 |        -0.18 | 471 |
|-----------------+---------+--------------+--------------+-----|
| Assets          |     CTL | $\Delta$ TUP | $\Delta$ CSH | $N$ |
|-----------------+---------+--------------+--------------+-----|
| Smallanimals    |  236.60 |      -86.068 |     -123.133 | 123 |
| Bicycle         |  109.08 |      -12.555 |      -11.414 | 171 |
| Radio           |   58.45 |       -5.968 |      -16.529 | 260 |
| Motorcycle      |  341.74 |      192.956 | 353.836^{**} |  93 |
| Net             |   19.16 |        0.668 |        0.247 | 423 |
| Poultry         |   42.40 |       -3.365 |       -8.894 | 161 |
| Bed             |  241.27 |        7.992 |       32.762 | 521 |
| Chairtables     |  206.79 |      -29.368 |        3.617 | 531 |
| Mobile          |   97.54 |       12.627 |       -4.198 | 414 |
| Netitn          |    7.82 |        1.215 |        1.178 | 181 |
| Cosmetics       |    0.68 |        0.027 |       -0.125 | 468 |
|-----------------+---------+--------------+--------------+-----|
| Household       |     CTL | $\Delta$ TUP | $\Delta$ CSH | $N$ |
|-----------------+---------+--------------+--------------+-----|
| Daily Food      |   25.18 |       -2.215 |       -0.261 | 643 |
| Daily Exp       |   29.90 |       -2.167 |       -0.288 | 646 |
| No. Houses      |    2.83 |        0.031 |        0.118 | 543 |
| In Business     |    0.40 |        0.038 |        0.017 | 265 |
| Cereals         |    9.19 |       -0.947 |         0.27 | 605 |
| # Child         |    3.26 |        0.118 |        0.108 | 594 |
| Asset Tot.      | 1757.05 |      -44.791 |       98.654 | 603 |
| Cash Savings    |  236.90 |        28.52 |      -66.812 | 431 |
| HH size         |    7.23 |       -0.175 |          0.3 | 648 |
|-----------------+---------+--------------+--------------+-----|

This is simply suggestive evidence that the treatment and control groups were similar
in observables at baseline, with the exception that the cash group has atypically
more motorcycles and clothing. But it does suggests that our stratified randomization
was not too far from creating comparable groups.

# table 1 redo
#+name: mean_balance
#+begin_src python :noweb no-export :exports none  :results output table raw labels=[] :colnames no
<<load_data>>

# combine low asset and high asset as TUP
df.replace(to_replace='Low asset', value='TUP', inplace=True)
df.replace(to_replace='High Asset', value='TUP', inplace=True)

df = df.get(['c_meat_b', 'c_fuel_b', 'c_clothesfootwear_b', 'c_soap_b',
             'c_fish_b', 'c_charities_b', 'c_cereals_b', 'c_transport_b',
             'c_cosmetics_b', 'c_sugar_b', 'c_egg_b', 'c_oil_b',
             'c_ceremonies_b', 'c_beans_b', 'c_fruit_b', 'c_textiles_b',
             'c_utensils_b', 'c_dowry_b', 'c_furniture_b', 'c_salt_b',
             'c_vegetables_b', 
             'asset_val_smallanimals_b', 'asset_val_bicycle_b', 'asset_val_radio_b',
             'asset_val_motorcycle_b', 'asset_val_net_b', 'asset_val_poultry_b',
             'asset_val_bed_b', 'asset_val_chairtables_b', 'asset_val_mobile_b',
             'asset_val_netITN_b',
             'group'])

means = df.groupby(['group']).mean()
means = means.T

# cosmetics repeated in consumption and assets
# create new variables for household section

means = df.groupby(['group']).mean()
means = means.T

ses = df.groupby(['group']).sem()


newdf = df_to_orgtbl(means, float_fmt = '%.3f')
print(newdf)


#+end_src

#+results: mean_balance
|                          | Control |     TUP |    Cash |
|--------------------------+---------+---------+---------|
| c_meat_b                 |  12.498 |  10.822 |  12.360 |
| c_fuel_b                 |  23.396 |  21.518 |  20.536 |
| c_clothesfootwear_b      | 249.849 | 229.016 | 250.080 |
| c_soap_b                 |  14.947 |  14.137 |  13.600 |
| c_fish_b                 |   7.363 |   6.976 |   7.024 |
| c_charities_b            |   9.584 |   9.349 |  10.704 |
| c_cereals_b              |  27.110 |  24.602 |  28.304 |
| c_transport_b            |   5.510 |   4.245 |   5.296 |
| c_cosmetics_b            |  19.955 |  21.064 |  16.568 |
| c_sugar_b                |   5.024 |   4.980 |   4.552 |
| c_egg_b                  |   3.020 |   3.040 |   3.384 |
| c_oil_b                  |   3.988 |   3.679 |   3.648 |
| c_ceremonies_b           |  34.490 |  54.108 |  57.264 |
| c_beans_b                |   2.053 |   2.727 |   2.744 |
| c_fruit_b                |   1.922 |   1.807 |   2.056 |
| c_textiles_b             |  54.682 |  54.470 |  75.704 |
| c_utensils_b             |  92.759 |  85.627 |  92.080 |
| c_dowry_b                | 437.845 | 473.293 | 464.840 |
| c_furniture_b            |  69.967 |  65.851 |  87.080 |
| c_salt_b                 |   1.265 |   1.261 |   1.360 |
| c_vegetables_b           |   4.604 |   4.092 |   4.056 |
| asset_val_smallanimals_b | 258.306 | 147.510 | 112.560 |
| asset_val_bicycle_b      | 111.224 |  96.390 | 100.880 |
| asset_val_radio_b        |  58.771 |  51.506 |  41.824 |
| asset_val_motorcycle_b   | 351.086 | 523.956 | 690.008 |
| asset_val_net_b          |  18.980 |  20.177 |  19.336 |
| asset_val_poultry_b      |  44.551 |  38.554 |  33.240 |
| asset_val_bed_b          | 233.865 | 247.269 | 273.840 |
| asset_val_chairtables_b  | 208.559 | 175.542 | 209.520 |
| asset_val_mobile_b       |  92.727 | 109.558 |  92.592 |
| asset_val_netITN_b       |   7.951 |   8.956 |   8.928 |

#+RESULTS:
|                          | Control |     TUP |    Cash |
|--------------------------+---------+---------+---------|
| c_meat_b                 |  12.498 |  10.822 |  12.360 |
| c_fuel_b                 |  23.396 |  21.518 |  20.536 |
| c_clothesfootwear_b      | 249.849 | 229.016 | 250.080 |
| c_soap_b                 |  14.947 |  14.137 |  13.600 |
| c_fish_b                 |   7.363 |   6.976 |   7.024 |
| c_charities_b            |   9.584 |   9.349 |  10.704 |
| c_cereals_b              |  27.110 |  24.602 |  28.304 |
| c_transport_b            |   5.510 |   4.245 |   5.296 |
| c_cosmetics_b            |  19.955 |  21.064 |  16.568 |
| c_sugar_b                |   5.024 |   4.980 |   4.552 |
| c_egg_b                  |   3.020 |   3.040 |   3.384 |
| c_oil_b                  |   3.988 |   3.679 |   3.648 |
| c_ceremonies_b           |  34.490 |  54.108 |  57.264 |
| c_beans_b                |   2.053 |   2.727 |   2.744 |
| c_fruit_b                |   1.922 |   1.807 |   2.056 |
| c_textiles_b             |  54.682 |  54.470 |  75.704 |
| c_utensils_b             |  92.759 |  85.627 |  92.080 |
| c_dowry_b                | 437.845 | 473.293 | 464.840 |
| c_furniture_b            |  69.967 |  65.851 |  87.080 |
| c_salt_b                 |   1.265 |   1.261 |   1.360 |
| c_vegetables_b           |   4.604 |   4.092 |   4.056 |
| asset_val_smallanimals_b | 258.306 | 147.510 | 112.560 |
| asset_val_bicycle_b      | 111.224 |  96.390 | 100.880 |
| asset_val_radio_b        |  58.771 |  51.506 |  41.824 |
| asset_val_motorcycle_b   | 351.086 | 523.956 | 690.008 |
| asset_val_net_b          |  18.980 |  20.177 |  19.336 |
| asset_val_poultry_b      |  44.551 |  38.554 |  33.240 |
| asset_val_bed_b          | 233.865 | 247.269 | 273.840 |
| asset_val_chairtables_b  | 208.559 | 175.542 | 209.520 |
| asset_val_mobile_b       |  92.727 | 109.558 |  92.592 |
| asset_val_netITN_b       |   7.951 |   8.956 |   8.928 |



** Attrition

Another crucial question is to what extent attrition in 2014 and 2015 will bias our
results. \Tab{attrition_count} reports the total number of households identified in
each treatment arm and year for the whole sample. \Tab{attrition_balanced_count}
reports the same numbers restricting ourselves to households with baseline surveys.
In the TUP group, we were unable to find 21 participants in 2014 (8% attrition), but
found 5 not identified in the baseline survey. We found 8 additional TUP households
with baseline surveys again in 2015 for a final attrition rate of 5%. Of those in the
Cash group, 12 were lost (9.6%) in 2014, then two more in 2015 (11%). The control
group saw very high attrition in 2014, missing 60 people (22%). Enumerators also found
a large number of households not found at baseline, such that there were only 16
fewer surveys collected in 2014 than in 2013. The high attrition was due largely to
the fact that these households did not enjoy the same consistent contact that BRAC had
with the TUP group, and the local area lacked infrastructure to easily locate people.
This was exacerbated by the uncertain political situation and early harvest.
Attrition in 2015 was 6.7%, with 19 baseline control households not responding, with
85 households found who were originally identified as control households from the
survey but not in the baseline survey. In order to take advantage of the households
not included in the baseline, the main specification below follows
\cite{banerjee-etal} by setting missing values to zero when controlling for baseline
levels of the dependent variable, and including an indicator for
whether the household was in the baseline.

   
\newpage

#+name: attrition_check
#+BEGIN_SRC python :dir ../analysis :results value table :exports none
import check_attrition
return check_attrition.TAB
#+END_SRC

#+caption: Total number of households in sample by group and round
#+attr_latex: :environment longtable :align lrrrrr
#+name: tab:attrition_count
|-------------+-------+-------+-------|
| Full Sample |  2013 |  2014 |  2015 |
|-------------+-------+-------+-------|
| Cash        | 124.0 | 113.0 | 111.0 |
| Control     | 281.0 | 265.0 | 347.0 |
| TUP         | 244.0 | 228.0 | 236.0 |
|-------------+-------+-------+-------|
| All         | 649.0 | 606.0 | 694.0 |
|-------------+-------+-------+-------|

#+caption: Number of households in sample with baseline survey by group and round
#+name: tab:attrition_balanced_count
|-----------------+---------+---------+---------|
| Balanced Sample |    2013 |    2014 |    2015 |
|-----------------+---------+---------+---------|
| Cash            | 124.000 | 112.000 | 110.000 |
| Control         | 281.000 | 219.000 | 262.000 |
| TUP             | 244.000 | 223.000 | 231.000 |
|-----------------+---------+---------+---------|
| All             | 649.000 | 554.000 | 603.000 |
|-----------------+---------+---------+---------|

*************** Further checks on assignment and samples
#+begin_src python :results output raw table
import pandas as pd
from cfe.df_utils import df_to_orgtbl
import sys
sys.path.append('../analysis')
from check_attrition import D

R=pd.read_stata('../../TUP-data/random_assignment.dta')
R=R.set_index('respid')      
R.index.name = 'idno'

A=pd.DataFrame({'D':D['group'],'R':R['group'],'B':D['Base'],'M':D['Mid'],'E':D['End']})

# Fix issue with variant capitalizations
A.D=A.D.str.title()
A.R=A.R.str.title()

# Examine observations where assignments differ:
print(df_to_orgtbl(A[A['D']!=A['R']]))
#+end_src

#+results:
|        | D       | R          | B         | M         | E         |
|--------+---------+------------+-----------+-----------+-----------|
| 1042.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1106.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1154.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1157.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1186.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1190.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1207.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1217.0 | Control | Cash       | \(1.000\) | \(1.000\) | \(1.000\) |
| 1222.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1248.0 | ---     | High Asset | ---       | ---       | ---       |
| 1252.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 1264.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1285.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1293.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 1295.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1300.0 | Control | Cash       | \(0.000\) | \(0.000\) | \(1.000\) |
| 1305.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1311.0 | Control | ---        | \(0.000\) | \(1.000\) | \(0.000\) |
| 1314.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1315.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1316.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1320.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1338.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1348.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1350.0 | Control | ---        | \(0.000\) | \(1.000\) | \(0.000\) |
| 1352.0 | Control | High Asset | \(1.000\) | \(1.000\) | \(1.000\) |
| 1368.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1403.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 1421.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1436.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1453.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1476.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1480.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1486.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1496.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1502.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1505.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1507.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1548.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1561.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1591.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1619.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1642.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 1667.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1679.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1690.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1702.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1703.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1730.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1731.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1744.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1747.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1748.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1749.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1750.0 | Control | ---        | \(1.000\) | \(0.000\) | \(0.000\) |
| 1755.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1773.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1780.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1792.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1797.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1802.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1813.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1814.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1818.0 | Control | Low Asset  | \(1.000\) | \(1.000\) | \(1.000\) |
| 1826.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1878.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 1892.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1894.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1910.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1915.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1922.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1927.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1950.0 | Control | ---        | \(0.000\) | \(1.000\) | \(0.000\) |
| 1956.0 | Control | Low Asset  | \(1.000\) | \(1.000\) | \(1.000\) |
| 1967.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1988.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1991.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1994.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1995.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2002.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2012.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2024.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2030.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2032.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 2039.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2047.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2051.0 | Control | High Asset | \(1.000\) | \(1.000\) | \(1.000\) |
| 2061.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2066.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2071.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2077.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2079.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2082.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2094.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 2103.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2106.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2107.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2114.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2115.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2139.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2160.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2205.0 | Control | ---        | \(1.000\) | \(0.000\) | \(0.000\) |
| 2214.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2225.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2226.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 2232.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2234.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2238.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2239.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2250.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2260.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2261.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 2267.0 | Control | Low Asset  | \(1.000\) | \(1.000\) | \(1.000\) |
| 2268.0 | Control | ---        | \(1.000\) | \(0.000\) | \(0.000\) |
| 2269.0 | Control | ---        | \(1.000\) | \(0.000\) | \(0.000\) |
| 2270.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |

We see seven cases in which there's some discrepancy in the assignment, and 107 cases in which the 
dataframe =D= has a household assigned to "Control" but where =random_assignment= has no assignment recorded. 
*************** END


# table 2 and 3 redo
#+begin_src python :exports none  :results output table raw labels=[] :colnames no
data_baseline  = "../../TUP-data/Baseline/TUP_baseline.dta"
data_assignment  = "../../TUP-data/random_assignment.dta"
data_midline = "../../TUP-data/Midline/TUP_midline.dta"
data_endline = "../../TUP-data/Endline/TUP_endline.dta"
import pandas as pd
import numpy as np
from cfe.df_utils import df_to_orgtbl

df_b = pd.read_stata(data_baseline)
df_a = pd.read_stata(data_assignment)
df_m = pd.read_stata(data_midline)
df_e = pd.read_stata(data_endline)

# baseline (2013)
# merge baseline data id number and group column from random assignment data based on respondent id/id number
df = pd.merge(df_b[ ['idno'] ], df_a[ ['respid', 'group'] ], how='inner', left_on="idno", right_on="respid")

# combine low asset and high asset as TUP
df.replace(to_replace='Low asset', value='TUP', inplace=True)
df.replace(to_replace='High Asset', value='TUP', inplace=True)

bygroupdf = df.groupby(['group']).count()

d2013 = [bygroupdf.at['Cash','idno'],bygroupdf.at['Control','idno'], bygroupdf.at['TUP','idno'], df.group.notna().sum()]

# midline (2014)
# merge midline data id number and group column from random assignment data based on respondent id/id number
df = pd.merge(df_m[ ['idno'] ], df_a[ ['respid', 'group'] ], how='inner', left_on="idno", right_on="respid")

# combine low asset and high asset as TUP
df.replace(to_replace='Low asset', value='TUP', inplace=True)
df.replace(to_replace='High Asset', value='TUP', inplace=True)

bygroupdf = df.groupby(['group']).count()

d2014 = [bygroupdf.at['Cash','idno'],bygroupdf.at['Control','idno'], bygroupdf.at['TUP','idno'], df.group.notna().sum()]

# endline (2015)
# merge endline data id number and group column from random assignment data based on respondent id/id number
df = pd.merge(df_e[ ['idno'] ], df_a[ ['respid', 'group'] ], how='inner', left_on="idno", right_on="respid")

# combine low asset and high asset as TUP
df.replace(to_replace='Low asset', value='TUP', inplace=True)
df.replace(to_replace='High Asset', value='TUP', inplace=True)

bygroupdf = df.groupby(['group']).count()

d2015 = [bygroupdf.at['Cash','idno'],bygroupdf.at['Control','idno'], bygroupdf.at['TUP','idno'], df.group.notna().sum()]

d = {'2013': d2013, '2014': d2014, '2015': d2015}
table = pd.DataFrame(data = d) 


newdf = df_to_orgtbl(table, float_fmt = '%.3f')
print(newdf)

  #+end_src

#+RESULTS:
|   |    2013 |    2014 |    2015 |
|---+---------+---------+---------|
| 0 | 125.000 | 114.000 | 113.000 |
| 1 | 245.000 | 206.000 | 239.000 |
| 2 | 249.000 | 233.000 | 241.000 |
| 3 | 619.000 | 553.000 | 593.000 |


Next we ask how those who did not turn up in subsequent rounds differed by a range of
baseline characteristics. \Tab{attrition_balance} reports the average level of
various characteristics in 2013. Then we report the difference in means between
households that were in and out of the midline or endline surveys.
Here we see that overall, households found in the mideline survey were larger with
more children and larger reported asset stocks. Households found in 2015 seemed to
have, at baseline, significantly smaller asset stocks and less consumption. This
imbalance highlights the need for a difference-in-difference or ancova design.

#+caption: Means of household baseline characteristics and regression coefficients for whether they were ultimately found at baseline or endline. (Note that this does not consider households found only in 2014 or 2015).
#+attr_latex: :environment longtable :align lrrrrr
#+name: tab:attrition_balance
|-----------------+---------------+---------------+---------------|
| HH Features     | $Mean_{Bsln}$ | $\beta_{Mid}$ | $\beta_{End}$ |
|-----------------+---------------+---------------+---------------|
| HH size         |         7.223 |       0.595** |         0.428 |
| # Child         |         3.328 |      0.656*** |         0.423 |
| Asset Prod.     |       512.822 |       126.360 |      -369.190 |
| Asset Tot.      |      1494.324 |       361.889 |     -689.174* |
| Daily Exp       |        25.212 |         1.257 |        -4.150 |
| Daily Food      |        24.300 |         0.299 |       -4.790* |
| In Business     |         0.415 |         0.038 |         0.007 |
| Land Access     |         2.324 |         0.014 |         0.305 |
| No. Houses      |         2.863 |         0.305 |         0.367 |
| Cash Savings    |       178.662 |        46.322 |        54.295 |
|-----------------+---------------+---------------+---------------|
| Assets          |               |               |               |
|-----------------+---------------+---------------+---------------|
| Bed             |       250.534 |        12.649 |       -51.133 |
| Bicycle         |       102.174 |        11.179 |         4.212 |
| Mobile          |       101.482 |         6.336 |       -13.028 |
| Motorcycle      |       481.885 |       213.002 |      -241.819 |
| Carts           |         2.751 |         1.929 |         2.962 |
| Cows            |       181.402 |        67.862 |       -89.273 |
| Smallanimals    |       180.716 |        18.966 |       -79.014 |
|-----------------+---------------+---------------+---------------|
| Consumption     |               |               |               |
|-----------------+---------------+---------------+---------------|
| Cereals         |         8.882 |        -0.084 |      -3.714** |
| Beans           |         0.826 |         0.269 |        -0.382 |
| Ceremonies      |         0.141 |        -0.020 |        -0.038 |
| Charities       |         0.027 |         0.007 |        -0.001 |
| Clothesfootwear |         0.663 |        0.180* |        -0.206 |
| Cosmetics       |         0.668 |         0.005 |         0.229 |
| Dowry           |         1.263 |         0.755 |        -0.399 |
| Egg             |         1.069 |        -0.005 |         0.106 |
| Fish            |         2.417 |        -0.132 |         0.036 |
| Fruit           |         0.656 |         0.009 |        -0.151 |
| Fuel            |         0.733 |         0.105 |        -0.049 |
| Meat            |         3.981 |         0.254 |         0.300 |
| Other           |           0.0 |         0.000 |         0.000 |
| Poultry         |        39.437 |       23.634* |        -2.243 |
| Salt            |         0.438 |     -0.140*** |        -0.043 |
| Soap            |         0.475 |       -0.181* |         0.047 |
| Sugar           |         1.647 |        -0.285 |        -0.020 |
| Textiles        |         0.165 |         0.010 |         0.011 |
| Transport       |         0.163 |         0.004 |         0.018 |
| Tv              |        39.915 |       -16.377 |         0.845 |
| Utensils        |         0.247 |         0.062 |        -0.023 |
| Vegetables      |         1.446 |         0.096 |        -0.151 |
|-----------------+---------------+---------------+---------------|

** Consumption

The first measure of welfare we consider is household consumption, defined as the
market value of goods or services used by the household. A sizable basket of goods
were included in the survey module. These are separated into three categories: Food
items (with a 3-day recall window), non-durables (a 30-day recall window), and
durables and large expenditures (a one-year recall window). This is perhaps the most
appropriate measure of the welfare or poverty of a household in our survey. 

The results for several important consumption measures are presented in Table
\ref{tab:consumption}. Importantly, we do not know about prices for each good in this
time, though we can say that inflation was as high as 100% between 2014 and 2015. We
take the sum of all consumption and expenditure questions together as a measure of
welfare. In light of the fact that we have data on an incomplete basket, we also
follow Collins and Ligon (2015), which details a method for deriving treatment
effects on a structurally estimated index of households' marginal utility, which we
include here as \(\log\lambda_{it}\).

We measured the effect of cash on consumption 12 months after the fact, where we find
a statistically significant increase of 18 SSP per day, representing a roughly 16%
increase over the control-group average. We first measured the consumption effect of
the TUP program 6-9 months after the asset transfers, where we found a similarly
sized effect of 18SSP. However we find no evidence of a consumption effect 18-21
months after the transfers (and reject equality with the 12-month cash results). This
result leaves open the question of whether the cash treatment had a persistent effect
on consumption, or whether the short-term effect found in 2015 is similarly
temporary. It was this question that motivated the collection of an additional five
rounds of data over a 6-month period in late 2015 and early 2016, in which we asked
about ten items, five food and five non-food. In Table \Tab{tab:mobile_consumption},
we consider the average treatment effect on households sampled for these phone
interviews, both for \(\log\lambda_{it}\) and for total consumption of this basket of
goods. We find that, consistent with the TUP program's results in 2015, all evidence
of an effect seem to be gone by 18th months after the transfer date.

These results are consistent with a story in which either sort of transfer has a
short-term consumption effect, but with the result fading in the year after. In
either group, the increase in total consumption appears to be driven mainly by
increased food consumption, with smaller effects on non-food consumption goods and
durables. As such, there is no evidence that the share of food consumed falls, as
might be predicted by Engel's law.

\newpage

#+name: consumption_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<consumption_analysis>>
return tab
#+end_src

#+RESULTS: consumption_results with Treatment Group Fixed Effects
|----------------------------------+-------------------+-----------------+------------------+-----------------|
|                                  | Tot               | logTot          | Food             | FoodShr         |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| CTL mean                         | \(115.404\)       | \(4.509^{***}\) | \(38.468\)       | \(0.333^{***}\) |
|                                  | \((78.750)\)      | \((0.756)\)     | \((26.250)\)     | \((0.000)\)     |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| CSH*2014                         | \(-7.838\)        | \(-0.049\)      | \(-2.613\)       | \(0.000\)       |
|                                  | \((5.891)\)       | \((0.058)\)     | \((1.964)\)      | \((0.000)\)     |
| CSH*2015                         | \(12.931^{**}\)   | \(0.104^{*}\)   | \(4.310^{**}\)   | \(-0.000\)      |
|                                  | \((5.811)\)       | \((0.057)\)     | \((1.937)\)      | \((0.000)\)     |
| TUP*2014                         | \(11.000^{**}\)   | \(0.126^{***}\) | \(3.667^{**}\)   | \(0.000^{***}\) |
|                                  | \((4.681)\)       | \((0.046)\)     | \((1.560)\)      | \((0.000)\)     |
| TUP*2015                         | \(-3.410\)        | \(-0.040\)      | \(-1.137\)       | \(0.000^{***}\) |
|                                  | \((4.547)\)       | \((0.045)\)     | \((1.516)\)      | \((0.000)\)     |
| 2014                             | \(76.831^{***}\)  | \(3.931^{***}\) | \(25.610^{***}\) | \(0.278^{***}\) |
|                                  | \((5.318)\)       | \((0.113)\)     | \((1.773)\)      | \((0.000)\)     |
| 2015                             | \(105.702^{***}\) | \(4.175^{***}\) | \(35.234^{***}\) | \(0.278^{***}\) |
|                                  | \((5.001)\)       | \((0.111)\)     | \((1.667)\)      | \((0.000)\)     |
| TUP                              | \(7.590^{**}\)    | \(0.086^{***}\) | \(2.530^{**}\)   | \(0.000^{***}\) |
|                                  | \((3.012)\)       | \((0.030)\)     | \((1.004)\)      | \((0.000)\)     |
| CSH                              | \(5.093\)         | \(0.056\)       | \(1.698\)        | \(0.000^{***}\) |
|                                  | \((3.774)\)       | \((0.037)\)     | \((1.258)\)      | \((0.000)\)     |
| Bsln2013                         | \(0.081^{**}\)    | \(0.073^{***}\) | \(0.081^{**}\)   | \(0.167^{***}\) |
|                                  | \((0.038)\)       | \((0.026)\)     | \((0.038)\)      | \((0.000)\)     |
| Bsln NA                          | \(20.521^{***}\)  | \(0.447^{***}\) | \(6.840^{***}\)  | \(0.056^{***}\) |
|                                  | \((6.964)\)       | \((0.121)\)     | \((2.321)\)      | \((0.000)\)     |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | \(-1.931\)        | \(0.022\)       | \(-0.644\)       | \(0.000^{**}\)  |
|                                  | \((8.377)\)       | \((0.083)\)     | \((2.792)\)      | \((0.000)\)     |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | \(-16.341^{***}\) | \(-0.145^{**}\) | \(-5.447^{***}\) | \(0.000^{***}\) |
|                                  | \((6.057)\)       | \((0.060)\)     | \((2.019)\)      | \((0.000)\)     |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| F-stat                           | \(10.142\)        | \(8.131\)       | \(10.142\)       | \(-212.981\)    |
| N                                | \(1291.000\)      | \(1291.000\)    | \(1291.000\)     | \(1291.000\)    |
|----------------------------------+-------------------+-----------------+------------------+-----------------|

#+caption: Average treatment effects by Group-Year, controlling for baseline levels.
#+attr_latex: :environment longtable :align lrrrrrrr
#+name: tab:consumption
|----------------------------------+-------------------+-----------------+------------------+-----------------|
|                                  | Tot               | logTot          | Food             | FoodShr         |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| CTL mean                         | \(115.404\)       | \(4.509^{***}\) | \(38.468\)       | \(0.333^{***}\) |
|                                  | \((78.750)\)      | \((0.756)\)     | \((26.250)\)     | \((0.000)\)     |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| CSH*2014                         | \(-2.745\)        | \(0.007\)       | \(-0.915\)       | \(0.000^{***}\) |
|                                  | \((8.008)\)       | \((0.079)\)     | \((2.669)\)      | \((0.000)\)     |
| CSH*2015                         | \(18.023^{**}\)   | \(0.160^{**}\)  | \(6.008^{**}\)   | \(-0.000\)      |
|                                  | \((7.831)\)       | \((0.077)\)     | \((2.610)\)      | \((0.000)\)     |
| TUP*2014                         | \(18.590^{***}\)  | \(0.212^{***}\) | \(6.197^{***}\)  | \(0.000^{***}\) |
|                                  | \((6.426)\)       | \((0.063)\)     | \((2.142)\)      | \((0.000)\)     |
| TUP*2015                         | \(4.179\)         | \(0.045\)       | \(1.393\)        | \(0.000^{***}\) |
|                                  | \((6.130)\)       | \((0.060)\)     | \((2.043)\)      | \((0.000)\)     |
| 2014                             | \(76.831^{***}\)  | \(3.931^{***}\) | \(25.610^{***}\) | \(0.278^{***}\) |
|                                  | \((5.318)\)       | \((0.113)\)     | \((1.773)\)      | \((0.000)\)     |
| 2015                             | \(105.702^{***}\) | \(4.175^{***}\) | \(35.234^{***}\) | \(0.278^{***}\) |
|                                  | \((5.001)\)       | \((0.111)\)     | \((1.667)\)      | \((0.000)\)     |
| Bsln2013                         | \(0.081^{**}\)    | \(0.073^{***}\) | \(0.081^{**}\)   | \(0.167^{***}\) |
|                                  | \((0.038)\)       | \((0.026)\)     | \((0.038)\)      | \((0.000)\)     |
| Bsln NA                          | \(20.521^{***}\)  | \(0.447^{***}\) | \(6.840^{***}\)  | \(0.056^{***}\) |
|                                  | \((6.964)\)       | \((0.121)\)     | \((2.321)\)      | \((0.000)\)     |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | \(0.566\)         | \(0.052\)       | \(0.189\)        | \(0.000^{***}\) |
|                                  | \((9.994)\)       | \((0.098)\)     | \((3.331)\)      | \((0.000)\)     |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | \(-13.844^{*}\)   | \(-0.115\)      | \(-4.615^{*}\)   | \(0.000^{***}\) |
|                                  | \((8.125)\)       | \((0.080)\)     | \((2.708)\)      | \((0.000)\)     |
|----------------------------------+-------------------+-----------------+------------------+-----------------|
| F-stat                           | \(10.142\)        | \(8.131\)       | \(10.142\)       | \(-210.422\)    |
| N                                | \(1291.000\)      | \(1291.000\)    | \(1291.000\)     | \(1291.000\)    |
|----------------------------------+-------------------+-----------------+------------------+-----------------|

#+name: mobile_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<mobile_analysis>>
return mtab
#+end_src

\newpage

#+caption: Average treatment effects using mobile data collection (results are robust to controlling for baseline levels)
#+attr_latex: :environment longtable :align lrrrrr
#+name: tab:mobile_consumption
|---------------------------+--------------------+----------------+---------------|
|                           | $\log\lambda_{it}$ | Tot            | logTot        |
|---------------------------+--------------------+----------------+---------------|
| CTL mean                  | $-0.018$           | $30.851$       | $3.158^{***}$ |
|                           | $(1.001)$          | $(27.768)$     | $(0.734)$     |
|---------------------------+--------------------+----------------+---------------|
| TUP                       | $0.023$            | $-0.624$       | $-0.011$      |
|                           | $(0.041)$          | $(1.152)$      | $(0.030)$     |
| CSH                       | $0.056$            | $0.776$        | $0.028$       |
|                           | $(0.052)$          | $(1.459)$      | $(0.038)$     |
| const                     | $-0.018$           | $30.851^{***}$ | $3.158^{***}$ |
|                           | $(0.027)$          | $(0.753)$      | $(0.020)$     |
|---------------------------+--------------------+----------------+---------------|
| $\beta^{TUP}-\beta^{CSH}$ | $-0.033$           | $-1.399$       | $-0.039$      |
|                           | $(0.055)$          | $(1.524)$      | $(0.040)$     |
|---------------------------+--------------------+----------------+---------------|
| F-stat                    | $0.584$            | $0.434$        | $0.475$       |
| N                         | $2877.000$         | $2878.000$     | $2878.000$    |
|---------------------------+--------------------+----------------+---------------|

** Food Insecurity

Observed changes in total consumption don't translate into an increase in reported
food security. In each year, we ask how often in a given week the respondent has had
experiences indicative of food insecurity. Included are (from left to right) going a
whole day without eating, going to sleep hungry, being without any food in the house,
eating fewer meals than normal at mealtimes, and limiting portions. We report the
percentage of people who report experiencing each in a typical week, as well as a
standardized composite z-score using all of these questions. Coefficients and
standard errors are both small relative to average levels and statistically
insignificant. The first column also aggregates these values as a weighted average
with inverse covariance weighting, yielding an index we call Food Insecurity. Again,
we find no systematic difference in food security.

#+name: foodsecure_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<foodsecure_analysis>>
return tab
#+end_src

|----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------|
|                                  | Food Insecurity | Whole Day       | Hungry          | No Food         | Few Meals       | Portions        |
|----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------|
| CTL mean                         | \(0.024\)       | \(0.790^{*}\)   | \(0.797^{**}\)  | \(0.716\)       | \(0.679\)       | \(0.638\)       |
|                                  | \((0.713)\)     | \((0.408)\)     | \((0.403)\)     | \((0.452)\)     | \((0.467)\)     | \((0.481)\)     |
|----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------|
| CSH*2014                         | \(0.044\)       | \(0.004\)       | \(0.040\)       | \(0.011\)       | \(0.032\)       | \(0.004\)       |
|                                  | \((0.076)\)     | \((0.041)\)     | \((0.039)\)     | \((0.043)\)     | \((0.050)\)     | \((0.051)\)     |
| CSH*2015                         | \(-0.069\)      | \(-0.063\)      | \(-0.032\)      | \(0.014\)       | \(0.003\)       | \(0.034\)       |
|                                  | \((0.074)\)     | \((0.040)\)     | \((0.039)\)     | \((0.042)\)     | \((0.049)\)     | \((0.050)\)     |
| TUP*2014                         | \(0.058\)       | \(0.017\)       | \(0.052^{*}\)   | \(0.024\)       | \(-0.009\)      | \(-0.012\)      |
|                                  | \((0.061)\)     | \((0.033)\)     | \((0.032)\)     | \((0.035)\)     | \((0.040)\)     | \((0.041)\)     |
| TUP*2015                         | \(-0.005\)      | \(-0.030\)      | \(0.006\)       | \(0.030\)       | \(-0.054\)      | \(0.016\)       |
|                                  | \((0.058)\)     | \((0.031)\)     | \((0.030)\)     | \((0.033)\)     | \((0.038)\)     | \((0.039)\)     |
| 2014                             | \(-0.040\)      | \(0.914^{***}\) | \(0.904^{***}\) | \(0.915^{***}\) | \(0.834^{***}\) | \(0.779^{***}\) |
|                                  | \((0.043)\)     | \((0.024)\)     | \((0.023)\)     | \((0.026)\)     | \((0.033)\)     | \((0.031)\)     |
| 2015                             | \(-0.002\)      | \(0.784^{***}\) | \(0.793^{***}\) | \(0.736^{***}\) | \(0.703^{***}\) | \(0.609^{***}\) |
|                                  | \((0.038)\)     | \((0.022)\)     | \((0.022)\)     | \((0.024)\)     | \((0.031)\)     | \((0.029)\)     |
| Bsln2013                         | \(-0.081^{**}\) | \(0.002\)       | \(-0.017\)      | \(-0.032\)      | \(-0.056^{**}\) | \(0.023\)       |
|                                  | \((0.034)\)     | \((0.023)\)     | \((0.021)\)     | \((0.023)\)     | \((0.027)\)     | \((0.027)\)     |
| Bsln_NAN                         | \(0.109^{**}\)  | \(0.022\)       | \(0.033\)       | \(-0.038\)      | \(0.019\)       | \(0.083^{**}\)  |
|                                  | \((0.053)\)     | \((0.031)\)     | \((0.032)\)     | \((0.035)\)     | \((0.043)\)     | \((0.041)\)     |
|----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | \(0.128\)       | \(0.080\)       | \(0.085^{*}\)   | \(0.011\)       | \(-0.013\)      | \(-0.046\)      |
|                                  | \((0.095)\)     | \((0.051)\)     | \((0.049)\)     | \((0.054)\)     | \((0.062)\)     | \((0.064)\)     |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | \(0.064\)       | \(0.033\)       | \(0.038\)       | \(0.017\)       | \(-0.057\)      | \(-0.018\)      |
|                                  | \((0.078)\)     | \((0.042)\)     | \((0.040)\)     | \((0.044)\)     | \((0.051)\)     | \((0.052)\)     |
|----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------|
| F-stat                           | \(1.742\)       | \(9.300\)       | \(8.328\)       | \(10.945\)      | \(6.700\)       | \(5.879\)       |
| N                                | \(1255.000\)    | \(1276.000\)    | \(1291.000\)    | \(1287.000\)    | \(1291.000\)    | \(1286.000\)    |
|----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------|



#+name: tab:foodsecure
#+caption: Percentage of respondents reporting a food security problem occurs at least once a week.
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+-------------+--------------+--------------+--------------+--------------+--------------|
|                                  | Z-score     | Whole Day    | Hungry       | No Food      | Fewmeals     | Portions     |
|----------------------------------+-------------+--------------+--------------+--------------+--------------+--------------|
| CTL mean                         | $-0.01$     | $0.21$       | $0.21$       | $0.28$       | $0.32$       | $0.36$       |
|                                  | $( 1.00)$   | $( 0.41)$    | $( 0.40)$    | $( 0.45)$    | $( 0.47)$    | $( 0.48)$    |
|----------------------------------+-------------+--------------+--------------+--------------+--------------+--------------|
| TUP*2014                         | $-0.10$     | $-0.02$      | $-0.05$      | $-0.03$      | $0.01$       | $0.01$       |
|                                  | $( 0.09)$   | $( 0.03)$    | $( 0.03)$    | $( 0.03)$    | $( 0.04)$    | $( 0.04)$    |
| TUP*2015                         | $-0.02$     | $0.03$       | $-0.01$      | $-0.03$      | $0.05$       | $-0.02$      |
|                                  | $( 0.09)$   | $( 0.03)$    | $( 0.03)$    | $( 0.03)$    | $( 0.04)$    | $( 0.04)$    |
| CSH*2014                         | $-0.05$     | $-0.00$      | $-0.04$      | $-0.01$      | $-0.03$      | $-0.00$      |
|                                  | $( 0.11)$   | $( 0.04)$    | $( 0.04)$    | $( 0.04)$    | $( 0.05)$    | $( 0.05)$    |
| CSH*2015                         | $0.03$      | $0.06$       | $0.03$       | $-0.01$      | $-0.00$      | $-0.04$      |
|                                  | $( 0.11)$   | $( 0.04)$    | $( 0.04)$    | $( 0.04)$    | $( 0.05)$    | $( 0.05)$    |
| Bsln2013                         | $0.07^{**}$ | $-0.00$      | $0.02$       | $0.03$       | $0.06^{**}$  | $-0.02$      |
|                                  | $( 0.03)$   | $( 0.02)$    | $( 0.02)$    | $( 0.02)$    | $( 0.03)$    | $( 0.03)$    |
| 2014                             | $0.07$      | $0.09^{***}$ | $0.10^{***}$ | $0.09^{***}$ | $0.17^{***}$ | $0.22^{***}$ |
|                                  | $( 0.06)$   | $( 0.02)$    | $( 0.02)$    | $( 0.03)$    | $( 0.03)$    | $( 0.03)$    |
| 2015                             | $0.03$      | $0.22^{***}$ | $0.21^{***}$ | $0.26^{***}$ | $0.30^{***}$ | $0.39^{***}$ |
|                                  | $( 0.06)$   | $( 0.02)$    | $( 0.02)$    | $( 0.02)$    | $( 0.03)$    | $( 0.03)$    |
| Bsln NA                          | $-0.17^{*}$ | $-0.02$      | $-0.03$      | $0.03$       | $-0.02$      | $-0.08^{*}$  |
|                                  | $( 0.09)$   | $( 0.03)$    | $( 0.03)$    | $( 0.03)$    | $( 0.04)$    | $( 0.04)$    |
|----------------------------------+-------------+--------------+--------------+--------------+--------------+--------------|
| F-stat                           | $1.45$      | $9.34$       | $8.36$       | $10.84$      | $6.70$       | $5.91$       |
| N                                | $1299.00$   | $1282.00$    | $1297.00$    | $1293.00$    | $1297.00$    | $1292.00$    |
|----------------------------------+-------------+--------------+--------------+--------------+--------------+--------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $-0.13$     | $-0.08$      | $-0.08^{*}$  | $-0.01$      | $0.01$       | $0.05$       |
|                                  | $( 0.14)$   | $( 0.05)$    | $( 0.05)$    | $( 0.05)$    | $( 0.06)$    | $( 0.06)$    |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $-0.06$     | $-0.03$      | $-0.04$      | $-0.02$      | $0.06$       | $0.02$       |
|                                  | $( 0.12)$   | $( 0.04)$    | $( 0.04)$    | $( 0.04)$    | $( 0.05)$    | $( 0.05)$    |
|----------------------------------+-------------+--------------+--------------+--------------+--------------+--------------|

** Assets
   
Turning now to asset holdings for the households, we estimate treatment effects for
total value of physical assets owned, total value of potentially productive assets,
as well as land and cash savings.

*** Total Asset Holdings

The cash group does not appear to have seen an increase in the value of assets
measured, with negative and imprecise point estimates. This contrasts sharply with
the TUP group, which seems to get notably wealthier and stay wealthier over time. The
TUP group has significantly more asset wealth than the cash or control groups in both
2014 and 2015, 18 months after receipt of transfers. They have 536 SSP more on
average in 2014 and 624 SSP more in 2015. So-called "Productive" assets include
anything that could plausibly be used in productive activity.[fn:: For now, we
include in this list: small and large livestock, farm equipment, mobiles, carts,
sewing equipment, sheds, and shop premises.] Here we see the TUP group has 320 SSP
(95%) more in this area over the control group, with a similar magnitude at midline.

Note also that the effect on total assets is higher in absolute value than the effect
on productive asset value, suggesting that the increased wealth cannot be explained
purely by households holding onto asset transfers for the length of the program's
monitoring phase. Indeed, we see in \Fig{fig:AssetTotal} that the TUP group is the
only one for whom total measured asset holdings did not fall on average over these
two years, which saw hyperinflation and a significant aggregate economic downturn.
This asset effect (including the savings effect below) is the only feature of
households' financial situation on which we we see a persistent effect.

#+CAPTION: Measured asset wealth by group-year
#+NAME: fig:AssetTotal
[[../figures/AssetTotal_groupyear.png]] 

#+name: asset_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<asset_analysis>>
return tab
#+end_src

RESULTS: asset_results WITH Tgroup Fixed Effects
|----------------------------------+--------------------+-------------------+-------------------|
|                                  | Total              | Productive        | Livestock         |
|----------------------------------+--------------------+-------------------+-------------------|
| CTL mean                         | \(1149.392\)       | \(303.427\)       | \(191.478\)       |
|                                  | \((1494.992)\)     | \((586.417)\)     | \((529.489)\)     |
|----------------------------------+--------------------+-------------------+-------------------|
| TUP*2014                         | \(301.119^{***}\)  | \(185.870^{***}\) | \(145.139^{***}\) |
|                                  | \((100.921)\)      | \((45.928)\)      | \((39.745)\)      |
| TUP*2015                         | \(152.392\)        | \(76.187^{*}\)    | \(87.150^{**}\)   |
|                                  | \((100.921)\)      | \((45.928)\)      | \((39.745)\)      |
| CSH*2014                         | \(124.143\)        | \(50.705\)        | \(10.077\)        |
|                                  | \((127.384)\)      | \((57.967)\)      | \((50.161)\)      |
| CSH*2015                         | \(-112.701\)       | \(-16.201\)       | \(2.840\)         |
|                                  | \((127.384)\)      | \((57.967)\)      | \((50.161)\)      |
| TUP                              | \(453.511^{***}\)  | \(262.057^{***}\) | \(232.289^{***}\) |
|                                  | \((63.925)\)       | \((29.053)\)      | \((25.152)\)      |
| CSH                              | \(11.442\)         | \(34.504\)        | \(12.917\)        |
|                                  | \((80.739)\)       | \((36.668)\)      | \((31.734)\)      |
| 2014                             | \(895.043^{***}\)  | \(268.534^{***}\) | \(136.818^{***}\) |
|                                  | \((88.645)\)       | \((39.674)\)      | \((34.189)\)      |
| 2015                             | \(1056.150^{***}\) | \(295.315^{***}\) | \(183.214^{***}\) |
|                                  | \((88.645)\)       | \((39.674)\)      | \((34.189)\)      |
| Bsln2013                         | \(0.072^{***}\)    | \(0.015\)         | \(0.021\)         |
|                                  | \((0.017)\)        | \((0.012)\)       | \((0.013)\)       |
| Bsln_NAN                         | \(0.000\)          | \(0.000\)         | \(0.000\)         |
|                                  | \((0.000)\)        | \((0.000)\)       | \((0.000)\)       |
|----------------------------------+--------------------+-------------------+-------------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | \(413.820^{**}\)   | \(202.071^{**}\)  | \(142.298^{**}\)  |
|                                  | \((181.534)\)      | \((82.625)\)      | \((71.498)\)      |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | \(265.093^{**}\)   | \(92.388\)        | \(84.310\)        |
|                                  | \((134.945)\)      | \((61.419)\)      | \((53.148)\)      |
|----------------------------------+--------------------+-------------------+-------------------|
| F-stat                           | \(12.984\)         | \(14.753\)        | \(15.917\)        |
| N                                | \(1488.000\)       | \(1488.000\)      | \(1488.000\)      |
|----------------------------------+--------------------+-------------------+-------------------|

RESULTS: asset_results WITHOUT Tgroup Fixed Effects
|----------------------------------+--------------------+-------------------+-------------------|
|                                  | Total              | Productive        | Livestock         |
|----------------------------------+--------------------+-------------------+-------------------|
| CTL mean                         | \(1149.392\)       | \(303.427\)       | \(191.478\)       |
|                                  | \((1494.992)\)     | \((586.417)\)     | \((529.489)\)     |
|----------------------------------+--------------------+-------------------+-------------------|
| TUP*2014                         | \(754.630^{***}\)  | \(447.927^{***}\) | \(377.428^{***}\) |
|                                  | \((135.492)\)      | \((61.624)\)      | \((53.338)\)      |
| TUP*2015                         | \(605.902^{***}\)  | \(338.244^{***}\) | \(319.439^{***}\) |
|                                  | \((135.492)\)      | \((61.624)\)      | \((53.338)\)      |
| CSH*2014                         | \(135.584\)        | \(85.208\)        | \(22.994\)        |
|                                  | \((171.069)\)      | \((77.777)\)      | \((67.307)\)      |
| CSH*2015                         | \(-101.259\)       | \(18.303\)        | \(15.757\)        |
|                                  | \((171.069)\)      | \((77.777)\)      | \((67.307)\)      |
| 2014                             | \(895.043^{***}\)  | \(268.534^{***}\) | \(136.818^{***}\) |
|                                  | \((88.645)\)       | \((39.674)\)      | \((34.189)\)      |
| 2015                             | \(1056.150^{***}\) | \(295.315^{***}\) | \(183.214^{***}\) |
|                                  | \((88.645)\)       | \((39.674)\)      | \((34.189)\)      |
| Bsln2013                         | \(0.072^{***}\)    | \(0.015\)         | \(0.021\)         |
|                                  | \((0.017)\)        | \((0.012)\)       | \((0.013)\)       |
| Bsln_NAN                         | \(0.000\)          | \(0.000\)         | \(0.000\)         |
|                                  | \((0.000)\)        | \((0.000)\)       | \((0.000)\)       |
|----------------------------------+--------------------+-------------------+-------------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | \(855.889^{***}\)  | \(429.624^{***}\) | \(361.670^{***}\) |
|                                  | \((218.012)\)      | \((99.221)\)      | \((85.859)\)      |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | \(707.162^{***}\)  | \(319.941^{***}\) | \(303.682^{***}\) |
|                                  | \((181.065)\)      | \((82.403)\)      | \((71.306)\)      |
|----------------------------------+--------------------+-------------------+-------------------|
| F-stat                           | \(12.984\)         | \(14.753\)        | \(15.917\)        |
| N                                | \(1488.000\)       | \(1488.000\)      | \(1488.000\)      |
|----------------------------------+--------------------+-------------------+-------------------|
#+end_example


#+name: tab:assets
#+caption: Average treatment effects by group-year on total value (in SSP) of all assets measured and of productive assets measured
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+-----------------+----------------|
|                                  | Total           | Productive     |
|----------------------------------+-----------------+----------------|
| CTL mean                         | $1225.61$       | $337.60$       |
|                                  | $(1502.46)$     | $(605.57)$     |
|----------------------------------+-----------------+----------------|
| TUP*2014                         | $535.79^{***}$  | $361.80^{***}$ |
|                                  | $(154.02)$      | $(74.19)$      |
| TUP*2015                         | $624.79^{***}$  | $320.74^{***}$ |
|                                  | $(146.01)$      | $(68.68)$      |
| CSH*2014                         | $-125.86$       | $18.50$        |
|                                  | $(191.31)$      | $(95.80)$      |
| CSH*2015                         | $-49.99$        | $-5.00$        |
|                                  | $(187.32)$      | $(88.40)$      |
| Bsln2013                         | $0.08^{***}$    | $0.00$         |
|                                  | $( 0.02)$       | $( 0.01)$      |
| 2014                             | $1259.75^{***}$ | $465.53^{***}$ |
|                                  | $(112.68)$      | $(55.96)$      |
| 2015                             | $1124.61^{***}$ | $392.97^{***}$ |
|                                  | $(103.46)$      | $(50.21)$      |
| Bsln NA                          | $21.30$         | $-131.14^{**}$ |
|                                  | $(146.51)$      | $(51.35)$      |
|----------------------------------+-----------------+----------------|
| N                                | $1305.00$       | $1247.00$      |
| F-stat                           | $8.53$          | $10.19$        |
|----------------------------------+-----------------+----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $585.78^{**}$   | $366.79^{***}$ |
|                                  | $(239.76)$      | $(114.58)$     |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $674.78^{***}$  | $325.74^{***}$ |
|                                  | $(194.72)$      | $(92.26)$      |
|----------------------------------+-----------------+----------------|


*** Savings

The TUP households were strongly encouraged to pay into a savings account maintained
by BRAC at each of their weekly meetings. Anecdotally, this discouraged some women
from attending the meetings, but we can see in Table \Tab{tab:Nonzero} that did
manage to change behavior. TUP participants appear to be 48% (19 pp.) more likely to
report having any savings at all at endline compared to control households. It's
worth noting though that since the TUP households also regard their savings behavior
as much more transparent to BRAC (and have received pressure to save from them) than
the other groups, these households may simply be more likely to reveal that they are
saving when asked. Nonetheless, it seems notable that among the group that
received cash, households were no more likely than the control group to report having
positive cash savings. This suggests to us that receiving cash is not in itself
sufficient to induce households to start holding cash on hand or in an account.
Conversely, increases in savings among TUP households were due not only to their
increased wealth, but also to additional pressure to hold cash in a savings account.

In Table \Tab{tab:Savings}, we consider the amount of cash saved in each year among
those who reported savings. Here we see that cash and TUP groups increase the amount
saved by a similar amount, with cash transfer recipients holding 47% (91.4 SSP) more
and TUP households saving 42% (81.3 SSP) more. This is significantly less than was
given to these households, but combined with the short-term consumption results, goes
some distance in explaining the lack of effect on physical asset wealth. The cash
seems to have gone primarily to consumption and savings. The asset transfer program
on the other hand seems to have achieved a similar result among a greater proportion
of households in addition to a notable increase in asset wealth.

It is common in this community (and most in the region) to store non-perishable food
like maize, cassava, or millet as a form of savings. This would seem particularly
reasonable in a high-inflation context, where the price of grain had doubled in the
previous year. At least as many households report saving in food (53%) as in cash
(46%), with an average market value of 106 SSP. However, we find no evidence that
either treatment group increased food savings.[fn:: Note that food savings was not
measured at baseline, so these controls are omitted.]

Neither do we find evidence that either treatment increased the size or likelihood of
giving or receiving interhousehold transfers, either in cash or in kind. These
results are omitted since only 35 and 60 households reported giving and receiving
transfers respectively, with no difference in group means.

#+name: savings_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<savings_analysis>>
return Table
#+end_src

#+name: tab:Nonzero
#+caption: Average treatment effects by group-year on percentage of households reporting any savings or land access 
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+--------------+--------------+--------------+--------------|
| % > 0                            | Savings      | Food Sav     | LandCult     | LandOwn      |
|----------------------------------+--------------+--------------+--------------+--------------|
| CTL mean                         | $0.45$       | $0.82$       | $0.82$       | $0.90$       |
|----------------------------------+--------------+--------------+--------------+--------------|
| CSH*2014                         | $-0.06$      | $0.00$       | $-0.04$      | $-0.01$      |
|                                  | $( 0.06)$    | $( 0.04)$    | $( 0.04)$    | $( 0.04)$    |
| CSH*2015                         | $0.03$       | $0.02$       | $0.05$       | $0.02$       |
|                                  | $( 0.05)$    | $( 0.04)$    | $( 0.04)$    | $( 0.04)$    |
| TUP*2014                         | $0.22^{***}$ | $-0.02$      | $-0.03$      | $-0.00$      |
|                                  | $( 0.04)$    | $( 0.03)$    | $( 0.03)$    | $( 0.03)$    |
| TUP*2015                         | $0.21^{***}$ | $-0.03$      | $0.01$       | $-0.01$      |
|                                  | $( 0.04)$    | $( 0.03)$    | $( 0.03)$    | $( 0.03)$    |
| 2014                             | $0.43^{***}$ | $1.00^{***}$ | $0.83^{***}$ | $0.82^{***}$ |
|                                  | $( 0.04)$    | $( 0.02)$    | $( 0.06)$    | $( 0.05)$    |
| 2015                             | $0.39^{***}$ | $0.82^{***}$ | $0.77^{***}$ | $0.84^{***}$ |
|                                  | $( 0.04)$    | $( 0.02)$    | $( 0.05)$    | $( 0.05)$    |
| Bsln2013                         | $0.05$       |              | $0.05$       | $0.07$       |
|                                  | $( 0.04)$    |              | $( 0.05)$    | $( 0.04)$    |
| Bsln NA                          | $0.08^{*}$   |              | $0.05$       | $0.05$       |
|                                  | $( 0.04)$    |              | $( 0.06)$    | $( 0.05)$    |
|----------------------------------+--------------+--------------+--------------+--------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $0.19$       | $-0.04$      | $-0.07$      | $-0.02$      |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $0.18$       | $-0.05$      | $-0.03$      | $-0.03$      |
|----------------------------------+--------------+--------------+--------------+--------------|
| F-stat                           | $8.83$       | $15.60$      | $0.79$       | $0.76$       |
| N                                | $1259.00$    | $870.00$     | $1231.00$    | $1251.00$    |
|----------------------------------+--------------+--------------+--------------+--------------|

#+name: tab:Savings
#+caption: Average treatment effects by group-year on total value (in SSP) of all cash and food savings and area (in fedan) of land being cultiviated by the household (including rented or temporary-use) and owned by the household.
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+----------------+----------------+----------------+----------------|
| Amt.                             | Savings        | Food Sav       | LandCult       | LandOwn        |
|----------------------------------+----------------+----------------+----------------+----------------|
| CTL mean                         | $191.19$       | $114.78$       | $61.88$        | $46.00$        |
|----------------------------------+----------------+----------------+----------------+----------------|
| CSH*2014                         | $28.74$        | $0.22$         | $10.18$        | $10.50$        |
|                                  | $(42.93)$      | $(15.38)$      | $(15.07)$      | $(12.57)$      |
| CSH*2015                         | $91.40^{**}$   | $-14.34$       | $-39.18^{***}$ | $-32.37^{***}$ |
|                                  | $(40.89)$      | $(14.98)$      | $(14.90)$      | $(11.95)$      |
| TUP*2014                         | $-27.09$       | $17.16$        | $-4.76$        | $-3.02$        |
|                                  | $(29.76)$      | $(12.33)$      | $(11.94)$      | $(10.04)$      |
| TUP*2015                         | $81.33^{***}$  | $1.13$         | $-17.38$       | $-12.56$       |
|                                  | $(29.32)$      | $(12.26)$      | $(11.65)$      | $( 9.41)$      |
| 2014                             | $106.72^{***}$ | $62.03^{***}$  | $11.37$        | $17.31^{**}$   |
|                                  | $(24.85)$      | $( 8.36)$      | $( 9.94)$      | $( 8.56)$      |
| 2015                             | $163.04^{***}$ | $114.78^{***}$ | $61.52^{***}$  | $51.89^{***}$  |
|                                  | $(24.13)$      | $( 7.60)$      | $( 9.54)$      | $( 7.88)$      |
| Bsln2013                         | $0.05^{**}$    |                | $0.94$         | $-2.43$        |
|                                  | $( 0.02)$      |                | $( 3.07)$      | $( 1.95)$      |
| Bsln NA                          | $40.07^{*}$    |                | $-1.60$        | $-6.02$        |
|                                  | $(21.24)$      |                | $( 9.92)$      | $( 8.29)$      |
|----------------------------------+----------------+----------------+----------------+----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $-118.49$      | $31.50$        | $34.42$        | $29.35$        |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $-10.07$       | $15.47$        | $21.79$        | $19.80$        |
|----------------------------------+----------------+----------------+----------------+----------------|
| F-stat                           | $7.41$         | $7.14$         | $4.91$         | $3.72$         |
| N                                | $671.00$       | $777.00$       | $1042.00$      | $1114.00$      |
|----------------------------------+----------------+----------------+----------------+----------------|


*** Land Holdings

We also examine land ownership and cultivation in each year. We find no evidence that
either group is more or less likely to report owning or cultivating at least some
land, though this may be in part because land ownership and cultivation is already
very common. Anecdotaly, divesting from land ownership entirely could be seen as a
relatively drastic decision. However, members of the cash group who are involved in
agriculture report cultivating 65% less and owning 70% less land than the control
group. This raises the interesting question of whether the cash group was likely to
switch occupations from farming to non-farm self-employment.

It could also raise questions around the underlying logic of the agrarian focus
transfer in the TUP program, if unconstrained transfers prompt households to divest
from these opportunities. This concern may be validated somewhat by the fact that TUP
participants primarily stated a preference for small retail training and transfers
over small animal husbandry or vegetable gardening.

** Income

Income was reliably measured only in 2015, and so our estimates do not control for
baseline values. The control group in 2015 has a measured income of roughly 4325 SSP
per year, or roughly $540 US (assuming an exchange rate of around 8). The TUP group
sees a 327 SSP ($41 US, 7%) increase in annual average income, but with a highly
skewed distribution and large standard errors. The related figure shows that total
income is not particularly different among groups. Perhaps the main lesson is that
the TUP group has measurably more reported livestock-related income, and less farm
income, indicating a shift away from farming. The cash group may exhibit some
substitution away from farm and livestock, but as is evident graphically, we do not
observe sizable changes in total income for either treatment group. 

\newpage
#+name: income_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<income_analysis>>
return tab
#+end_src

#+Caption: Distribution of total observed income by group
#+NAME: fig:Income_group
[[../figures/Income_group.png]] 

#+name: tab:Income
#+caption: Average treatment effects by group-year on total value (in SSP) of income reported in 2015 by sector.
#+attr_latex: :environment longtable :align lrrrrrrr
|---------------------------+---------------+---------------+------------+------------|
|                           | Farm          | Livestock     | Non-Farm   | Total      |
|---------------------------+---------------+---------------+------------+------------|
| CTL mean                  | $773.05$      | $640.33$      | $3774.49$  | $4325.54$  |
|---------------------------+---------------+---------------+------------+------------|
| TUP                       | $-142.20^{*}$ | $281.12^{**}$ | $86.24$    | $327.83$   |
|                           | $(77.21)$     | $(126.30)$    | $(469.48)$ | $(455.95)$ |
| CSH                       | $-26.15$      | $-83.81$      | $61.80$    | $7.92$     |
|                           | $(100.82)$    | $(177.25)$    | $(620.53)$ | $(600.43)$ |
|---------------------------+---------------+---------------+------------+------------|
| N                         | $531.00$      | $380.00$      | $606.00$   | $671.00$   |
| F-stat                    | $1.75$        | $3.48$        | $0.02$     | $0.28$     |
|---------------------------+---------------+---------------+------------+------------|
| $\beta^{TUP}-\beta^{CSH}$ | $-116.05$     | $364.94^{**}$ | $24.44$    | $319.91$   |
|                           | $(105.79)$    | $(174.74)$    | $(651.27)$ | $(629.93)$ |
|---------------------------+---------------+---------------+------------+------------|

** Exposure to Conflict

In 2014, households were surveyed shortly after the NGO's offices had re-opened in
the wake of the outbreak of widespread armed conflict. Respondents were asked a short
set of questions about whether they were directly affected, and if so, in what way.
There has only been a few incidents of violence near Yei town at that point, and the most
directly involved ethnic groups made up a small portion of the local population. There
is no clear comparison group to which we might compare our sample, and the economic
climate changed over this same period in several ways that were probably not directly
caused by the violence. As such, we have no clear means of identifying the effect of
the conflict itself on household welfare. Nonetheless, it is interesting to consider
correlates with self-reported exposure to the conflict, and to see if program
assignment had any effect on households' exposure or response.

Our main outcomes of interest are whether individuals say they were "worried" or
"directly affected" by the violence, unable to invest in a farm or business as a
result, migrated as a cautionary measure, or did something else to protect the lives
of family members. A final question among those who took no cautionary measures was
whether this because they did not have the means (i.e. "NoMeans"). TUP participants
are 24% (13 pp.) less likely to report having been "affected" by the conflict, and
38% (6 pp.) less likely to report that they were affected specifically by being
unable to plant crops or invest in their business. This was the second most common
way in which households reported being affected behind "needed to relocate or
migrate", where respondents are not clearly different. Nonetheless, this raises the
possibility that having received a significant asset transfer and the expectation of
NGO support around the outbreak of
conflict may have helped mitigate the conflict's negative effect on investment and
protect households from being affected overall.

#+name: conflict_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<conflict_analysis>>
return Table
#+end_src

\newpage

#+name: tab:conflict_exposure
#+caption: Average treatment effects by group-year on the probability of having been affected in a significant way by the outbreak of violence in late 2013
#+attr_latex: :environment longtable :align lrrrrrrr
|----------+---------------+--------------+--------------+--------------+--------------+--------------|
|          | Affected      | Migrated     | NoInvest     | NoMeans      | ProtectLives | Worried      |
|----------+---------------+--------------+--------------+--------------+--------------+--------------|
| CTL mean | $0.53^{***}$  | $0.33^{***}$ | $0.16^{***}$ | $0.33^{***}$ | $0.38^{***}$ | $0.93^{***}$ |
|          | $( 0.03)$     | $( 0.02)$    | $( 0.02)$    | $( 0.02)$    | $( 0.03)$    | $( 0.01)$    |
| TUP      | $-0.13^{***}$ | $0.04$       | $-0.06^{**}$ | $-0.06$      | $0.02$       | $-0.02$      |
|          | $( 0.04)$     | $( 0.04)$    | $( 0.03)$    | $( 0.04)$    | $( 0.05)$    | $( 0.02)$    |
|----------+---------------+--------------+--------------+--------------+--------------+--------------|
| F-stat   | $9.20$        | $0.96$       | $3.95$       | $2.55$       | $0.19$       | $0.49$       |
| N        | $601.00$      | $655.00$     | $655.00$     | $655.00$     | $585.00$     | $603.00$     |
|----------+---------------+--------------+--------------+--------------+--------------+--------------|

#+Caption: % of Sample reporting exposure to conflict by group.
#+NAME: fig:conflict_exposure
[[../figures/conflict_exposure.png]] 

* Concluding Remarks
  
BRAC's South Sudan pilot of the TUP program represents the only such test of the
ultra-poor graduation framework conducted in an area of significant political and
economic instability. It also represents one of the only direct comparisons of this
model to a similarly expensive unconditional cash transfer, arguably its most
sensible benchmark for success. As such, while our study may not generalize to
contexts with high-functioning cash economies and relative political stability, it
provides suggestive evidence as to the best way of transfering wealth in order to
help poor and vulnerable households.

Cash transfers appear to increase consumption and possibly shift investment from
agriculture to non-farm activities, without a related increase in wealth or income.
Conversely, the TUP program increased wealth and directly shifted work from
agriculture to livestock, with increased consumption in the short run. We also find
that having received asset transfers dampened the negative investment effects
following the outbreak of violence.[fn:: Whether a cash transfer would have had a
similar mitigating effect is hard to say.] We tentatively conclude that targeted
asset transfers can play a constructive role in helping poor, self-employed
households when they face economic uncertainty. And while cash increases household
consumption, the goal of improving income or wealth is well served by the additional
services that the ultra-poor graduation framework offer.
  
* References
   \renewcommand{\refname}{}
   \printbibliography


* COMMENT Extra Analysis

** Good-level analysis

Next, \ref{tab:consumption_full} sets aside these aggregated measures to look more
carefully at potential changes in the composition of consumption in each group. Given
the large number of zeros, we use a linear model to consider first the 
frequency of non-zero consumption of each good among treatment and control
households, then look at levels of consumption among households with non-zero
consumption. \Tab{consumption_full} presents point estimates.

A few changes in the composition of consumption are interesting. TUP households appear to consume 17% less
sorghum (often considered an inferior good in Yei) and more on rice, which is
considered a higher-quality staple. While almost everyone reports some health
spending over the past month, both treatment groups spent more, though only
statistically significant in the cash group, which saw a 50% increase over the
control group. The cash group was also 30% (14 pp) more likely to have spent money
on funerals, though they did not spend more on average.

#+name: consumption_disaggreate_results
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
<<consumption_analysis>>
from matplotlib import pyplot as plt
#~ Only keep disaggregate items
Goods = C.filter(regex="^c_").rename(columns=lambda col: col[2:] if col.startswith("c_") else col)
#~ 1 if coded as >0, 0 if zero, keep NaN's missing, since those are only from missing surveys.
Nonzero = Goods.applymap(lambda i: float(i>0) if not np.isnan(i) else np.nan)
#~ Restrict Goods df to positive responses.
Goods = Goods.replace(0,np.nan)
too_many_zeros = 30
many_zeros = [item for item in Goods if Goods[item].notnull().sum()<too_many_zeros]
Nonzero = Nonzero.drop(many_zeros,1)
Goods = Goods.drop(many_zeros,1)

#~ Merge in Control Vars
controls = ["cons","TUP","CSH"]
Goods = Goods.join(C[controls],how="left")
Nonzero = Nonzero.join(C[controls],how="left")
Items = [item[:-2] for item in Goods if item.endswith("_e")]
CTL = Goods[controls].sum(axis=1)==1 #~ i.e. only constant ==1, TUP & Cash ==0
Goods_ctl_mean =   Goods.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Zeros_ctl_mean = Nonzero.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])

Zero, Zero_se = reg_table(regressions(Nonzero,"_e", outcomes=Items, Baseline="_b"), resultdf=True, Transpose=True)
Good, Good_se = reg_table(regressions(Goods,"_e", outcomes=Items, Baseline="_b"  ), resultdf=True, Transpose=True)
#~ Make full table of Standard errors
SE = Zero_se[["TUP","CSH"]].join(Good_se[["TUP","CSH"]], lsuffix=" (%>0)", rsuffix=" (Amt.)")

#~ Make full table of point estimates and control-group means
FullTable = pd.DataFrame({"Mean (CTL)":Goods_ctl_mean, "% >0 (CTL)":Zeros_ctl_mean}).join(Zero[["TUP","CSH"]])
FullTable = FullTable.join(Good[["TUP","CSH","N"]], lsuffix=" (%>0)", rsuffix=" (Amt.)")

#~ Make % change graph
fig, ax = plt.subplots(2,1, figsize=(6,9))
for i, group in enumerate(("TUP","CSH")):
    pct_change = FullTable[group+" (Amt.)"]/FullTable["Mean (CTL)"]
    se_change  = SE[group+" (Amt.)"]/FullTable["Mean (CTL)"]
    pct_change.sort()
    pct_change.plot(kind="bar", yerr=SE[group+" (Amt.)"], ax=ax[i])
    ax[i].set_title(group, fontsize=6)
fig.savefig("../figures/Consumption.png")

FullTable = df_to_orgtbl(FullTable, sedf=SE)
return FullTable
#+end_src

#+name: tab:consumption_full
#+caption: Control group means and estimated treatment effects for percent consuming any and total amounts consumed.
#+attr_latex: :environment longtable :align lrrrrrrr
 #+RESULTS: consumption_disaggreate_results

** Disaggregate Asset Results 

#+name: asset_disaggreate_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<assets_disaggreate_analysis>>
return tab
#+end_src

#+name: tab:asset_disaggregate
#+caption: Control means and treatment effects for Assets owned by >40 households
#+RESULTS: asset_disaggreate_results
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|
|              | # own  |              |             | Value     |                |                |          |
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|
| Item         | CTL    | TUP          | Cash        | CTL       | TUP            | Cash           | N        |
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|
| Pangas       | $1.06$ | $0.01$       | $0.13^{**}$ | $11.86$   | $1.66^{**}$    | $0.04$         | $410.00$ |
| Chickens     | $3.79$ | $0.70$       | $-0.32$     | $154.35$  | $23.31$        | $0.80$         | $162.00$ |
| Mobile       | $1.88$ | $-0.09$      | $0.08$      | $113.96$  | $2.62$         | $1.70$         | $569.00$ |
| Radio        | $1.62$ | $0.84$       | $-0.40$     | $57.25$   | $4.98$         | $5.10$         | $333.00$ |
| Shed         | $1.22$ | $-0.14$      | $-0.22$     | $48.81$   | $37.57$        | $6.81$         | $53.00$  |
| Stoves       | $1.44$ | $0.34$       | $0.31$      | $20.32$   | $18.19^{**}$   | $8.31$         | $84.00$  |
| Potspans     | $4.46$ | $-0.06$      | $-0.23$     | $102.73$  | $15.90$        | $-15.40$       | $582.00$ |
| Ducks        | $5.72$ | $4.26^{***}$ | $-0.16$     | $230.93$  | $109.99^{***}$ | $-19.34$       | $223.00$ |
| Motorcycle   | $1.51$ | $-0.48$      | $0.12$      | $2288.48$ | $300.46$       | $-196.32$      | $66.00$  |
| Chairtables  | $5.02$ | $0.25$       | $0.39$      | $167.62$  | $19.00$        | $-24.73$       | $638.00$ |
| Net          | $3.07$ | $0.03$       | $-0.08$     | $24.49$   | $0.66$         | $-3.81$        | $382.00$ |
| Axes         | $1.02$ | $0.03$       | $-0.02$     | $17.74$   | $0.02$         | $-3.94^{**}$   | $218.00$ |
| Smallanimals | $3.39$ | $0.29$       | $-0.90$     | $767.26$  | $-151.35$      | $-311.05^{**}$ | $155.00$ |
| Charcoal     | $2.20$ | $-0.26$      | $-0.83$     | $35.81$   | $-1.43$        | $-4.65$        | $176.00$ |
| Bicycle      | $6.34$ | $-5.46$      | $-5.52$     | $272.90$  | $-31.50$       | $-42.67$       | $135.00$ |
| Bed          | $3.17$ | $-0.23$      | $-0.40$     | $300.64$  | $19.32$        | $-57.78^{*}$   | $628.00$ |
| Tv           | $1.48$ | $-0.36$      | $-0.26$     | $380.45$  | $121.95$       | $348.23^{**}$  | $45.00$  |
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|


* COMMENT Code appendix
  
** Food Security

 #+name: foodsecure_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle foodsecurity.py
 #~ DevLunch_analysis.py
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
 D = full_data(DIR = DATADIR,balance=[])
 WEEKLY = True

 codes = {"1-2 times a week": 3,
         "3-6 times a week": 2,
         "Everyday": 1,
         "everyday": 1,
         "Less than once a week": 4,
         "less than once a week": 4,
         "Never": 5,
         "never": 5}

 recode = lambda x: codes.setdefault(x,x)

 Aval2013 = D.filter(regex="^fs_.*_b").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Aval2014 = D.filter(regex="^fs_.*_m").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Aval2015 = D.filter(regex="^fs_.*_e").rename(columns=lambda x: x[3:-2]).applymap(recode)

 if WEEKLY:
     weekly = lambda i: float(i<3) if pd.notnull(i) else np.nan
     Aval2013 = Aval2013.applymap(weekly)
     Aval2014 = Aval2014.applymap(weekly)      
     Aval2015 = Aval2015.applymap(weekly)
 
 index_vars = "worried,portions,fewmeals,nofood,hungry,wholeday".split(",")
 Outcomes = index_vars+["z-score"]
 #~ Creates Year dummies, z-scores and baseline values as `var'2013
 for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
     Aval["Year"]=Year
     if not weekly:
        for var in index_vars:
            Aval[index_vars] = (Aval[index_vars]-Aval[index_vars].mean())/Aval[index_vars].std()
     FS_sum = Aval[index_vars].sum(axis=1)
     Aval["z-score"] = (FS_sum-FS_sum.mean())/FS_sum.std()
     for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
    
 Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
 Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
 Vals = Vals.join(D[["TUP","CSH"]])

 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

 Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 Vals=Vals.loc[2014:2015]
 regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = Vals["TUP"]+Vals["CSH"] ==0
 CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+end_src

** Consumption

 #+name: lambda_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/SS-lambdas.py

 import numpy as np
 import pandas as pd
 import cfe.estimation as nd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append("../../data")
 from TUP import full_data, consumption_data, regressions, reg_table, df_to_orgtbl, mobile_data
 food =  ['cereals', 'maize', 'sorghum', 'millet', 'potato', 'sweetpotato', 'rice', 'bread', 'beans', 'oil', 'salt', 'sugar', 'meat', 'livestock', 'poultry', 'fish', 'egg', 'nuts', 'milk', 'vegetables', 'fruit', 'tea', 'spices', 'alcohol', 'otherfood']
 month = ['fuel', 'medicine', 'airtime', 'cosmetics', 'soap', 'transport', 'entertainment', 'childcare', 'tobacco', 'batteries', 'church', 'othermonth']    
 ConsumptionItems = food+['airtime','fuel']
 mobile=True

 D = full_data(DIR=DATADIR)
 C, HH, T = consumption_data(D,WRITE=False,include2016=False)
 HH['log HHSIZE'] = HH["hh_size"].apply(np.log)
 HH = HH.drop("hh_size",1)
 y,z = C.replace(0,np.nan).apply(np.log).sort_index(level=[0,1,2])[ConsumptionItems].copy(),HH.sort_index(level=[0,1,2]).copy()
 y.index.names, z.index.names = ['j','t','mkt'], ['j','t','mkt']
 keep = pd.notnull(y.index.get_level_values("mkt"))
 y,z = y.loc[keep,:].align(z,join="left",axis=0)
 b,ce,d,sed= nd.estimate_reduced_form(y,z,return_se=True,VERBOSE=True)
 ce = ce.dropna(how='all')
 print("Getting Loglambdas")
 bphi,logL=nd.get_loglambdas(ce,TEST="warn")
 try:
    xrange
    logL.to_pickle(DATADIR + "ss-lambdas.df")
 except NameError: logL.to_pickle(DATADIR + "ss-lambdas3.df")
 
 if mobile:
     M,Mc,Mhh = mobile_data(use_dates=True,DIR = DATADIR+"Mobile/")
     y = Mc.replace(0,np.nan).apply(np.log).sort_index(level=[0,1,2]).filter(items=ConsumptionItems).copy()
     z = Mhh.sort_index(level=[0,1,2]).copy()
     y.index.names, z.index.names = ['j','t','mkt'], ['j','t','mkt']
     keep = pd.notnull(y.index.get_level_values("mkt"))
     y,z = y.loc[keep,:].align(z,join="left",axis=0)
     b,ce,d,sed= nd.estimate_reduced_form(y,z,return_se=True,VERBOSE=True)
     ce = ce.dropna(how='all')
     print("Getting Loglambdas")
     Mbphi,MlogL=nd.get_loglambdas(ce,TEST="warn")
     MlogL -= MlogL.mean()
     MlogL /= MlogL.std()
     MlogL = MlogL.unstack('t').drop('4February',1).stack()
     try:
       xrange
       MlogL.to_pickle(DATADIR + "ss-lambdas_mobile.df")
     except NameError: MlogL.to_pickle(DATADIR + "ss-lambdas_mobile3.df")

 #+end_src 

 #+name: consumption_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/SS-consumption.py
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 from TUP import full_data, consumption_data, regressions, reg_table, df_to_orgtbl, mobile_data
 
 USE_LAMBDAS = False #~ Whether to use the output of SS-lambdas.py as an outcome (You may have to run it first.)
 TGROUP_FE   = True  #~ Whether to use TUP & CSH dummy variables as controls (I'm pretty sure we should be)

 food = ['cereals', 'maize', 'sorghum', 'millet', 'potato', 'sweetpotato', 'rice', 'bread', 'beans', 'oil', 'salt', 'sugar', 'meat', 'livestock', 'poultry', 'fish', 'egg', 'nuts', 'milk', 'vegetables', 'fruit', 'tea', 'spices', 'alcohol', 'otherfood']
 month = ['fuel', 'medicine', 'airtime', 'cosmetics', 'soap', 'transport', 'entertainment', 'childcare', 'tobacco', 'batteries', 'church', 'othermonth']    
 year = ['clothesfootwear', 'womensclothes', 'childrensclothes', 'shoes', 'homeimprovement', 'utensils', 'furniture', 'textiles', 'ceremonies', 'funerals', 'charities', 'dowry', 'other']    


 D = full_data(DIR=DATADIR)
 C, HH, T = consumption_data(D,WRITE=True) #"csv")
 if USE_LAMBDAS:
    logL = pd.read_pickle(DATADIR + "ss-lambdas.df")
    logL.index.names=["HH","Year","Location"]
    C = C.join(logL,how="left").rename(columns={"loglambda":"$\log\lambda_{it}$"})
 C = C.reorder_levels([1,2,0])#.sort_index(level=0)
 keep = pd.notnull(C.index.get_level_values("Location"))
 C = C.loc[keep,:]

 # Make aggregate variables
 C["Food"]   = C.filter(items=food).sum(axis=1).replace(0,np.nan)
 C["Month"]   = C.filter(items=food).sum(axis=1)
 C["Year"]   = C.filter(items=food).sum(axis=1)
 C["Tot"]    = C[["Food","Month","Year"]].sum(axis=1).replace(0,np.nan)

 def align_indices(df1,df2):
    """
    Reorder levels of df2 to match that of df1
    Must have same index.names
    """
    I1, I2 = df1.index, df2.index
    try: assert(not set(I1.names).difference(I2.names))
    except AssertionError: raise ValueError("Index names must be the same")
    new_order = []
    for lvl in I1.names: new_order.append(I2.names.index(lvl))
    df2 = df2.reorder_levels(new_order)
    return df1, df2

 def winsorize(Series, **kwargs):
    """
    Need to implement two-sided censoring as well.
    WARNING: if Top<0, all zeros will be changed to Top
    """
    percent    = kwargs.setdefault("percent",99)
    stdev      = kwargs.setdefault("stdev",False)
    drop       = kwargs.setdefault("drop",False)
    drop_zeros = kwargs.setdefault("drop_zeros",True)
    twoway     = kwargs.setdefault("twoway",False)

    if drop_zeros: S = Series.replace(0,np.nan).dropna()
    else: S = Series.dropna()
    N_OBS = S.notnull().sum()
    if N_OBS<10: return S

    if percent: Top = np.percentile(S, percent)
    if stdev:   
        Top =  S.dropna().mean()
        Top += stdev*S.dropna().std()
    try: assert((not drop_zeros) or Top>0)
    except AssertionError: raise ValueError("Top < 0 but zeros excluded")
    if drop: replace_with = np.nan
    else:    replace_with = Top

    _Series = Series.copy()
    _Series[_Series>Top]=replace_with

    if not twoway: return _Series
    else:
        kwargs['twoway'] = False
        return -1*winsorize(-1*_Series, **kwargs)

 def USD_conversion(Exp,exchange_rate=1.,PPP=1.,inflation=1.,time='Year'):
    """
    Convert nominal local currency into price- and inflation-adjusted USD

    Exp - A numeric or pd.Series object 
    exchange_rate - Taken as LCU/USD. 
    PPP - Taken as $Real/$nominal
    inflation - Taken as % inflation compared to some baseline.
    time - If a list is passed, `time' indicates the name or position of the time level in Exp.index
        NOTE: This has to be a cumulative number, so if inflation is 20% for two straight years, that year should be divided by (1+.2)**2
    Final calculation will basically be Exp_usdppp = Exp*(exchange_rate*PPP)/inflation

    if pd.Series are passed for any kwarg, index name needs to be in the multi-index of Exp.
    """
    if type(inflation)==list: inflation=[1./i for i in inflation]
    else: inflation = 1/inflation
    if type(exchange_rate)==list: exchange_rate=[1./i for i in exchange_rate]
    else: exchange_rate = 1/exchange_rate
    
    _Exp = Exp.copy()
    VARS = (exchange_rate, PPP,inflation)
    if list in map(type,VARS):
        if time in _Exp.index.names: time=_Exp.index.names.index(time)
        time = _Exp.index.levels[time]
    for var in VARS:
        if type(var)==list: var=pd.Series(var,index=time)
        try: _Exp = _Exp.mul(var)
        except ValueError: #~ If Series index doesn't have a name, try this...
            var.index.name = var.name
            _Exp = _Exp.mul(var)
    return _Exp

 def percapita_conversion(Exp,HH,children=["boys","girls"],adult_equivalent=1.,minus_children='hh_size'):
    """
    Returns household per-capita expenditures given:
        `Exp'- Total household expenditures
        `HH' - Total number of individuals in the household
            If HH is a pd.DataFrame, Exp is divided by HH.sum(1)
            if `children' is the name of a column or a list of column names, 
            those first get divided by the factor adult_equivalent
    """
    try: HH.columns #~ If HH is a series, just divide though
    except AttributeError: return Exp.div(HH)
    _HH = HH.copy()
    if type(children)==str: children=[children]
    children = _HH.columns.intersection(children).tolist()
    if minus_children: _HH[minus_children] -= _HH[children].sum(1)
    if children: _HH[children] *= adult_equivalent
    Exp,_HH = align_indices(Exp,_HH)
    return Exp.div(_HH.sum(1).replace(0,1))

 #~ Source: http://data.worldbank.org/indicator/PA.NUS.PRVT.PP?locations=SS&name_desc=false
 xrate = [ 2.161, 2.162, 3.293] #~ Using PPP adjusted xrate and just setting PPP=1.
 PPP = 1.
 inflation= 1. #~ Bank data uses international $, which is inflation adjusted.
 C["Exp_usd"] = winsorize(USD_conversion(C["Tot"],exchange_rate=xrate,PPP=PPP,inflation=inflation))
 C["Tot_pc"] = percapita_conversion(C["Exp_usd"],HH,adult_equivalent=.5)
 #C["Exp_usdpc_tc"] = winsorize(C["Exp_usdpc"])

 #C["z-score"]  = (C["Tot"]-C["Tot"].mean())/C["Tot"].std()
 C["FoodShr"]= C["Food"].div(C["Tot"]) #$\approx$ FoodShare variable
 C["logTot"] = C["Tot"].apply(np.log)
 C = C.join(T, how="left",lsuffix="_")

 if USE_LAMBDAS: Outcomes = ["Tot","logTot","FoodShr", "Food", "$\log\lambda_{it}$"]
 else: Outcomes = ["Tot","logTot","FoodShr", "Food"]

 #$\approx$ Make Baseline variable
 for var in Outcomes: 
     Bl = C.loc[2013,var].reset_index("Location",drop=True)
     #if var in mC: mC = mC.join(Bl,rsuffix="2013", how="left")
     C = C.join(Bl,rsuffix="2013", how="left")


 C["Y"]=np.nan
 for yr in (2013, 2014, 2015): C.loc[yr,"Y"]=str(int(yr))

 C = C.join(pd.get_dummies(C["Y"]), how="left",lsuffix="_")
 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         interaction = C[group]*C[year]
         if interaction.sum()>0: C["{}*{}".format(group,year)] = interaction

 if TGROUP_FE: Controls = ["2014","2015", 'TUP*2014', 'CSH*2014', 'TUP*2015', 'CSH*2015', 'TUP','CSH']
 else:         Controls = ["2014","2015", 'TUP*2014', 'CSH*2014', 'TUP*2015', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 C = C.loc[2014:2015]
 regs  = regressions(C, outcomes=Outcomes,  controls=Controls,  Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = C["TUP"]+C["CSH"] ==0
 CTLmean = {var: C[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: C[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+END_SRC
   
 #+name: mobile_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/SS-mobile.py
 import sys
 DATADIR = "../../data/"
 sys.path.append("../../data")
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 from TUP import full_data, consumption_data, regressions, reg_table, df_to_orgtbl, mobile_data
 ITEMS = ["beans", "sugar", "fish", "nuts", "vegetables", "airtime", "fuel"]

 D = full_data(DIR=DATADIR)
 HH, T = consumption_data(D,WRITE=True)[1:] #"csv")
 M, C,mHH= mobile_data(DIR = DATADIR+"Mobile/")
 try: logL = pd.read_pickle(DATADIR+"ss-lambdas_mobile.df")
 except EnvironmentError: raise IOError("Need to run SS-lambdas.py")
 logL.index.names=["HH","Year","Location"]
 logL.name       =["loglambda"]
 C    = C.join(logL,how="left").rename(columns={"loglambda":"$\log\lambda_{it}$"})
 C    = C.reorder_levels([1,0,2]).sortlevel()
 keep = pd.notnull(C.index.get_level_values("Location"))
 C    = C.loc[keep,:]
 # Make aggregate variables
 C["Tot"]    = C.filter(ITEMS).sum(axis=1).replace(0,np.nan)
 C["logTot"] = C["Tot"].apply(np.log)
 C           = C.join(T, how="left",lsuffix="_")
 C['const']  = 1.

 Outcomes =["Tot",  "logTot", "$\log\lambda_{it}$"]
 Controls= ['const', 'TUP', 'CSH']

 regs = regressions(C,outcomes=Outcomes, controls=Controls, Baseline=2013)
 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])
 CTL = C["TUP"]+C["CSH"] ==0
 CTLmean = {var: C.loc[CTL,var].mean() for var in Outcomes}
 CTLsd = {var: C.loc[CTL,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest= regs[var].t_test("TUP - CSH = 0").summary_frame()
     diff.loc[   r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest["coef"][0]
     diff_se.loc[r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest["std err"][0]

 results = results.append(diff)
 SE = SE.append(diff_se)
 mtab = df_to_orgtbl(results, sedf=SE)

 #+end_src
   
** Assets
   
#+name: asset_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/asset_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
from matplotlib import pyplot as plt
import sys
sys.path.append("../../data")
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

TGROUP_FE = True

D = full_data(DIR="../../data/")

Outcomes = ["Total", "Productive","Livestock"]
Aval2013 = asset_vars(D,year=2013)[0][Outcomes]
Aval2014 = asset_vars(D,year=2014)[0][Outcomes]
Aval2015 = asset_vars(D,year=2015)[0][Outcomes]

#$\approx$ Creates Year dummies and baseline values as `var'2013
for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
    Aval["Year"]=Year
    for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
Vals = Vals.join(D[["TUP","CSH"]])
Vals["CTL"] = (Vals["TUP"]+Vals["CSH"] ==0).apply(int)

for group in ("TUP", "CSH"):
    for year in ("2013", "2014", "2015"):
        Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

#~ Make graph of distribution
stringify = lambda var: Vals[var].apply(lambda x: var if x else "")
Vals["Group"] = stringify("TUP")+stringify("CSH")+stringify("CTL")


amean = Vals.groupby(["Year","Group"]).mean()[["Total","Productive","Livestock"]]
aN = Vals.groupby(["Year","Group"]).count()[["Total","Productive","Livestock"]]
astd = Vals.groupby(["Year","Group"]).std()[["Total","Productive","Livestock"]]
ase = astd/np.sqrt(aN)
asset_pctchange = (amean/amean.ix[2013]).unstack("Year") - 1

for var in ("Total","Productive","Livestock"):
   #~ fig,ax = plt.subplots(1,2)
   #~ for i,yr in enumerate((2014,2015)):
   #~     Vals.ix[yr].dropna(subset=[[var,"TUP","CSH","CTL"]]).groupby("Group")[var].plot(kind="kde",ax=ax[i])
   #~     ax[i].set_title("{} Asset Value in {}".format(var,yr))
   #~     ax[i].legend()
   #~     #~ ax[i].set_aspect(1)
   #~     ax[i].set_xlim(left=0)
   #~ plt.savefig("../figures/Asset{}_kde.png".format(var))
   #~ plt.clf()
   amean.unstack("Group")["Total"].plot(kind="bar",yerr=ase.unstack("Group")["Total"].values)
   plt.tight_layout()
   plt.xticks(rotation=45)
   plt.savefig("../figures/Asset{}_groupyear.png".format(var))
   plt.clf()

if TGROUP_FE: Controls = ["2014","2015", 'TUP*2014', 'CSH*2014', 'TUP*2015', 'CSH*2015', 'TUP','CSH']
else:         Controls = ["2014","2015", 'TUP*2014', 'CSH*2014', 'TUP*2015', 'CSH*2015']


#$\approx$ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
Vals=Vals.loc[2014:2015]
regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

CTL = Vals["TUP"]+Vals["CSH"] ==0
CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

for var in Outcomes:
    ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


results = results.append(diff)
SE = SE.append(diff_se)

tab = df_to_orgtbl(results, sedf=SE)
#+end_src

#+name: assets_disaggreate_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
import numpy as np
import pandas as pd
import sys
sys.path.append("../../data")
from TUP import full_data, regressions, asset_vars, reg_table, df_to_orgtbl

D = full_data(balance=[])
D["cons"] = 1.
Count = D.filter(regex="^asset_n_").rename(columns=lambda col: col[8:])
Vals = D.filter(regex="^asset_val_").rename(columns=lambda col: col[10:])
#~ 1 if coded as >0, 0 if zero, keep NaN's missing, since those are only from missing surveys.
Nonzero = Count.applymap(lambda i: float(i>0) if not np.isnan(i) else np.nan)
#~ Restrict both df's to nonzero responses.
Count = Count.replace(0,np.nan)
Vals  =  Vals.replace(0,np.nan)
too_many_zeros = 30
many_zeros = [item for item in Vals if Vals[item].notnull().sum()<too_many_zeros]
for df in (Nonzero, Count, Vals): df.drop(many_zeros,1, inplace=True)

#~ Merge in Control Vars
controls = ["cons","TUP","Cash"]
Nonzero  = Nonzero.join(D[controls],how="left")
Count    =   Count.join(D[controls],how="left")
Vals     =    Vals.join(D[controls],how="left")

Items = [item[:-2] for item in Vals if item.endswith("_e")]
CTL = Vals[controls].sum(axis=1)==1 #~ i.e. only constant ==1, TUP & Cash ==0
Zeros_ctl_mean = Nonzero.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Count_ctl_mean =   Count.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Vals_ctl_mean  =    Vals.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])

ZeroTable       = reg_table(regressions(Nonzero,"_e", outcomes=Items, controls = ["cons",'Cash','TUP'], Baseline="_b"), orgtbl=True, Transpose=True)
Count, Count_se = reg_table(regressions(Count,"_e",   outcomes=Items, controls = ["cons",'Cash','TUP'], Baseline="_b"), resultdf=True, Transpose=True)
Vals, Vals_se   = reg_table(regressions(Vals,"_e",    outcomes=Items, controls = ["cons",'Cash','TUP'], Baseline="_b"), resultdf=True, Transpose=True)

#~ Make full table of Standard errors-- MAKE SURE YOU HAVE THE SUFFIXES RIGHT.
SE = Count_se[["TUP","Cash"]].join(Vals_se[["TUP","Cash"]], rsuffix=" (SSP)", lsuffix=" (# own)")

#~ Make full table of point estimates and control-group means
FullTable = pd.DataFrame({"# own (CTL)":Count_ctl_mean, "Value (CTL)":Vals_ctl_mean}).join(Count[["TUP","Cash"]])
FullTable = FullTable.join(Vals[["TUP","Cash","N"]], rsuffix=" (SSP)", lsuffix=" (# own)")
FullTable = df_to_orgtbl(FullTable, sedf=SE)
AllTables = FullTable+"\n\n"+ZeroTable
return AllTables
#+end_src

** Savings

#+name: savings_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/savings_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
import sys
sys.path.append("../../data")
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl

def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

D = full_data(balance=[])

Sav = pd.DataFrame(index=D.index) #~ Set up empty DataFrame to fill

years = [("_b",2013), ("_m",2014), ("_e", 2015)]
for suff,year in years: #~ Make Aggregate savings and land holding variables
    Sav["Savings{}".format(year)]  = D.filter(regex="^savings_(home|bank|BRAC|NGOs|other){}".format(suff)).sum(1)
    Sav["Food Sav{}".format(year)] = D.filter(regex="^savings_(maize|sorghum|otherfood)_val{}".format(suff)).sum(1)

    Sav["LandCult{}".format(year)] = D.filter(regex="^land_(owncult|rentcult|communitycult){}".format(suff)).sum(1)
    Sav["LandOwn{}".format(year)]  = D.filter(regex="^land_own.*{}".format(suff)).sum(1)

    Sav["Get Trans{}".format(year)]  = D.filter(regex="^transfers_get.*{}".format(suff)).sum(1)
    Sav["Give Trans{}".format(year)] = D.filter(regex="^transfers_give.*{}".format(suff)).sum(1)

Outcomes = ["Savings","Food Sav","LandCult","LandOwn", "Get Trans", "Give Trans"] #~ Loans give/received omitted

by_year = []
for yr in ("2013","2014","2015"): #~ Provide Baseline Values & put in long format
    S_Year = Sav.filter(like=yr).rename(columns=lambda x:x[:-4])
    for var in Outcomes: 
        if var+"2013" in Sav: S_Year[var+"2013"] = Sav[var+"2013"]
    #~ Note that adding "Year" has to come after topcode, which switches strings to Nan...
    S_Year["Year"] = yr
    by_year.append(S_Year)

#~ In long format with Year,HH index
Sav = pd.concat(by_year).reset_index().set_index(["Year", "HH"], drop=False)
#~ Make Year Dummies for fixed effects


Sav = Sav.join(pd.get_dummies(Sav["Year"]).rename(columns=lambda col: str(int(col))), how="left")
Sav = Sav.drop(["HH","Year"],1)
Sav = Sav.join(D[["TUP","CSH"]])

for group in ("TUP", "CSH"): #~ Make Treatment-by-year interactions
    for year in ("2013", "2014", "2015"):
        Sav["{}*{}".format(group,year)] = Sav[group]*Sav[year]

def isPositive(i):
    #~ Returns 1 if number is positive, 0 if number<=0, nan if already nan, and self if string.
    #~ Note that it's safe to run dummy variables through..
    try:
        if np.isnan(i): return i
        else: return float(i>0)
    except TypeError: return i

too_many_null = 30
Nonzero = Sav.applymap(isPositive)

#~ Save DataFrame with zeros
Savings = Sav.copy()
#~ Naturally, only do this after Creating Nonzero dataframe.
for var in Outcomes: #~ Set zeros to missing And topcode among non-zero values
    for outcome in (var,var+"2013"):
       if outcome in Sav:
           Sav[outcome] = Sav[outcome].replace(0,np.nan)
           Sav[outcome] = Sav.groupby(level="Year")[outcome].apply(topcode) #~ (Untested)

many_null = [item for item in Sav if Sav[item].count()<too_many_null]
many_null2 =[item for item in Savings if Savings[item].count()<too_many_null]
Sav = Sav.drop(many_null,1).copy()
Savings = Savings.drop(many_null,1).copy()

Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

Sav = Sav.loc["2014":"2015"]
Savings = Savings.loc["2014":"2015"]
Nonzero = Nonzero.loc["2014":"2015"]
Sav_regs = regressions(Sav,     outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)
Zer_regs = regressions(Nonzero, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)
Amt_regs = regressions(Savings, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

Sav_results, Sav_SE  = reg_table(Sav_regs,  resultdf=True,table_info=["N","F-stat"])
Zer_results, Zer_SE  = reg_table(Zer_regs,  resultdf=True,table_info=["N","F-stat"])
Amt_results, Amt_SE  = reg_table(Amt_regs,  resultdf=True,table_info=["N","F-stat"])

CTL  = Sav["TUP"]+Sav["CSH"] ==0
CTL2 = Savings["TUP"]+Savings["CSH"] ==0
#~ Get control group means and standard deviations
Sav_CTLmean = {var: Sav[CTL].loc["2015",var].mean() for var in Outcomes}
Zer_CTLmean = {var: Nonzero[CTL].loc["2015",var].mean() for var in Outcomes}
Amt_CTLmean = {var: Savings[CTL2].loc["2015",var].mean() for var in Outcomes}

Sav_CTLsd = {var: Sav[CTL].loc["2015",var].std() for var in Outcomes}
Zer_CTLsd = {var: Nonzero[CTL].loc["2015",var].std() for var in Outcomes}
Amt_CTLsd = {var: Savings[CTL2].loc["2015",var].std() for var in Outcomes}

Sav_diff, Sav_diff_se = pd.DataFrame(Sav_CTLmean,index=["CTL mean"]), pd.DataFrame(Sav_CTLsd,index=["CTL mean"])
Zer_diff, Zer_diff_se = pd.DataFrame(Zer_CTLmean,index=["CTL mean"]), pd.DataFrame(Zer_CTLsd,index=["CTL mean"])
Amt_diff, Amt_diff_se = pd.DataFrame(Amt_CTLmean,index=["CTL mean"]), pd.DataFrame(Amt_CTLsd,index=["CTL mean"])

for var in Outcomes:
    #~ Savings regressions first
    ttest1= Sav_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Sav_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Sav_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Sav_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Sav_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Sav_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

    #~ Nonzero regressions second
    ttest1= Zer_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Zer_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Zer_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Zer_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Zer_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Zer_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

    #~ Savings regressions third
    ttest1= Amt_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Amt_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Amt_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Amt_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Amt_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Amt_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

Save_results = Sav_results.append(Sav_diff)
Zero_results = Zer_results.append(Zer_diff)
Amnt_results = Amt_results.append(Amt_diff)
Save_SE = Sav_SE.append(Sav_diff_se)
Zero_SE = Zer_SE.append(Zer_diff_se)
Amnt_SE = Amt_SE.append(Amt_diff_se)

#~ Land = ["LandCult","LandOwn"] 
#~ Savings = ["Savings","Food Sav", "Get Trans", "Give Trans"] 
#~ Land_results = Sav_results[Land]
#~ zLan_results = Zer_results[Land]
#~ Land_SE = Sav_SE[Land]
#~ zLan_SE = Zer_SE[Land]
#~ 
#~ Sav_results = Sav_results[Savings]
#~ Zer_results = Zer_results[Savings]
#~ Sav_SE =           Sav_SE[Savings]
#~ Zer_SE =           Zer_SE[Savings]

Save_tab = df_to_orgtbl(Save_results, sedf=Sav_SE)
Zero_tab = df_to_orgtbl(Zero_results, sedf=Zer_SE)
Amnt_tab = df_to_orgtbl(Amnt_results, sedf=Amt_SE)

Table = Zero_tab +"\n"+ Save_tab + "\n"+ Amnt_tab

#+end_src

** Income

#+name: income_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
from pandas.io import stata
import statsmodels.api as sm
from matplotlib import pyplot as plt
import sys
sys.path.append("../../data")
from TUP import full_data, regressions, reg_table, df_to_orgtbl
"""
Note that topcoding has a large effect on the distribution here, and we see only a small (presumably non-random) portion of actual income for each household.
"""

# Top-Code or censor outliers?
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

D = full_data(balance=[])
keep = D.index

I_file = '../../data/Endline/sections_8_17.csv'
I = pd.read_csv(I_file).rename(columns={"id":"HH"}).set_index("HH", drop=True).ix[keep]

#~Getting non-agriculture income data is easy
I = I.filter(regex="^s16")
Imonths    = I.filter(regex="s16_\dc").rename(columns=lambda x: x[:-1])
Ipermonth  = I.filter(regex="s16_\dd").rename(columns=lambda x: x[:-1])
Income_12m = Imonths.mul(Ipermonth).sum(axis=1)
Iyear      = I.filter(regex="s16_\de").rename(columns=lambda x: x[:-1]).sum(axis=1)

A_file = "../../data/Endline/Agriculture_cleaned.csv"
A = pd.read_csv(A_file).rename(columns={"id":"HH"}).set_index("HH",drop=False).ix[keep]
unit_prices = A.groupby(["harvest_type", "harvest_price_unit"])["harvest_price"].median()
prices = unit_prices.loc[zip(A["harvest_type"],A["harvest_price_unit"])]
A["price"]=list(prices)

A["harvest_unit_match"] = A["harvest_price_unit"] == A["harvest_unit"]
A["price"] = A["harvest_unit_match"]*A["harvest_price"] + (1-A["harvest_unit_match"])*A["price"]

A["income_farm_year"] = A["harvest_size"]*A["price"]
Ayear = A.groupby("HH")["income_farm_year"].sum()

unit_prices = A.groupby(["livestock_type", "livestock_price_unit"])["livestock_price"].median()
prices = unit_prices.loc[zip(A["livestock_type"],A["livestock_price_unit"])]
A["price"]=list(prices)
A["livestock_unit_match"] = A["livestock_price_unit"] == A["livestock_unit"]
A["price"] = A["livestock_unit_match"]*A["livestock_price"] + (1-A["livestock_unit_match"])*A["price"]

A["income_livestock_year"] = A["livestock_size"]*A["price"]
Lyear = A.groupby("HH")["income_livestock_year"].sum()

Outcomes = ["Total", "Non-Farm", "Farm",  "Livestock"]
Controls = ["cons", "TUP","CSH"]
Vals = pd.DataFrame({"Non-Farm": Income_12m, "Farm":Ayear, "Livestock":Lyear})
Vals = Vals.apply(topcode)

Vals["Total"] = Vals.sum(axis=1)
Vals["cons"] = 1.

Vals = Vals.join(D[["TUP","CSH"]],how="left")
Vals["CTL"] = (Vals["TUP"]+Vals["CSH"] ==0).apply(int)

#~ Make graph of distribution
stringify = lambda var: Vals[var].apply(lambda x: var if x else "")
Vals["Group"] = stringify("TUP")+stringify("CSH")+stringify("CTL")
Vals.dropna(subset=[["Total","TUP","CSH","CTL"]]).groupby("Group")["Total"].plot(kind="kde")
plt.title("Total Income Distribution by Group")
plt.savefig("../figures/IncomeDistribution.png")
plt.clf()
#~ Make bar graphs
Imean  = Vals.groupby("Group").mean()[Outcomes]
Icount = Vals.groupby("Group").count()[Outcomes]
Istd = Vals.groupby("Group").std()[Outcomes]
Ise=Istd/np.sqrt(Icount)
Imean.T.plot(kind="bar",yerr=Ise.T)
plt.tight_layout()
plt.xticks(rotation=45)
plt.savefig('../figures/Income_group.png')
plt.clf()

regs = {var: sm.OLS(Vals[var], Vals[Controls], missing="drop").fit() for var in Outcomes}
results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

CTL = Vals["CTL"] 
CTLmean = {var: Vals.query("CTL==1")[var].mean() for var in Outcomes}
CTLsd = {var: Vals.query("CTL==1").std() for var in Outcomes}
diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

for var in Outcomes:
    ttest1= regs[var].t_test("TUP - CSH = 0").summary_frame()

    diff.loc[   r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest1["coef"][0]
    diff_se.loc[r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest1["std err"][0]

results = results.append(diff)
SE = SE.append(diff_se)

tab = df_to_orgtbl(results, sedf=SE)

#+end_src

#+RESULTS: income_analysis
: None

** Conflict Exposure
#+name: conflict_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
import sys
sys.path.append("../../data")
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl

# Top-Code or censor outliers?
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

#~ Read in data
D = full_data("../../data/TUP_full.dta", balance=[])
D = D[D.merge_midline != 1]
C = D.filter(like="conflict").rename(columns = lambda x: x[:-2]) #~ Set up empty DataFrame to fill

#~ Make Outcome variables
#~ NOTE: Not looking at whether they protected assets as only 50 #~ said they did, and all said "Migrated to stay with other family"
C["Bsln_NaN"] = D["merge_midline"] == 2
C["Worried"]  = C["conflict_worried"]
C["Affected"] = C["conflict_affected"]
protect_lives_codes = lambda x: {"Nothing": 0, "Migrate to stay with friend/family": 1, "Migrated and found new accommodation": 1,
                                 "Looked for protection with Govt. Military": 2, "Looked for protection with NGO": 2}.get(x)
C["ProtectLives"] = C["conflict_protectlives"].apply(protect_lives_codes)
C["Migrated"] = (C["ProtectLives"]==1) + \
                (C["conflict_affected1"]=="Needed to elocate or migrate")
C["NoMeans"] = C["conflict_whynotprotect"]=="Didn't have the means"
C["NoInvest"]= C.filter(like="affected").applymap(lambda x: x=="Could not plant crop or invest in business").sum(axis=1)
C = C.drop([var for var in C if var.startswith("conflict_")], 1)
Outcomes = ["ProtectLives", "Worried", "Affected", "Migrated", "NoMeans", "NoInvest"]

#~ Bring in Treatment variables
C["TUP"] = D["TUP"]
C["cons"] = 1.
C = C.applymap(lambda x: int(x) if not np.isnan(x) else x)
Controls = ["cons", "TUP"] #~, "Bsln_NaN"]

#~ Plot
byT=C.groupby("TUP")
Cbar=byT.mean().drop(["Bsln_NaN",'cons'],1)
Cstd=byT.std().drop(["Bsln_NaN",'cons'],1)
Cn=byT.count().drop(["Bsln_NaN",'cons'],1)
for df in (Cbar,Cstd,Cn): 
        df.index=["CTL","TUP"]
        df.index.name="Group"
Cse=Cstd/np.sqrt(Cn)
Cbar.T.plot(kind="bar",yerr=Cse.T)
plt.tight_layout()
plt.xticks(rotation=45)
plt.savefig("../figures/conflict_exposure.png")
plt.clf()


C_regs = regressions(C, outcomes=Outcomes, controls=Controls, Baseline=False, baseline_na=False)
C_results, C_SE  = reg_table(C_regs,  resultdf=True,table_info=["N","F-stat"])

#~ Get control group means and standard deviations and add to regression table
CTLmean = {var: C.query("TUP==0")[var].mean() for var in Outcomes}
CTLsdv  = {var: C.query("TUP==0")[var].std()  for var in Outcomes}
CTLmean, CTLsdv = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsdv,index=["CTL mean"])
C_results = C_results.append(CTLmean)
C_SE      = C_SE.append(CTLsdv)
C_results.drop('cons',inplace=True)
C_SE.drop('cons',inplace=True)

Table = df_to_orgtbl(C_results, sedf=C_SE)
return Table

#+end_src

** Confidence & Autonomy

#+name: decision_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
 D = full_data(DIR = DATADIR,balance=[])

 Decide14 = D.filter(regex="^decide_.*_m").rename(columns=lambda x: x[:-2].split("_")[-1])#~.applymap(recode)
 Decide15 = D.filter(regex="^decide_.*_e").rename(columns=lambda x: x[:-2].split("_")[-1])#~.applymap(recode)

{'decide_arguments',
 'decide_fearful',

 'decide_pressuretospend'}

 who_decides = ['dailypurchase', 'familyvisits', 'healthchild', 'healthown', 'majorpurchase', 'moneyown']
 who_recode  = lambda x: float("Herself" in x) if pd.notnull(x) else x

 if WEEKLY:
     weekly = lambda i: float(i<3) if pd.notnull(i) else np.nan
     Aval2013 = Aval2013.applymap(weekly)
     Aval2014 = Aval2014.applymap(weekly)      
     Aval2015 = Aval2015.applymap(weekly)
 
 index_vars = "worried,portions,fewmeals,nofood,hungry,wholeday".split(",")
 Outcomes = index_vars+["z-score"]
 #~ Creates Year dummies, z-scores and baseline values as `var'2013
 for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
     Aval["Year"]=Year
     if not weekly:
        for var in index_vars:
            Aval[index_vars] = (Aval[index_vars]-Aval[index_vars].mean())/Aval[index_vars].std()
     FS_sum = Aval[index_vars].sum(axis=1)
     Aval["z-score"] = (FS_sum-FS_sum.mean())/FS_sum.std()
     for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
    
 Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
 Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
 Vals = Vals.join(D[["TUP","CSH"]])

 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

 Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 Vals=Vals.loc[2014:2015]
 regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = Vals["TUP"]+Vals["CSH"] ==0
 CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+end_src

#+name: confidence_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
 D = full_data(DIR = DATADIR,balance=[])

 autonomy_codes = {'go_NGO',
                   'go_church',
                   'go_healthcenter',
                   'go_housenonrelative',
                   'go_houserelative',
                   'go_market',
                   'go_school',
                   'go_water'}

 codes = {"1-2 times a week": 3,
         "3-6 times a week": 2,
         "Everyday": 1,
         "everyday": 1,
         "Less than once a week": 4,
         "less than once a week": 4,
         "Never": 5,
         "never": 5}

 recode = lambda x: codes.setdefault(x,x)

 Autnmy14 = D.filter(regex="^cango_.*_m").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Autnmy15 = D.filter(regex="^cango_.*_e").rename(columns=lambda x: x[3:-2]).applymap(recode)

{'decide_arguments',
 'decide_fearful',

 'decide_dailypurchase',
 'decide_familyvisits',
 'decide_healthchild',
 'decide_healthown',
 'decide_majorpurchase',
 'decide_moneyown',
 'decide_pressuretospend'}

 Decide14 = D.filter(regex="^decide_.*_m").rename(columns=lambda x: x[:-2])#~.applymap(recode)
 Decide15 = D.filter(regex="^decide_.*_e").rename(columns=lambda x: x[:-2])#~.applymap(recode)

 Confid14 = D.filter(regex="^conf_.*_m").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Confid15 = D.filter(regex="^conf_.*_e").rename(columns=lambda x: x[3:-2]).applymap(recode)

 if WEEKLY:
     weekly = lambda i: float(i<3) if pd.notnull(i) else np.nan
     Aval2013 = Aval2013.applymap(weekly)
     Aval2014 = Aval2014.applymap(weekly)      
     Aval2015 = Aval2015.applymap(weekly)
 
 index_vars = "worried,portions,fewmeals,nofood,hungry,wholeday".split(",")
 Outcomes = index_vars+["z-score"]
 #~ Creates Year dummies, z-scores and baseline values as `var'2013
 for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
     Aval["Year"]=Year
     if not weekly:
        for var in index_vars:
            Aval[index_vars] = (Aval[index_vars]-Aval[index_vars].mean())/Aval[index_vars].std()
     FS_sum = Aval[index_vars].sum(axis=1)
     Aval["z-score"] = (FS_sum-FS_sum.mean())/FS_sum.std()
     for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
    
 Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
 Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
 Vals = Vals.join(D[["TUP","CSH"]])

 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

 Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 Vals=Vals.loc[2014:2015]
 regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = Vals["TUP"]+Vals["CSH"] ==0
 CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+end_src

** Extras
#+name: get_loglambdas
#+BEGIN_SRC python :noweb no-export :results silent

"""
This code does not yet work in this file. Come back to it...
"""
  df = TUP.process_data(C, HH, T, year = year) #~ Process_data() builds consumption data if not given as an argument
  df['Constant']=1
  df["CTL"] = 1-df["TUP"] #~ Code the cash group as controls since they're not in the midline analysis

  explist=[s[2:-2] for s in df.columns[[s.startswith('c_') and s.endswith(year[1]) for s in df.columns]]]
  df = df.rename(columns= lambda x: x[:-2] if x.endswith(year[1]) else x)

  bothdf=[]
  xvars=['hh_size_b','child_total_b','Loc']
  for x in explist:
      if 'c_'+x+r'_b' not in df:
          #~ When you take out the baseline controls in favor of repeated cross-sections, this is where to start...
          print(x+" has no baseline data or had too few non-zero responses at baseline. Skipping.")
          continue
      ydf=pd.DataFrame(df[['c_'+x]].rename(columns={'c_'+x:x.capitalize()}).stack())
      rdict=dict(zip(xvars+['c_'+x+r'_b'],["%s_%s" % (s,x.capitalize()) for s in xvars]+['Baseline_%s' % x.capitalize()]))
      xdf=pd.DataFrame(df[xvars+['c_'+x+r'_b']])
      xdf.index=pd.MultiIndex.from_tuples([(i,x.capitalize()) for i in xdf.index])
      locations=pd.get_dummies(xdf['Loc'],prefix='Loc_%s' % x.capitalize())
      del xdf['Loc']
      xdf.rename(columns=rdict,inplace=True)
      xdf=xdf.join(locations)
      xdf.replace(to_replace=np.NaN,value=0,inplace=True)

      # Add row to restrict location dummies to sum to one
      ydf=pd.concat([ydf,pd.DataFrame([0],index=[(0,x.capitalize())])])
      xdf=pd.concat([xdf,pd.DataFrame([s.startswith('Loc_')+0. for s in xdf.columns],index=xdf.columns,columns=[(0,x.capitalize())]).T]) 

      xdf[0]=ydf
      xdf.dropna(how='any',inplace=True)
      bothdf.append(xdf)

  #~ Are this fillna() call and the xdf.replace call above a problem? It seems necessary for the block-diagonal ols function
  #~ we're using, but aren't we coding zeros as missing and calculating residuals for only those positive consumption? Wouldn't
  #~ replacing them to zero insert some non-zero residual for households that never consume a given good?
  #~ And isn't this the motivation behind svd_missing?
  mydf=pd.concat(bothdf).fillna(value=0)

  X=mydf.iloc[:,1:]

  y=mydf[[0]]

  x=np.exp(y.unstack().iloc[1:,:]) # Expenditures (in levels)
  xshares=x.divide(x.sum(axis=1),axis=0).fillna(value=0).mean() # Expenditure shares (taking missing as zero)
  xshares.index=xshares.index.droplevel(0)

  b,se=ols(X,y)

  ## betahat=b[['Constant_%s' % s.capitalize() for s in explist]]
  ## betahat.rename(columns=dict(zip(betahat.columns,[s.capitalize() for s in explist])),inplace=True)

  e=y-X.dot(b.T)

  e.rename(columns={0:'Resid'},inplace=True)
  e.index.names=['HH','Good']

  testdf=pd.merge(df[['TUP','CTL']].reset_index(),e.reset_index(),how='outer',on=['HH'])
  testdf.set_index(['HH','Good'],inplace=True)

  TUP=testdf['TUP'].mul(testdf['Resid']).dropna().unstack()
  CTL=testdf['CTL'].mul(testdf['Resid']).dropna().unstack()

  e=(e-e.mean()).unstack()

  # Test of significant differences between treatment and control:
  # Weighting matrix:
  A=np.matrix((TUP-CTL).cov().as_matrix()).I
  g=np.matrix((TUP-CTL).mean())
  J=e.shape[0]*g*A*g.T # Chi2 statistic

  p=1-chi2.cdf(J,e.shape[1])

  chi2test="Chi2 test: %f (%f)" % (J,p)

  N=pd.Series([d.shape[0]-1 for d in bothdf],index=[d.index.levels[1][0] for d in bothdf])

  resultdf=pd.DataFrame({'TUP':TUP.mean(),'CTL':CTL.mean(),'$N$':N})
  sedf=pd.DataFrame({'TUP':TUP.std()/np.sqrt(resultdf['$N$']),'CTL':CTL.std()/np.sqrt(resultdf['$N$'])})
  resultdf['Diff.']=resultdf['TUP']-resultdf['CTL']
  sedf['Diff.']=np.sqrt((sedf['TUP']**2) + (sedf['CTL']**2))

  # Use svd (with missing data) to construct beta & log lambda

  myb,myl = get_loglambdas(e,TEST=True)

  myb.index=myb.index.droplevel(0)

  # Normalize log lambdas
  l=myl/myl.std()
#+END_SRC

#+name: residuals_by_group
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
def residuals_by_group(models, groups, outcomes=[], kind="kde", figure_dir="../figures/", seriesname="Treat", blanks_to = "Control"):
    """
     Takes a set of statsmodels regression results and,
     for each outcome, produces a plot comparing the
     distribution of residuals by group.

     models:
         A dictionary of the form {variable name: sm.RegressionResults}. Empty defaults to all available.
     groups:
         A list, pd.Series, or pd.DataFrame with variables
         (A later version could contain an arbitrary set of categorical and give groups for every combination.)
     outcomes:
         A list specifying which variables in models to make plots for.
     kind:
         kde (or density) or histogram (or hist)
         Density plots are on a single axis. Histograms are stacked by group.
     figure_dir:
         The directory into which the figures get saved. If doesn't exist, throws error (future version might make that directory on the fly.)
     Seriesname:
         If a series or list is passed without a name, defaults to `seriesname'
     blanks_to:
         Observations with no treatment status from "groups" gets renamed to `blanks_to'
    """

    #~ Make outcomes a list. If empty, defaults to all variables in models
    if type(outcomes)==str: outcomes=[outcomes]
    if not outcomes: outcomes = sorted(models.keys())

    #~ Make data frame and make "Group" categorical
    df = pd.DataFrame(groups).rename(columns={0:seriesname})
    for var in df: df[var] = df[var].applymap(lambda x: var if x else "")
    df["Group"] = df.sum(axis=1).replace("",blanks_to)

    #~ Make residuals
    for var in outcomes:
        #~ Make column of residual values
        resid_var = "resid_{}".format(var)
        df[resid_var] = models[var].resid
        #~ Groupby object
        groups = df.dropna(subset=[resid_var]).groupby("Group")[resid_var]

        #~ Plot density by group
        if kind in ("kde", "density"):
            fig, ax = plt.subplots()
            groups.plot(kind=kind, ax=ax, legend=True)
            fig.savefig(figure_dir+resid_var+".png")

        #~ Plot histograms by group
        elif kind in ("hist", "histogram"):
            i=0
            fig, ax = plt.subplots(len(set(df["Group"])),1,sharex=True)
            for group, data in grps[var]:
                ax[i].hist(data.values, bins=20)
                ax[i].set_title(group)
                i+=1
            i=0
            fig.savefig(figure_dir+resid_var+".png")
        print(resid_var+".png created.")


#+end_src
   
* Code Appendix

** Code block to load data & merge in treatment information
#+name: load_data
#+begin_src python :noweb noexport :exports none  :results output table raw labels=[] :colnames no
data_baseline  = "../../TUP-data/Baseline/TUP_baseline.dta"
data_assignment  = "../../TUP-data/random_assignment.dta"
import pandas as pd
import numpy as np
from cfe.df_utils import df_to_orgtbl

df_b = pd.read_stata(data_baseline)
df_a = pd.read_stata(data_assignment)

# merge baseline data and group column from random assignment data based on respondent id/id number
df = pd.merge(df_b, df_a[ ['respid', 'group'] ], how='inner', left_on="idno", right_on="respid")
#+end_src


 
