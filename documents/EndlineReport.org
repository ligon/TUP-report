:SETUP:    
#+TITLE: Valuing Assets Provided to Low-Income Households in South Sudan
#+AUTHOR: Reajul Chowdhury, Elliott Collins, Ethan Ligon, Kaivan Munshi 
#+DATE: December 15, 2016
#+OPTIONS: texht:t toc:nil
#+LATEX_CLASS_OPTIONS: [12pt,article]
#+LATEX_HEADER:       \newcommand{\T}{\top}
#+LATEX_HEADER:       \newcommand{\E}{\ensuremath{\mbox{E}}}
#+LATEX_HEADER:       \newcommand{\R}{\ensuremath{\mathbb{R}}}
#+LATEX_HEADER:       \newcommand{\one}{\ensuremath{\mathbbm{1}}}
#+LATEX_HEADER:       \newcommand{\Eq}[1]{(\ref{eq:#1})}
#+LATEX_HEADER:       \renewcommand{\vec}[1]{\boldsymbol{#1}}
#+LaTeX_HEADER:       \usepackage{biblatex}
#+LaTeX_HEADER:       \bibliography{prospectus}
#+LaTeX_HEADER:       \usepackage[style=authordate]{biblatex}
#+LATEX_HEADER_EXTRA: \usepackage{bbm}
#+LATEX_HEADER_EXTRA: \usepackage{dcolumn}\newcolumntype{d}[1]{D{.}{.}{#1}}
#+LATEX_HEADER_EXTRA: \newtheorem{proposition}{Proposition} \newcommand{\Prop}[1]{Proposition \ref{prop:#1}}
#+LATEX_HEADER_EXTRA: \newtheorem{theorem}{Theorem} \newcommand{\Thm}[1]{Theorem \ref{thm:#1}}
#+LATEX_HEADER_EXTRA: \newtheorem{remark}{Remark} \newcommand{\Rem}[1]{Remark \ref{rem:#1}}
#+LATEX_HEADER_EXTRA: \newtheorem{condition}{Condition} \newcommand{\Cond}[1]{Condition \ref{cond:#1}}
#+LATEX_HEADER_EXTRA: \newtheorem{lemma}{Lemma} \newcommand{\Lem}[1]{Lemma \ref{prop:#1}}
#+LATEX_HEADER_EXTRA: \newcommand{\Fig}[1]{Figure \ref{fig:#1}} \newcommand{\Tab}[1]{Table \ref{tab:#1}}
:END:

#+begin_abstract

Several previous studies have found that the ``graduation'' or ``Transfers to the
Ultra-Poor'' (TUP) framework is an effective approach to alleviating the constraints
that prevent extremely poor households from increasing their productivity. The
framework consists of a sizable transfer of productive physical capital, coupled with
training and continuous support over the course of one or two years. A second and
related literature suggests that unconditional cash transfers (UCT's) may have a
comparable effect. This field experiment, examining the first two years of BRAC's TUP
pilot in South Sudan, offers a direct comparison of these very different (but
similarly expensive) approaches to alleviating capital constraints. We consider how
households' response to each may be affected by South Sudan's unique economic
conditions in 2014 and 2015, which has faced a level of instability against which TUP
framework has not been rigorously studied. We find evidence of positive consumption
effects from both treatments, but a persistent wealth effect only from the TUP. We
also elicit suggestive evidence that BRAC's support may have helped TUP beneficiaries
cope with the short-term economic effects of the outbreak of violence in 2014.

#+end_abstract
\newpage

* Introduction
 
Poor rural households typically earn money from low-return activities like
small-scale cultivation or casual day labor. One can reasonably expect that they will
face both financial and human capital constraints, keeping them from investing 
and expanding into more lucrative activities. Experience and research over many years
has lead many to believe that households facing particularly acute poverty are unable
to solve this problem through the small, high-interest loans typically marketed to
them. It was these considerations that lead to the development of the initial
``Transfers to the Ultra-Poor'' (TUP) program in Bangladesh. First implemented by
BRAC in 2007, the program aims to simultaneously alleviate physical and human capital
constraints by providing households with a significant transfer of food and productive assets,
followed by two years of training and support by extension officers. The general
framework[fn:: known as the ``graduation framework'' pointing to the original
ambition to move households into an activity where they are able to finance further
income growth without costly transfers.]
has since expanded to a wide range of countries, with a general pattern of
success in increasing aggregate investment, labor supply, and aggregate consumption.
(Banerjee et al., 2015) (Bandiera et al., 2016)

A second and related literature has gained new interest in parallel with this
literature which examines the effect of offering direct unconditional cash transfers
(UCT's) to poor households. (Haushoffer & Shapiro, 2016) (Blattman et al., 2014)
(Blattman et al., 2013) While this and the TUP framework are both direct capital transfer
interventions, they are very different in their approach, with TUP programs guiding
and constraining the use of capital towards productive investment while UCT's allow
households to invest and consume as they see fit.

Here, we examine the experimental evaluation of BRAC's pilot TUP program in South
Sudan and compare it to a round of unconditional cash transfers. Our results
contribute to the general literature in two important ways. First, South Sudan's
political and economic institutions have overwhelmingly politically unstable since
this study's inception, which may affect the value of the program for
households in important ways. Second, a randomly selected group of households
received cash transfers equal in market value to the assets provided to the TUP
households. While an experimental literature has been established studying the
graduation framework in isolation, this is among the first experiments attempting to
directly compare it to a obvious alternative investment.

* The Program
   
The pilot program itself was similar to the other TUP programs completed by BRAC. It
consisted of four phases: targeting and selection, training and enterprise selection,
asset transfers, and monitoring. 

*** Targeting, Selection, & Training

The fist phase of the program was to complete a census of households in the area
around BRAC's office in the town of Yei in Western Equitoria. This census contained
questions to assess eligibility for the program. First, households were excluded if
they had a salaried worker in the household, were participating in another NGO
program, or had no access to cultivable land (which was in some cases necessary for
the program's model). Households were then deemed eligible if they fit at least three
criteria in a list of five poverty indicators.[fn:: These criteria were that the
household had a head working as a day laborer (generally an occupation with poverty
wages), two or more children, at least one child working, fewer than three rooms, or
a woman who has not completed secondary school.] The census was completed in April of
2013 and 745 were identified as eligible. Of these, 649 were identified in a baseline
survey. These households were stratified on employment, asset ownership, and size and
selected into treatment groups. 250 were enrolled in the TUP program, 125 in the UCT
group, and the final 274 in a pure control group.

*** Asset Transfers & Monitoring

The second phase of the program was training and enterprise selection. Unlike most
programs of this type, the number of households given each kind of asset was set in
advance, with 75 enrolled in agricultural activities (vegetable cultivation), 85 in
duck rearing, 45 in goat rearing, and the rest in small trade businesses. While the
staff tried to map housheolds' asset types to their respective preferences and
skills, a disproportionate number stated a preferences for goats and small trade.
Households then atttended training sessions. The first of these were for general
business skills around literacy, numercacy, and financial management. The next were
sector specific and focused on how to properly raise livestock or gardens. 

Once training is completed, asset transfers began in late 2013 and continued through
the first few months of 2014. The productive assets related to each enterprise were
valued at around $240 per household, with a random subset recieving an additional $60
in assets later in 2014. Shortly thereafter, households started to attend weekly or
semi-weekly meetings with other nearby participants to discuss with each other and a
BRAC extension officer the details of their businesses. These meetings also included
food transfers for a while, which were designed to help get households to the point
of receiving revenue from their assets without having to sell them.

In all, the market value of these food transfers were valued at $110, bringing the
total value of all transfers to $350-$410. The 125 households in the UCT group were
randomly divided in half to receive cash in these amounts. Unfortunately, political
instability disrupted NGO operations throughtout South Sudan, preventing the
simultaneous disbursal of the cash and asset transfers. Instead, a second survey was
conducted in June of 2014, with the cash transfers being disbursed immediately
thereafter. This resulted in a timing difference of 3 to 6 months between the two.

** The Data

The census was conducted in April of 2013 in the area around BRAC's offices in Yei
County to identify women eligible for participation. A baseline survey was conducted
that Summer, which successfully interviewed 649 of these women and randomly selected
them into the TUP, UCT, and control groups. Half of each beneficiary group was
randomly selected to receive additional "top-up" transfers with market value of $60
(around 20% of the original transfers).

In response to the outbreak of violence in late 2013 and subsequent closing of the
offices in Yei, a midline survey was conducted in June 2014 to try to separate pre-
and post-conflict changes in outcomes. For lack of a valid comparison group, we will
not speak with any authority about the effect of the conflict on economic conditions
in Yei, though we will report estimates of treatment effects on the severity or
likelihood of having been effected exposure to the conflict. Some of the original
asset transfers were done before the office closure, which may affect estimates of
the difference between programs if rates of return changed in the few intervening
months. Finally, an endline survey was conducted in mid-2015 to estimate the effect
of program participation on households' financial situation and overall welfare. The
key here is that the survey conducted in mid-2014 provides us with /short-term/
treatment effects of the TUP program within 6 months of the asset transfers, while
providing a second baseline for the Cash transfers. Likewise, the 2015 survey
allows us to estimate treatment effects one year after the cash transfers, and 15-18
months after the asset transfers.

** Empirical Strategy 

We estimate a single model using interactions between time effects and group
assignment, as well as baseline values of the outcome variable where available. 

\begin{equation*}
Y_{it} =\sum_{t=2014}^{2015}\delta_{t}+\beta_{t}^{Cash}I_{t}*Cash_{it}+\beta_{t}^{TUP}I_{t}*TUP_{it}+\gamma Y_{i,2013}+\epsilon_{i}
\end{equation*}

where $\delta_{t}$ are time fixed effects and $I_{t}$ is an indicator if the year
/t/, and $Y_{it}$ is an outcome of interest for household /i/ in year /t/. The
interaction of /Cash/ and /2015/ is the endline treatment effect of the cash
treatment. We take the interactions of TUP assignment with 2014 and 2015 indicators
as the treatment effects at 3-6 and 15-18 months respectively. The analagous
interactions with the Cash group offer a second baseline and a 12-month treatment
effect, respectively. Since those transfers happened after the midline survey, its
interaction with /2014/ acts as a placebo; there is no /ex ante/ reason to expect
that they were different from the rest of the control group at that point.
Given the slight difference in timing, we report a t-test of the hypothesis
\beta_{TUP,t}-\beta_{Cash,2015}=0 for both $t \in {2014,2015}$. Since the difference
in timing is much smaller, we consider \beta_{TUP,2015}-\beta_{Cash,2015}=0 to be the
central hypothesis of interest.
 
* Results
** Randomization Check

A crucial assumption is that the treatment and control groups were selected
appropriately. We check this by presenting summary statistics by group on a
range of factors related to consumption, asset holdings, and household
characteristics. We check for balance on observables in Table 1.

#+name: balance_check
#+BEGIN_SRC python :dir ../analysis :results value table :exports none
import check_balance
return check_balance.tables
#+END_SRC

#+name: tab:balance_check
#+caption: Means of some analysis variables at baseline.  Asterisks indicate p<.1, .05, and .01 respectively
#+attr_latex: :environment longtable :align lrrrrr
|-----------------+---------+-------------+--------------+-----|
| Consumption     |     CTL |  \Delta TUP |   \Delta CSH | $N$ |
|-----------------+---------+-------------+--------------+-----|
| Meat            |    4.21 |      -0.568 |       -0.052 | 378 |
| Fuel            |    0.76 |      -0.039 |       -0.072 | 456 |
| Clothesfootwear |    0.67 |      -0.026 |        0.033 | 595 |
| Soap            |    0.48 |      -0.008 |       -0.026 | 536 |
| Fish            |    2.50 |      -0.154 |       -0.156 | 474 |
| Charities       |    0.03 |      -0.006 |          0.0 | 134 |
| Cereals         |    9.19 |      -0.947 |         0.27 | 605 |
| Transport       |    0.18 |      -0.033 |        0.002 | 193 |
| Cosmetics       |    0.68 |       0.027 |       -0.125 | 468 |
| Sugar           |    1.71 |      -0.078 |       -0.189 | 604 |
| Egg             |    1.10 |      -0.091 |        0.038 | 276 |
| Oil             |    1.36 |       -0.13 |       -0.141 | 613 |
| CSH             |    0.00 |         0.0 |          1.0 | 125 |
| Ceremonies      |    0.13 |       0.006 |        0.026 | 152 |
| Beans           |    0.70 |       0.232 |        0.226 | 192 |
| Fruit           |    0.69 |      -0.089 |        0.001 | 272 |
| Textiles        |    0.16 |      -0.004 |    0.056^{*} | 376 |
| Utensils        |    0.25 |      -0.009 |        0.008 | 442 |
| Dowry           |    1.27 |      -0.041 |        0.028 | 126 |
| Furniture       |    0.20 |      -0.014 |        0.045 | 368 |
| TUP             |    0.00 |         1.0 |          0.0 | 249 |
| Salt            |    0.45 |      -0.026 |        0.007 | 617 |
| Vegetables      |    1.54 |      -0.165 |        -0.18 | 471 |
|-----------------+---------+-------------+--------------+-----|
| Assets          |     CTL |  \Delta TUP |   \Delta CSH | $N$ |
|-----------------+---------+-------------+--------------+-----|
| Smallanimals    |  236.60 |     -86.068 |     -123.133 | 123 |
| Bicycle         |  109.08 |     -12.555 |      -11.414 | 171 |
| CSH             |    0.00 |         0.0 |          1.0 | 125 |
| Radio           |   58.45 |      -5.968 |      -16.529 | 260 |
| Motorcycle      |  341.74 |     192.956 | 353.836^{**} |  93 |
| TUP             |    0.00 |         1.0 |          0.0 | 249 |
| Net             |   19.16 |       0.668 |        0.247 | 423 |
| Poultry         |   42.40 |      -3.365 |       -8.894 | 161 |
| Bed             |  241.27 |       7.992 |       32.762 | 521 |
| Chairtables     |  206.79 |     -29.368 |        3.617 | 531 |
| Mobile          |   97.54 |      12.627 |       -4.198 | 414 |
| Netitn          |    7.82 |       1.215 |        1.178 | 181 |
| Cosmetics       |    0.68 |       0.027 |       -0.125 | 468 |
|-----------------+---------+-------------+--------------+-----|
| Household       |     CTL |  \Delta TUP |   \Delta CSH | $N$ |
|-----------------+---------+-------------+--------------+-----|
| Daily Food      |   25.18 |      -2.215 |       -0.261 | 643 |
| Daily Exp       |   29.90 |      -2.167 |       -0.288 | 646 |
| No. Houses      |    2.83 |       0.031 |        0.118 | 543 |
| In Business     |    0.40 |       0.038 |        0.017 | 265 |
| Cereals         |    9.19 |      -0.947 |         0.27 | 605 |
| # Child         |    3.26 |       0.118 |        0.108 | 594 |
| Asset Tot.      | 1757.05 |     -44.791 |       98.654 | 603 |
| Cash Savings    |  236.90 |       28.52 |      -66.812 | 431 |
| HH size         |    7.23 |      -0.175 |          0.3 | 648 |
|-----------------+---------+-------------+--------------+-----|

This is simply suggestive evidence that the treatment and control groups were similar
in observables at baseline, with the exception that the cash group has atypically
more motorcycles and clothing. But it does suggests that our stratified randomization
was not too far from creating comparable groups.

** Consumption

The first measure of welfare to consider is household consumption, defined as the market
value of goods or services used by the household. A sizable basket of goods were
included in the survey module. These are separated into three categories: Food items
(with a 3-day recall window), non-durables (a 30-day recall window), and durables
and large expenditures (a one-year recall window). Consumption, as both the total amount
and the composition of household spending, is perhaps the most appropriate
measure of the welfare or poverty of a household in our survey. 

The results for several important consumption measures are presented in Table
\ref{tab:consumption}. Importantly, we do not know about prices for each good in this
time, though we can say that inflation was as high as 100% between 2014 and 2015.
Nonetheless, we take the sum of all consumption and expenditure questions together as
a measure of welfare. [fn:: Details on this issue are discussed further in Beegle
(2012).]

The main result is that TUP participants had higher consumption consumption in 2014,
a few months into the primary monitoring phase after the asset transfers. Similarly,
the Cash group has higher consumption in 2015, measured just over a year after
disbursal. Food transfers had ceased weeks before the 2014 survey was conducted, and
the assets had been transferred 6-8 months prior. The TUP group sees no notable
difference from control in that period. The short-term consumption effects of either
program are economically significant, representing a 23% and 14% increase in average
total consumption for TUP and Cash, respectively.

These results are consistent with a story in which either sort of transfer has a
short-term consumption effect. Importantly, we do not reject the null hypothesis that
the two effects are equal to one another. In either group, the increase in total
consumption appears to be driven mainly by increased food consumption, with smaller
effects on non-food consumption goods and durables. As such, there is no evidence
that the share of food consumed falls, as might be predicted by Engel's law.

\newpage

#+name: consumption_aggregate_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<consumption_analysis>>
return tab
#+end_src

#+name: tab:consumption
#+caption: Average treatment effects by Group-Year, controlling for baseline levels.
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
|                                  | Tot           | logTot        | Food          | FoodShr       | Non-durable   | Durable       |
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
| CTL mean                         | $39.80^{*}$   | $ 3.52^{***}$ | $27.46^{*}$   | $ 0.70^{***}$ | $ 9.73$       | $ 3.07$       |
|                                  | $(22.18)$     | $( 0.61)$     | $(15.54)$     | $( 0.18)$     | $(10.38)$     | $( 5.48)$     |
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
| TUP*2014                         | $ 9.34^{***}$ | $ 0.23^{***}$ | $ 6.12^{***}$ | $-0.01$       | $ 1.94^{*}$   | $ 1.28^{**}$  |
|                                  | $( 2.26)$     | $( 0.06)$     | $( 1.57)$     | $( 0.02)$     | $( 1.02)$     | $( 0.50)$     |
| TUP*2015                         | $ 1.69$       | $ 0.04$       | $ 0.72$       | $-0.01$       | $ 1.13$       | $ 0.09$       |
|                                  | $( 2.15)$     | $( 0.05)$     | $( 1.50)$     | $( 0.01)$     | $( 0.96)$     | $( 0.47)$     |
| CSH*2014                         | $-1.03$       | $-0.02$       | $-0.97$       | $ 0.01$       | $ 0.96$       | $-0.38$       |
|                                  | $( 2.80)$     | $( 0.07)$     | $( 1.95)$     | $( 0.02)$     | $( 1.28)$     | $( 0.62)$     |
| CSH*2015                         | $ 5.66^{**}$  | $ 0.14^{**}$  | $ 3.50^{*}$   | $-0.01$       | $ 2.17^{*}$   | $ 0.06$       |
|                                  | $( 2.75)$     | $( 0.07)$     | $( 1.91)$     | $( 0.02)$     | $( 1.24)$     | $( 0.61)$     |
| Bsln_NAN                         | $ 6.83^{***}$ | $ 0.31^{***}$ | $ 6.13^{***}$ | $ 0.09^{***}$ | $-0.74$       | $ 0.80^{*}$   |
|                                  | $( 2.47)$     | $( 0.09)$     | $( 1.68)$     | $( 0.03)$     | $( 0.87)$     | $( 0.43)$     |
| 2014                             | $35.09^{***}$ | $ 3.25^{***}$ | $26.03^{***}$ | $ 0.69^{***}$ | $ 8.32^{***}$ | $ 2.12^{***}$ |
|                                  | $( 1.89)$     | $( 0.08)$     | $( 1.30)$     | $( 0.03)$     | $( 0.80)$     | $( 0.36)$     |
| 2015                             | $35.93^{***}$ | $ 3.29^{***}$ | $24.62^{***}$ | $ 0.64^{***}$ | $10.14^{***}$ | $ 2.74^{***}$ |
|                                  | $( 1.77)$     | $( 0.08)$     | $( 1.22)$     | $( 0.03)$     | $( 0.74)$     | $( 0.33)$     |
| Bsln2013                         | $ 0.10^{***}$ | $ 0.06^{***}$ | $ 0.07^{**}$  | $ 0.07^{**}$  | $-0.11$       | $ 0.05$       |
|                                  | $( 0.03)$     | $( 0.02)$     | $( 0.03)$     | $( 0.03)$     | $( 0.15)$     | $( 0.03)$     |
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
| F-stat                           | $ 4.83$       | $ 4.77$       | $ 5.79$       | $ 6.30$       | $ 2.23$       | $ 2.12$       |
| N                                | $1305.00$     | $1305.00$     | $1295.00$     | $1295.00$     | $1296.00$     | $1260.00$     |
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $ 3.68$       | $ 0.09$       | $ 2.61$       | $-0.01$       | $-0.23$       | $ 1.22$       |
|                                  | $( 3.51)$     | $( 0.09)$     | $( 2.44)$     | $( 0.02)$     | $( 1.59)$     | $( 0.78)$     |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $-3.97$       | $-0.10$       | $-2.78$       | $-0.00$       | $-1.04$       | $ 0.03$       |
|                                  | $( 2.85)$     | $( 0.07)$     | $( 1.98)$     | $( 0.02)$     | $( 1.30)$     | $( 0.64)$     |
|----------------------------------+---------------+---------------+---------------+---------------+---------------+---------------|

** Food Security

Generally speaking, observed changes in total consumption don't translate into an
increase in reported food security. In each year, we ask how often in a given week
the respondent has had experiences indicative of food insecurity. Included are (from
left to right) going a whole day without eating, going to sleep hungry, being without
any food in the house, eating fewer meals than normal at mealtimes, and limiting
portions. We report the percentage of people who report experiencing each in a
typical week, as well as a standardized composite z-score using all of these
questions. There is little evidence of a significant treatment effect at endline.

#+name: foodsecure_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<foodsecure_analysis>>
return tab
#+end_src

#+name: tab:foodsecure
#+caption: Percentage of respondents reporting a food security problem occurs at least once a week.
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+--------------+---------------+---------------+---------------+---------------+---------------|
|                                  | Z-score      | Whole Day     | Hungry        | No Food       | Fewmeals      | Portions      |
|----------------------------------+--------------+---------------+---------------+---------------+---------------+---------------|
| CTL mean                         | $-0.01$      | $ 0.21$       | $ 0.21$       | $ 0.28$       | $ 0.32$       | $ 0.36$       |
|                                  | $( 1.00)$    | $( 0.41)$     | $( 0.40)$     | $( 0.45)$     | $( 0.47)$     | $( 0.48)$     |
|----------------------------------+--------------+---------------+---------------+---------------+---------------+---------------|
| TUP*2014                         | $-0.10$      | $-0.02$       | $-0.05$       | $-0.03$       | $ 0.01$       | $ 0.01$       |
|                                  | $( 0.09)$    | $( 0.03)$     | $( 0.03)$     | $( 0.03)$     | $( 0.04)$     | $( 0.04)$     |
| TUP*2015                         | $-0.02$      | $ 0.03$       | $-0.01$       | $-0.03$       | $ 0.05$       | $-0.02$       |
|                                  | $( 0.09)$    | $( 0.03)$     | $( 0.03)$     | $( 0.03)$     | $( 0.04)$     | $( 0.04)$     |
| CSH*2014                         | $-0.05$      | $-0.00$       | $-0.04$       | $-0.01$       | $-0.03$       | $-0.00$       |
|                                  | $( 0.11)$    | $( 0.04)$     | $( 0.04)$     | $( 0.04)$     | $( 0.05)$     | $( 0.05)$     |
| CSH*2015                         | $ 0.03$      | $ 0.06$       | $ 0.03$       | $-0.01$       | $-0.00$       | $-0.04$       |
|                                  | $( 0.11)$    | $( 0.04)$     | $( 0.04)$     | $( 0.04)$     | $( 0.05)$     | $( 0.05)$     |
| Bsln2013                         | $ 0.07^{**}$ | $-0.00$       | $ 0.02$       | $ 0.03$       | $ 0.06^{**}$  | $-0.02$       |
|                                  | $( 0.03)$    | $( 0.02)$     | $( 0.02)$     | $( 0.02)$     | $( 0.03)$     | $( 0.03)$     |
| 2014                             | $ 0.07$      | $ 0.09^{***}$ | $ 0.10^{***}$ | $ 0.09^{***}$ | $ 0.17^{***}$ | $ 0.22^{***}$ |
|                                  | $( 0.06)$    | $( 0.02)$     | $( 0.02)$     | $( 0.03)$     | $( 0.03)$     | $( 0.03)$     |
| 2015                             | $ 0.03$      | $ 0.22^{***}$ | $ 0.21^{***}$ | $ 0.26^{***}$ | $ 0.30^{***}$ | $ 0.39^{***}$ |
|                                  | $( 0.06)$    | $( 0.02)$     | $( 0.02)$     | $( 0.02)$     | $( 0.03)$     | $( 0.03)$     |
| Bsln_NAN                         | $-0.17^{*}$  | $-0.02$       | $-0.03$       | $ 0.03$       | $-0.02$       | $-0.08^{*}$   |
|                                  | $( 0.09)$    | $( 0.03)$     | $( 0.03)$     | $( 0.03)$     | $( 0.04)$     | $( 0.04)$     |
|----------------------------------+--------------+---------------+---------------+---------------+---------------+---------------|
| F-stat                           | $ 1.45$      | $ 9.34$       | $ 8.36$       | $10.84$       | $ 6.70$       | $ 5.91$       |
| N                                | $1299.00$    | $1282.00$     | $1297.00$     | $1293.00$     | $1297.00$     | $1292.00$     |
|----------------------------------+--------------+---------------+---------------+---------------+---------------+---------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $-0.13$      | $-0.08$       | $-0.08^{*}$   | $-0.01$       | $ 0.01$       | $ 0.05$       |
|                                  | $( 0.14)$    | $( 0.05)$     | $( 0.05)$     | $( 0.05)$     | $( 0.06)$     | $( 0.06)$     |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $-0.06$      | $-0.03$       | $-0.04$       | $-0.02$       | $ 0.06$       | $ 0.02$       |
|                                  | $( 0.12)$    | $( 0.04)$     | $( 0.04)$     | $( 0.04)$     | $( 0.05)$     | $( 0.05)$     |
|----------------------------------+--------------+---------------+---------------+---------------+---------------+---------------|

** Assets
   
We turn now to asset holdings for the households. Controlling for baseline asset holdings
where possible, we estimate treatment effects for total value of assets owned, total
value of potentially "productive" assets, as well as land and financial assets. 

*** Total Asset Holdings

Perhaps interestingly, the cash group does not appear to have seen an increase in the
value of assets measured, with negative and imprecise point estimates. The most
important result is that the TUP group has significantly more asset wealth than the
cash or control groups in the short term and two years after receipt of transfers.
The TUP group has a change of 536 SSP on average (43% increase over controls, p<.01).
So-called "Productive" assets include anything that could plausibly be used in
productive activity. [fn:: For now, we include in this list: small and large
livestock, farm equipment, mobiles, carts, sewing equipment, sheds, and shop
premises.] Here we see the TUP group has 320 SSP (95%) more in this area over the
control group, with a similar magnitude at midline.

Importantly, this is not due to a preciptous increase in assets reported over this
time. Note also that the effect on total assets is higher in absolute value than the
effect on productive asset value, suggesting that the increased wealth cannot be
explained purely by households holding onto asset transfers for the length of the
program's monitoring phase. Instead, the TUP group is the only one for whom total
measured asset holdings did not fall on average over these two years, which saw
hyperinflation and a significant aggregate economic downturn.

#+CAPTION: Measured asset wealth by group-year
#+NAME: fig:AssetTotal
[[../figures/AssetTotal_groupyear.png]] 

#+name: asset_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<asset_analysis>>
return tab
#+end_src

#+name: tab:assets
#+caption: Average treatment effects by group-year on total value (in SSP) of all assets measured and of productive assets measured
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+-----------------+----------------|
|                                  | Total           | Productive     |
|----------------------------------+-----------------+----------------|
| CTL mean                         | $1225.61$       | $337.60$       |
|                                  | $(1502.46)$     | $(605.57)$     |
|----------------------------------+-----------------+----------------|
| TUP*2014                         | $535.79^{***}$  | $361.80^{***}$ |
|                                  | $(154.02)$      | $(74.19)$      |
| TUP*2015                         | $624.79^{***}$  | $320.74^{***}$ |
|                                  | $(146.01)$      | $(68.68)$      |
| CSH*2014                         | $-125.86$       | $18.50$        |
|                                  | $(191.31)$      | $(95.80)$      |
| CSH*2015                         | $-49.99$        | $-5.00$        |
|                                  | $(187.32)$      | $(88.40)$      |
| Bsln2013                         | $ 0.08^{***}$   | $ 0.00$        |
|                                  | $( 0.02)$       | $( 0.01)$      |
| 2014                             | $1259.75^{***}$ | $465.53^{***}$ |
|                                  | $(112.68)$      | $(55.96)$      |
| 2015                             | $1124.61^{***}$ | $392.97^{***}$ |
|                                  | $(103.46)$      | $(50.21)$      |
| Bsln_NAN                         | $21.30$         | $-131.14^{**}$ |
|                                  | $(146.51)$      | $(51.35)$      |
|----------------------------------+-----------------+----------------|
| N                                | $1305.00$       | $1247.00$      |
| F-stat                           | $ 8.53$         | $10.19$        |
|----------------------------------+-----------------+----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $585.78^{**}$   | $366.79^{***}$ |
|                                  | $(239.76)$      | $(114.58)$     |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $674.78^{***}$  | $325.74^{***}$ |
|                                  | $(194.72)$      | $(92.26)$      |
|----------------------------------+-----------------+----------------|


*** Savings

Both treatment arms had significant impact on the average level of cash savings
within households. The TUP households are strongly encouraged to pay into a savings
account maintained by BRAC each time they meet. Anecdotally, this has discouraged
some women from attending the meetings, but it results in TUP participants being 44%
(20 pp) more likely to report having any savings at all. It's
worth noting though that since the TUP households also regard their savings behavior
as much more transparent to BRAC (and have received pressure to save from them) than
the other groups, these households may simply be more likely to reveal that they are
saving when asked. Among those who have savings, TUP households report having roughly
43% (81 SSP) more in value.

Cash households appear no more likely than the control households to report having
cash savings (around 45% in each group), but households that report saving report
having 47% (91.4 SSP) more in value. This is significantly less than was given to
these households, but combined with the short-term consumption results, goes some
distance in explaining the lack of effect on physical asset wealth. 

It is common in this community (and most in the region) to store non-perishable food
like maize, cassava, or millet as a form of savings. This would seem particularly
reasonable in a high-inflation context, where the price of grain had doubled in the
previous year. At least as many households report saving in food (53%) as in cash
(46%), with an average market value of 106 SSP. However, we find no evidence that
either treatment group increased food savings. [fn:: Note that food savings was not
measured at baseline, so these controls are omitted.]

Neither do we find evidence that either treatment increased the size or likelihood of
giving or receiving interhousehold transfers, either in cash or in kind. These
results are omitted since only 35 and 60 households reported giving and recieving
transfers respectively, with no difference in group means.

#+name: savings_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<savings_analysis>>
return Table
#+end_src

#+name: tab:Nonzero
#+caption: Average treatment effects by group-year on percentage of households reporting any savings or land access 
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+---------------|---------------+---------------+---------------|
| % > 0                            | Savings       | Food Sav      | LandCult      | LandOwn       |
|----------------------------------+---------------|---------------+---------------+---------------|
| CTL mean                         | $ 0.45$       | $ 0.82$       | $ 0.82$       | $ 0.90$       |
|----------------------------------+---------------|---------------+---------------+---------------|
| CSH*2014                         | $-0.06$       | $ 0.00$       | $-0.04$       | $-0.01$       |
|                                  | $( 0.06)$     | $( 0.04)$     | $( 0.04)$     | $( 0.04)$     |
| CSH*2015                         | $ 0.03$       | $ 0.02$       | $ 0.05$       | $ 0.02$       |
|                                  | $( 0.05)$     | $( 0.04)$     | $( 0.04)$     | $( 0.04)$     |
| TUP*2014                         | $ 0.22^{***}$ | $-0.02$       | $-0.03$       | $-0.00$       |
|                                  | $( 0.04)$     | $( 0.03)$     | $( 0.03)$     | $( 0.03)$     |
| TUP*2015                         | $ 0.21^{***}$ | $-0.03$       | $ 0.01$       | $-0.01$       |
|                                  | $( 0.04)$     | $( 0.03)$     | $( 0.03)$     | $( 0.03)$     |
| 2014                             | $ 0.43^{***}$ | $ 1.00^{***}$ | $ 0.83^{***}$ | $ 0.82^{***}$ |
|                                  | $( 0.04)$     | $( 0.02)$     | $( 0.06)$     | $( 0.05)$     |
| 2015                             | $ 0.39^{***}$ | $ 0.82^{***}$ | $ 0.77^{***}$ | $ 0.84^{***}$ |
|                                  | $( 0.04)$     | $( 0.02)$     | $( 0.05)$     | $( 0.05)$     |
| Bsln2013                         | $ 0.05$       |               | $ 0.05$       | $ 0.07$       |
|                                  | $( 0.04)$     |               | $( 0.05)$     | $( 0.04)$     |
| Bsln_NAN                         | $ 0.08^{*}$   |               | $ 0.05$       | $ 0.05$       |
|                                  | $( 0.04)$     |               | $( 0.06)$     | $( 0.05)$     |
|----------------------------------+---------------|---------------+---------------+---------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $ 0.19$       | $-0.04$       | $-0.07$       | $-0.02$       |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $ 0.18$       | $-0.05$       | $-0.03$       | $-0.03$       |
|----------------------------------+---------------|---------------+---------------+---------------|
| F-stat                           | $ 8.83$       | $15.60$       | $ 0.79$       | $ 0.76$       |
| N                                | $1259.00$     | $870.00$      | $1231.00$     | $1251.00$     |
|----------------------------------+---------------|---------------+---------------+---------------|


#+name: tab:Savings
#+caption: Average treatment effects by group-year on total value (in SSP) of all cash and food savings and area (in fedan) of land being cultiviated by the household (including rented or temporary-use) and owned by the household.
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+----------------+----------------+----------------+----------------|
| Amt.                             | Savings        | Food Sav       | LandCult       | LandOwn        |
|----------------------------------+----------------+----------------+----------------+----------------|
| CTL mean                         | $191.19$       | $114.78$       | $61.88$        | $46.00$        |
|----------------------------------+----------------+----------------+----------------+----------------|
| CSH*2014                         | $28.74$        | $ 0.22$        | $10.18$        | $10.50$        |
|                                  | $(42.93)$      | $(15.38)$      | $(15.07)$      | $(12.57)$      |
| CSH*2015                         | $91.40^{**}$   | $-14.34$       | $-39.18^{***}$ | $-32.37^{***}$ |
|                                  | $(40.89)$      | $(14.98)$      | $(14.90)$      | $(11.95)$      |
| TUP*2014                         | $-27.09$       | $17.16$        | $-4.76$        | $-3.02$        |
|                                  | $(29.76)$      | $(12.33)$      | $(11.94)$      | $(10.04)$      |
| TUP*2015                         | $81.33^{***}$  | $ 1.13$        | $-17.38$       | $-12.56$       |
|                                  | $(29.32)$      | $(12.26)$      | $(11.65)$      | $( 9.41)$      |
| 2014                             | $106.72^{***}$ | $62.03^{***}$  | $11.37$        | $17.31^{**}$   |
|                                  | $(24.85)$      | $( 8.36)$      | $( 9.94)$      | $( 8.56)$      |
| 2015                             | $163.04^{***}$ | $114.78^{***}$ | $61.52^{***}$  | $51.89^{***}$  |
|                                  | $(24.13)$      | $( 7.60)$      | $( 9.54)$      | $( 7.88)$      |
| Bsln2013                         | $ 0.05^{**}$   |                | $ 0.94$        | $-2.43$        |
|                                  | $( 0.02)$      |                | $( 3.07)$      | $( 1.95)$      |
| Bsln_NAN                         | $40.07^{*}$    |                | $-1.60$        | $-6.02$        |
|                                  | $(21.24)$      |                | $( 9.92)$      | $( 8.29)$      |
|----------------------------------+----------------+----------------+----------------+----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $-118.49$      | $31.50$        | $34.42$        | $29.35$        |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $-10.07$       | $15.47$        | $21.79$        | $19.80$        |
|----------------------------------+----------------+----------------+----------------+----------------|
| F-stat                           | $ 7.41$        | $ 7.14$        | $ 4.91$        | $ 3.72$        |
| N                                | $671.00$       | $777.00$       | $1042.00$      | $1114.00$      |
|----------------------------------+----------------+----------------+----------------+----------------|


*** Land Holdings

We also examine land ownership and cultivation in each year. We find no evidence that
either group is more or less likely to report owning or cultivating land, though this
may be in part because land ownership and cultivation is already very common.
However, members of the cash group who are involved in agriculture are found to be
cultivating significantly less land after the fact, which reports cultivating 65%
less and owning 70% less land than the control group. This raises the interesting
question of whether the cash group was likely to switch occupations from farming to
non-farm self-employment.

** Income

Income was reliably measured only in 2015, and so our estimates do not control for
baseline values. The control group in 2015 has a measured income of roughly 4325 SSP
per year, or roughly $540 US (assuming an exchange rate of around 8). The TUP group
sees a 327 SSP ($41 US, 7%) increase in annual average income, but with a fairly
skewed distribution and high standard errors. The related figure shows that total
income is not particularly different among groups. Perhaps the main lesson is that
the TUP group has measurably more reported livestock-related income, and less farm
income, indicating a shift away from farming. The cash group may exhibit some
substitution away from farm and livestock, but as is evident graphically, we do not
observe sizable changes in income for either treatment group. 

#+name: income_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<income_analysis>>
return tab
#+end_src

#+Caption: Distribution of total observed income by group
#+NAME: fig:Income_group
[[../figures/Income_group.png]] 



#+name: tab:Income
#+caption: Average treatment effects by group-year on total value (in SSP) of income reported in 2015 by sector.
#+attr_latex: :environment longtable :align lrrrrrrr
|---------------------------+----------------+----------------+-----------------+-----------------|
|                           | Farm           | Livestock      | Non-Farm        | Total           |
|---------------------------+----------------+----------------+-----------------+-----------------|
| CTL mean                  | $773.05$       | $640.33$       | $3774.49$       | $4325.54$       |
|---------------------------+----------------+----------------+-----------------+-----------------|
| TUP                       | $-142.20^{*}$  | $281.12^{**}$  | $86.24$         | $327.83$        |
|                           | $(77.21)$      | $(126.30)$     | $(469.48)$      | $(455.95)$      |
| CSH                       | $-26.15$       | $-83.81$       | $61.80$         | $ 7.92$         |
|                           | $(100.82)$     | $(177.25)$     | $(620.53)$      | $(600.43)$      |
|---------------------------+----------------+----------------+-----------------+-----------------|
| N                         | $531.00$       | $380.00$       | $606.00$        | $671.00$        |
| F-stat                    | $ 1.75$        | $ 3.48$        | $ 0.02$         | $ 0.28$         |
|---------------------------+----------------+----------------+-----------------+-----------------|
| $\beta^{TUP}-\beta^{CSH}$ | $-116.05$      | $364.94^{**}$  | $24.44$         | $319.91$        |
|                           | $(105.79)$     | $(174.74)$     | $(651.27)$      | $(629.93)$      |
|---------------------------+----------------+----------------+-----------------+-----------------|

** COMMENT Occupation
    
*** TODO Occupation 

Endline:
    household_roster.dta S1_6
    Check relationship code: S1_2

Baseline:
    income activities.dta (or income.dta, post income_cleanup.do)
    Occupation code: S3_2
    Check relationship code by merging on id and line no. from household roster.dta. 

** COMMENT Other Outcomes
   
*** Confidence & Autonomy
*** Vulnerability

** Exposure to Conflict

In 2014, households were surveyed shortly after the NGO's offices had re-opened in
the wake of the outbreak of widespread armed conflict. Respondents were asked a short
set of questions about whether they were directly affected, and if so, in what way.
There were only a few incidents of violence near Yei town at that point, and the most
directly involved ethnic groups made up a small portion of the local populace. There
is no clear comparison group to which we might compare our sample, and the economic
climate changed over this same period in several ways that were probably not directly
caused by the violence. As such, we have no clear means of identifying the effect of
the conflict itself on household welfare. Nonetheless, it is interesting to consider
correlates with self-reported exposure to the conflict, and to see if program
assignment had any effect on households' exposure or response.

Our main outcomes of interest are whether individuals say they were "worried" or
"directly affected" by the violence, unable to invest in a farm or business as a
result, migrated as a cautionary measure, or did something else to protect the lives
of family members. A final question among those who took no cautionary measures was
whether this because they did not have the means (i.e. "NoMeans"). TUP participants
are 24% (13 pp.) less likely to report having been "affected" by the conflict, and
38% (6 pp.) less likely to report that they were affected specifically by being
unable to plant crops or invest in their business. This was the second most common
way in which households reported being affected behind "needed to relocate or
migrate", where respondents are not clearly different. Nonetheless, this raises the
possibility that having received a significant asset transfer around the outbreak of
conflict may have helped mitigate the conflict's negative effect on investment and
protect households from being affected overall.

#+Caption: % of Sample reporting exposure to conflict by group.
#+NAME: fig:conflict_exposure
[[../figures/conflict_exposure.png]] 

#+name: conflict_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<conflict_analysis>>
return Table
#+end_src

#+name: tab:Income
#+caption: Average treatment effects by group-year on the probability of having been affected in a significant way by the outbreak of violence in late 2013
#+attr_latex: :environment longtable :align lrrrrrrr
|----------+---------------+---------------+---------------+---------------+---------------+---------------|
|          | Affected      | Migrated      | NoInvest      | NoMeans       | ProtectLives  | Worried       |
|----------+---------------+---------------+---------------+---------------+---------------+---------------|
| CTL mean | $ 0.53^{***}$ | $ 0.33^{***}$ | $ 0.16^{***}$ | $ 0.33^{***}$ | $ 0.38^{***}$ | $ 0.93^{***}$ |
|          | $( 0.03)$     | $( 0.02)$     | $( 0.02)$     | $( 0.02)$     | $( 0.03)$     | $( 0.01)$     |
| TUP      | $-0.13^{***}$ | $ 0.04$       | $-0.06^{**}$  | $-0.06$       | $ 0.02$       | $-0.02$       |
|          | $( 0.04)$     | $( 0.04)$     | $( 0.03)$     | $( 0.04)$     | $( 0.05)$     | $( 0.02)$     |
|----------+---------------+---------------+---------------+---------------+---------------+---------------|
| F-stat   | $ 9.20$       | $ 0.96$       | $ 3.95$       | $ 2.55$       | $ 0.19$       | $ 0.49$       |
| N        | $601.00$      | $655.00$      | $655.00$      | $655.00$      | $585.00$      | $603.00$      |
|----------+---------------+---------------+---------------+---------------+---------------+---------------|

\newpage
* Concluding Remarks
  
BRAC's South Sudan pilot of the TUP program represents the only such test of the
ultra-poor graduation framework conducted in an area of significant political and
economic instability. It also represents among the only direct comparisons of this
model to a similarly expensive unconditional cash transfer, arguably its most
sensible benchmark for success. As such, it provides suggestive evidence as to the
best way of transfering wealth in order to help poor and vulnerable households.

Cash transfers appear (at least over a short period) to increase consumption and
possibly shift investment from agriculture to non-farm activities, without a related
increase in wealth or income. Conversely, the TUP program increased wealth and
directly shifted work from agriculture to livestock, with increased consumption in
the short run. We also find that having received asset transfers dampened the
negative investment effects following the outbreak of violence. [fn:: Whether a cash
transfer would have had a similar mitigating effect is hard to say.] We tentatively
conclude that targeted asset transfers can play a constructive role in helping poor,
self-employed households when they face economic uncertainty. And while cash
increases household consumption, the goal of improving income or wealth is aided by
the additional services that the ultra-poor graduation framework offer.
  


* COMMENT Extra Analysis

** Good-level analysis

Next, \ref{tab:consumption_full} sets aside these aggregated measures to look more
carefully at potential changes in the composition of consumption in each group. Given
the large number of zeros, we use a linear model to consider first the 
frequency of non-zero consumption of each good among treatment and control
households, then look at levels of consumption among households with non-zero
consumption. \Tab{consumption_full} presents point estimates.

A few changes in the composition of consumption are interesting. TUP households appear to consume 17% less
sorghum (often considered an inferior good in Yei) and more on rice, which is
considered a higher-quality staple. While almost everyone reports some health
spending over the past month, both treatment groups spent more, though only
statistically significant in the cash group, which saw a 50% increase over the
control group. The cash group was also 30% (14 pp) more likely to have spent money
on funerals, though they did not spend more on average.

#+name: consumption_disaggreate_results
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
<<consumption_analysis>>
#+LATEX_HEADER_EXTRA: \usepackage{stringstrings}\renewcommand{\cite}[1]{\caselower[q]{#1}\citet{\thestring}}
from matplotlib import pyplot as plt
#~ Only keep disaggregate items
Goods = C.filter(regex="^c_").rename(columns=lambda col: col[2:] if col.startswith("c_") else col)
#~ 1 if coded as >0, 0 if zero, keep NaN's missing, since those are only from missing surveys.
Nonzero = Goods.applymap(lambda i: float(i>0) if not np.isnan(i) else np.nan)
#~ Restrict Goods df to positive responses.
Goods = Goods.replace(0,np.nan)
too_many_zeros = 30
many_zeros = [item for item in Goods if Goods[item].notnull().sum()<too_many_zeros]
Nonzero = Nonzero.drop(many_zeros,1)
Goods = Goods.drop(many_zeros,1)

#~ Merge in Control Vars
controls = ["cons","TUP","CSH"]
Goods = Goods.join(C[controls],how="left")
Nonzero = Nonzero.join(C[controls],how="left")
Items = [item[:-2] for item in Goods if item.endswith("_e")]
CTL = Goods[controls].sum(axis=1)==1 #~ i.e. only constant ==1, TUP & Cash ==0
Goods_ctl_mean =   Goods.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Zeros_ctl_mean = Nonzero.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])

Zero, Zero_se = reg_table(regressions(Nonzero,"_e", outcomes=Items, Baseline="_b"), resultdf=True, Transpose=True)
Good, Good_se = reg_table(regressions(Goods,"_e", outcomes=Items, Baseline="_b"  ), resultdf=True, Transpose=True)
#~ Make full table of Standard errors
SE = Zero_se[["TUP","CSH"]].join(Good_se[["TUP","CSH"]], lsuffix=" (%>0)", rsuffix=" (Amt.)")

#~ Make full table of point estimates and control-group means
FullTable = pd.DataFrame({"Mean (CTL)":Goods_ctl_mean, "% >0 (CTL)":Zeros_ctl_mean}).join(Zero[["TUP","CSH"]])
FullTable = FullTable.join(Good[["TUP","CSH","N"]], lsuffix=" (%>0)", rsuffix=" (Amt.)")

#~ Make % change graph
fig, ax = plt.subplots(2,1, figsize=(6,9))
for i, group in enumerate(("TUP","CSH")):
    pct_change = FullTable[group+" (Amt.)"]/FullTable["Mean (CTL)"]
    se_change  = SE[group+" (Amt.)"]/FullTable["Mean (CTL)"]
    pct_change.sort()
    pct_change.plot(kind="bar", yerr=SE[group+" (Amt.)"], ax=ax[i])
    ax[i].set_title(group, fontsize=6)
fig.savefig("../figures/Consumption.png")

FullTable = df_to_orgtbl(FullTable, sedf=SE)
return FullTable
#+end_src

#+name: tab:consumption_full
#+caption: Control group means and estimated treatment effects for percent consuming any and total amounts consumed.
#+attr_latex: :environment longtable :align lrrrrrrr
 #+RESULTS: consumption_disaggreate_results

** Disaggregate Asset Results 

#+name: asset_disaggreate_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<assets_disaggreate_analysis>>
return tab
#+end_src

#+name: tab:asset_disaggregate
#+caption: Control means and treatment effects for Assets owned by >40 households
#+RESULTS: asset_disaggreate_results
|--------------+---------+---------------+--------------+-----------+----------------+----------------+----------|
|              | # own   |               |              | Value     |                |                |          |
|--------------+---------+---------------+--------------+-----------+----------------+----------------+----------|
| Item         | CTL     | TUP           | Cash         | CTL       | TUP            | Cash           | N        |
|--------------+---------+---------------+--------------+-----------+----------------+----------------+----------|
| Pangas       | $ 1.06$ | $ 0.01$       | $ 0.13^{**}$ | $11.86$   | $ 1.66^{**}$   | $ 0.04$        | $410.00$ |
| Chickens     | $ 3.79$ | $ 0.70$       | $-0.32$      | $154.35$  | $23.31$        | $ 0.80$        | $162.00$ |
| Mobile       | $ 1.88$ | $-0.09$       | $ 0.08$      | $113.96$  | $ 2.62$        | $ 1.70$        | $569.00$ |
| Radio        | $ 1.62$ | $ 0.84$       | $-0.40$      | $57.25$   | $ 4.98$        | $ 5.10$        | $333.00$ |
| Shed         | $ 1.22$ | $-0.14$       | $-0.22$      | $48.81$   | $37.57$        | $ 6.81$        | $53.00$  |
| Stoves       | $ 1.44$ | $ 0.34$       | $ 0.31$      | $20.32$   | $18.19^{**}$   | $ 8.31$        | $84.00$  |
| Potspans     | $ 4.46$ | $-0.06$       | $-0.23$      | $102.73$  | $15.90$        | $-15.40$       | $582.00$ |
| Ducks        | $ 5.72$ | $ 4.26^{***}$ | $-0.16$      | $230.93$  | $109.99^{***}$ | $-19.34$       | $223.00$ |
| Motorcycle   | $ 1.51$ | $-0.48$       | $ 0.12$      | $2288.48$ | $300.46$       | $-196.32$      | $66.00$  |
| Chairtables  | $ 5.02$ | $ 0.25$       | $ 0.39$      | $167.62$  | $19.00$        | $-24.73$       | $638.00$ |
| Net          | $ 3.07$ | $ 0.03$       | $-0.08$      | $24.49$   | $ 0.66$        | $-3.81$        | $382.00$ |
| Axes         | $ 1.02$ | $ 0.03$       | $-0.02$      | $17.74$   | $ 0.02$        | $-3.94^{**}$   | $218.00$ |
| Smallanimals | $ 3.39$ | $ 0.29$       | $-0.90$      | $767.26$  | $-151.35$      | $-311.05^{**}$ | $155.00$ |
| Charcoal     | $ 2.20$ | $-0.26$       | $-0.83$      | $35.81$   | $-1.43$        | $-4.65$        | $176.00$ |
| Bicycle      | $ 6.34$ | $-5.46$       | $-5.52$      | $272.90$  | $-31.50$       | $-42.67$       | $135.00$ |
| Bed          | $ 3.17$ | $-0.23$       | $-0.40$      | $300.64$  | $19.32$        | $-57.78^{*}$   | $628.00$ |
| Tv           | $ 1.48$ | $-0.36$       | $-0.26$      | $380.45$  | $121.95$       | $348.23^{**}$  | $45.00$  |
|--------------+---------+---------------+--------------+-----------+----------------+----------------+----------|


* COMMENT Code appendix
  
** Food Security

 #+name: foodsecure_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
 D = full_data(balance=[])
 WEEKLY = True

 codes = {"1-2 times a week": 3,
         "3-6 times a week": 2,
         "Everyday": 1,
         "everyday": 1,
         "Less than once a week": 4,
         "less than once a week": 4,
         "Never": 5,
         "never": 5}

 recode = lambda x: codes.setdefault(x,x)

 Aval2013 = D.filter(regex="^fs_.*_b").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Aval2014 = D.filter(regex="^fs_.*_m").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Aval2015 = D.filter(regex="^fs_.*_e").rename(columns=lambda x: x[3:-2]).applymap(recode)

 if WEEKLY:
     weekly = lambda i: float(i<3) if pd.notnull(i) else np.nan
     Aval2013 = Aval2013.applymap(weekly)
     Aval2014 = Aval2014.applymap(weekly)      
     Aval2015 = Aval2015.applymap(weekly)
 
 index_vars = "worried,portions,fewmeals,nofood,hungry,wholeday".split(",")
 Outcomes = index_vars+["z-score"]
 #~ Creates Year dummies, z-scores and baseline values as `var'2013
 for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
     Aval["Year"]=Year
     if not weekly:
        for var in index_vars:
            Aval[index_vars] = (Aval[index_vars]-Aval[index_vars].mean())/Aval[index_vars].std()
     FS_sum = Aval[index_vars].sum(axis=1)
     Aval["z-score"] = (FS_sum-FS_sum.mean())/FS_sum.std()
     for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
    
 Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
 Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
 Vals = Vals.join(D[["TUP","CSH"]])

 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

 Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 Vals=Vals.loc[2014:2015]
 regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = Vals["TUP"]+Vals["CSH"] ==0
 CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+end_src

** Consumption

 #+name: consumption_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py

 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 from TUP import full_data, consumption_data, regressions, reg_table, df_to_orgtbl

 food = ['c_cereals', 'c_maize', 'c_sorghum', 'c_millet', 'c_potato', 'c_sweetpotato', 'c_rice', 'c_bread', 'c_beans', 'c_oil', 'c_salt', 'c_sugar', 'c_meat', 'c_livestock', 'c_poultry', 'c_fish', 'c_egg', 'c_nuts', 'c_milk', 'c_vegetables', 'c_fruit', 'c_tea', 'c_spices', 'c_alcohol', 'c_otherfood']
 month = ['c_fuel', 'c_medicine', 'c_airtime', 'c_cosmetics', 'c_soap', 'c_transport', 'c_entertainment', 'c_childcare', 'c_tobacco', 'c_batteries', 'c_church', 'c_othermonth']    
 year = ['c_clothesfootwear', 'c_womensclothes', 'c_childrensclothes', 'c_shoes', 'c_homeimprovement', 'c_utensils', 'c_furniture', 'c_textiles', 'c_ceremonies', 'c_funerals', 'c_charities', 'c_dowry', 'c_other']    
 normalize = {3:food, 30:month, 360:year}

 D = full_data(normalize=normalize,balance=[])

 C, HH, T = consumption_data(D, how="long")
 C = C.join(T, how="left")
 Outcomes = ["Tot", "FoodShr", "Food",  "logTot", "Month", "Year"]

 #$\approx$ Make aggregate variables
 for Year,suffix in ( ("2013","_b"), ("2014","_m"), ("2015","_e") ):
     C["Food"]   = C[[item for item in food  if item in C]].sum(axis=1).replace(0,np.nan)
     C["Month"]  = C[[item for item in month if item in C]].sum(axis=1).replace(0,np.nan)
     C["Year"]   = C[[item for item in year  if item in C]].sum(axis=1).replace(0,np.nan)
     C["Tot"]    = C[["Food","Month","Year"]].sum(axis=1).replace(0,np.nan)
     C["FoodShr"]= C["Food"]/C["Tot"] #$\approx$ FoodShare variable
     C["logTot"] = C["Tot"].apply(np.log)
     try: #~ THIS NEEDS TO BE FIXED TO CALL TUP.py the way neediness.org does...
         lmdas = pd.read_pickle("../data/modified/ss-loglambdas_b{}.df".format(suffix))
         C["$\log\lambda$"+Year] = lmdas["lambda"]
     except IOError:
         if not Year =="2013": print("ss-loglambdas_b{} doesn't exist. Run goods_analysis_2015.py".format(suffix))


 #$\approx$ Make Baseline variable
 for var in Outcomes: 
     Bl = C.loc[2013,var]
     C = C.join(Bl,rsuffix="2013", how="left")


 C["Y"]=np.nan
 for yr in (2013, 2014, 2015): C.loc[yr,"Y"]=str(int(yr))

 C = C.join(pd.get_dummies(C["Y"]), how="left")
 for group in ("TUP", "CSH"):
     for year in ("2013", "2014", "2015"):
         interaction = C[group]*C[year]
         if interaction.sum()>0: C["{}*{}".format(group,year)] = interaction

 Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']
 C = C.loc[2014:2015]
 #$\approx$ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
 regs = regressions(C, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)
 #$\approx$ regs = {var: sm.OLS(C[var], C[Controls], missing='drop').fit() for var in Outcomes}

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = C["TUP"]+C["CSH"] ==0
 CTLmean = {var: C[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: C[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+END_SRC
 
** Assets
   
#+name: asset_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle Endline_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
from matplotlib import pyplot as plt
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

D = full_data(balance=[])

Outcomes = ["Total", "Productive"]
Aval2013 = asset_vars(D,year=2013)[0][Outcomes]
Aval2014 = asset_vars(D,year=2014)[0][Outcomes]
Aval2015 = asset_vars(D,year=2015)[0][Outcomes]

#$\approx$ Creates Year dummies and baseline values as `var'2013
for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
    Aval["Year"]=Year
    for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
Vals = Vals.join(D[["TUP","CSH"]])
Vals["CTL"] = (Vals["TUP"]+Vals["CSH"] ==0).apply(int)

for group in ("TUP", "CSH"):
    for year in ("2013", "2014", "2015"):
        Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

#~ Make graph of distribution
stringify = lambda var: Vals[var].apply(lambda x: var if x else "")
Vals["Group"] = stringify("TUP")+stringify("CSH")+stringify("CTL")


amean = Vals.groupby(["Year","Group"]).mean()[["Total","Productive"]]
aN = Vals.groupby(["Year","Group"]).count()[["Total","Productive"]]
astd = Vals.groupby(["Year","Group"]).std()[["Total","Productive"]]
ase = astd/np.sqrt(aN)
asset_pctchange = (amean/amean.ix[2013]).unstack("Year") - 1

for var in ("Total","Productive"):
   fig,ax = plt.subplots(1,2)
   for i,yr in enumerate((2014,2015)):
       Vals.ix[yr].dropna(subset=[[var,"TUP","CSH","CTL"]]).groupby("Group")[var].plot(kind="kde",ax=ax[i])
       ax[i].set_title("{} Asset Value in {}".format(var,yr))
       ax[i].legend()
       #~ ax[i].set_aspect(1)
       ax[i].set_xlim(left=0)
   plt.savefig("../figures/Asset{}_kde.png".format(var))
   plt.clf()
   amean.unstack("Group")["Total"].plot(kind="bar",yerr=ase.unstack("Group")["Total"].values)
   plt.tight_layout()
   plt.xticks(rotation=45)
   plt.savefig("../figures/Asset{}_groupyear.png".format(var))
   plt.clf()

Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

#$\approx$ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both TUP*2014 and TUP*2015
Vals=Vals.loc[2014:2015]
regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

CTL = Vals["TUP"]+Vals["CSH"] ==0
CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

for var in Outcomes:
    ttest1= regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


results = results.append(diff)
SE = SE.append(diff_se)

tab = df_to_orgtbl(results, sedf=SE)
#+end_src

#+RESULTS: asset_analysis
: None

#+name: assets_disaggreate_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
import numpy as np
import pandas as pd
from TUP import full_data, regressions, asset_vars, reg_table, df_to_orgtbl

D = full_data(balance=[])
D["cons"] = 1.
Count = D.filter(regex="^asset_n_").rename(columns=lambda col: col[8:])
Vals = D.filter(regex="^asset_val_").rename(columns=lambda col: col[10:])
#~ 1 if coded as >0, 0 if zero, keep NaN's missing, since those are only from missing surveys.
Nonzero = Count.applymap(lambda i: float(i>0) if not np.isnan(i) else np.nan)
#~ Restrict both df's to nonzero responses.
Count = Count.replace(0,np.nan)
Vals  =  Vals.replace(0,np.nan)
too_many_zeros = 30
many_zeros = [item for item in Vals if Vals[item].notnull().sum()<too_many_zeros]
for df in (Nonzero, Count, Vals): df.drop(many_zeros,1, inplace=True)

#~ Merge in Control Vars
controls = ["cons","TUP","Cash"]
Nonzero  = Nonzero.join(D[controls],how="left")
Count    =   Count.join(D[controls],how="left")
Vals     =    Vals.join(D[controls],how="left")

Items = [item[:-2] for item in Vals if item.endswith("_e")]
CTL = Vals[controls].sum(axis=1)==1 #~ i.e. only constant ==1, TUP & Cash ==0
Zeros_ctl_mean = Nonzero.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Count_ctl_mean =   Count.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Vals_ctl_mean  =    Vals.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])

ZeroTable       = reg_table(regressions(Nonzero,"_e", outcomes=Items, controls = ["cons",'Cash','TUP'], Baseline="_b"), orgtbl=True, Transpose=True)
Count, Count_se = reg_table(regressions(Count,"_e",   outcomes=Items, controls = ["cons",'Cash','TUP'], Baseline="_b"), resultdf=True, Transpose=True)
Vals, Vals_se   = reg_table(regressions(Vals,"_e",    outcomes=Items, controls = ["cons",'Cash','TUP'], Baseline="_b"), resultdf=True, Transpose=True)

#~ Make full table of Standard errors-- MAKE SURE YOU HAVE THE SUFFIXES RIGHT.
SE = Count_se[["TUP","Cash"]].join(Vals_se[["TUP","Cash"]], rsuffix=" (SSP)", lsuffix=" (# own)")

#~ Make full table of point estimates and control-group means
FullTable = pd.DataFrame({"# own (CTL)":Count_ctl_mean, "Value (CTL)":Vals_ctl_mean}).join(Count[["TUP","Cash"]])
FullTable = FullTable.join(Vals[["TUP","Cash","N"]], rsuffix=" (SSP)", lsuffix=" (# own)")
FullTable = df_to_orgtbl(FullTable, sedf=SE)
AllTables = FullTable+"\n\n"+ZeroTable
return AllTables
#+end_src

** Savings

#+name: savings_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl

def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

D = full_data(balance=[])

Sav = pd.DataFrame(index=D.index) #~ Set up empty DataFrame to fill

years = [("_b",2013), ("_m",2014), ("_e", 2015)]
for suff,year in years: #~ Make Aggregate savings and land holding variables
    Sav["Savings{}".format(year)]  = D.filter(regex="^savings_(home|bank|BRAC|NGOs|other){}".format(suff)).sum(1)
    Sav["Food Sav{}".format(year)] = D.filter(regex="^savings_(maize|sorghum|otherfood)_val{}".format(suff)).sum(1)

    Sav["LandCult{}".format(year)] = D.filter(regex="^land_(owncult|rentcult|communitycult){}".format(suff)).sum(1)
    Sav["LandOwn{}".format(year)]  = D.filter(regex="^land_own.*{}".format(suff)).sum(1)

    Sav["Get Trans{}".format(year)]  = D.filter(regex="^transfers_get.*{}".format(suff)).sum(1)
    Sav["Give Trans{}".format(year)] = D.filter(regex="^transfers_give.*{}".format(suff)).sum(1)

Outcomes = ["Savings","Food Sav","LandCult","LandOwn", "Get Trans", "Give Trans"] #~ Loans give/received omitted

by_year = []
for yr in ("2013","2014","2015"): #~ Provide Baseline Values & put in long format
    S_Year = Sav.filter(like=yr).rename(columns=lambda x:x[:-4])
    for var in Outcomes: 
        if var+"2013" in Sav: S_Year[var+"2013"] = Sav[var+"2013"]
    #~ Note that adding "Year" has to come after topcode, which switches strings to Nan...
    S_Year["Year"] = yr
    by_year.append(S_Year)

#~ In long format with Year,HH index
Sav = pd.concat(by_year).reset_index().set_index(["Year", "HH"], drop=False)
#~ Make Year Dummies for fixed effects


Sav = Sav.join(pd.get_dummies(Sav["Year"]).rename(columns=lambda col: str(int(col))), how="left")
Sav = Sav.drop(["HH","Year"],1)
Sav = Sav.join(D[["TUP","CSH"]])

for group in ("TUP", "CSH"): #~ Make Treatment-by-year interactions
    for year in ("2013", "2014", "2015"):
        Sav["{}*{}".format(group,year)] = Sav[group]*Sav[year]

def isPositive(i):
    #~ Returns 1 if number is positive, 0 if number<=0, nan if already nan, and self if string.
    #~ Note that it's safe to run dummy variables through..
    try:
        if np.isnan(i): return i
        else: return float(i>0)
    except TypeError: return i

too_many_null = 30
Nonzero = Sav.applymap(isPositive)

#~ Save DataFrame with zeros
Savings = Sav.copy()
#~ Naturally, only do this after Creating Nonzero dataframe.
for var in Outcomes: #~ Set zeros to missing And topcode among non-zero values
    for outcome in (var,var+"2013"):
       if outcome in Sav:
           Sav[outcome] = Sav[outcome].replace(0,np.nan)
           Sav[outcome] = Sav.groupby(level="Year")[outcome].apply(topcode) #~ (Untested)

many_null = [item for item in Sav if Sav[item].count()<too_many_null]
many_null2 =[item for item in Savings if Savings[item].count()<too_many_null]
Sav = Sav.drop(many_null,1).copy()
Savings = Savings.drop(many_null,1).copy()

Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

Sav = Sav.loc["2014":"2015"]
Savings = Savings.loc["2014":"2015"]
Nonzero = Nonzero.loc["2014":"2015"]
Sav_regs = regressions(Sav,     outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)
Zer_regs = regressions(Nonzero, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)
Amt_regs = regressions(Savings, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

Sav_results, Sav_SE  = reg_table(Sav_regs,  resultdf=True,table_info=["N","F-stat"])
Zer_results, Zer_SE  = reg_table(Zer_regs,  resultdf=True,table_info=["N","F-stat"])
Amt_results, Amt_SE  = reg_table(Amt_regs,  resultdf=True,table_info=["N","F-stat"])

CTL  = Sav["TUP"]+Sav["CSH"] ==0
CTL2 = Savings["TUP"]+Savings["CSH"] ==0
#~ Get control group means and standard deviations
Sav_CTLmean = {var: Sav[CTL].loc["2015",var].mean() for var in Outcomes}
Zer_CTLmean = {var: Nonzero[CTL].loc["2015",var].mean() for var in Outcomes}
Amt_CTLmean = {var: Savings[CTL2].loc["2015",var].mean() for var in Outcomes}

Sav_CTLsd = {var: Sav[CTL].loc["2015",var].std() for var in Outcomes}
Zer_CTLsd = {var: Nonzero[CTL].loc["2015",var].std() for var in Outcomes}
Amt_CTLsd = {var: Savings[CTL2].loc["2015",var].std() for var in Outcomes}

Sav_diff, Sav_diff_se = pd.DataFrame(Sav_CTLmean,index=["CTL mean"]), pd.DataFrame(Sav_CTLsd,index=["CTL mean"])
Zer_diff, Zer_diff_se = pd.DataFrame(Zer_CTLmean,index=["CTL mean"]), pd.DataFrame(Zer_CTLsd,index=["CTL mean"])
Amt_diff, Amt_diff_se = pd.DataFrame(Amt_CTLmean,index=["CTL mean"]), pd.DataFrame(Amt_CTLsd,index=["CTL mean"])

for var in Outcomes:
    #~ Savings regressions first
    ttest1= Sav_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Sav_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Sav_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Sav_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Sav_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Sav_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

    #~ Nonzero regressions second
    ttest1= Zer_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Zer_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Zer_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Zer_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Zer_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Zer_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

    #~ Savings regressions third
    ttest1= Amt_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Amt_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Amt_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Amt_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Amt_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Amt_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

Save_results = Sav_results.append(Sav_diff)
Zero_results = Zer_results.append(Zer_diff)
Amnt_results = Amt_results.append(Amt_diff)
Save_SE = Sav_SE.append(Sav_diff_se)
Zero_SE = Zer_SE.append(Zer_diff_se)
Amnt_SE = Amt_SE.append(Amt_diff_se)

#~ Land = ["LandCult","LandOwn"] 
#~ Savings = ["Savings","Food Sav", "Get Trans", "Give Trans"] 
#~ Land_results = Sav_results[Land]
#~ zLan_results = Zer_results[Land]
#~ Land_SE = Sav_SE[Land]
#~ zLan_SE = Zer_SE[Land]
#~ 
#~ Sav_results = Sav_results[Savings]
#~ Zer_results = Zer_results[Savings]
#~ Sav_SE =           Sav_SE[Savings]
#~ Zer_SE =           Zer_SE[Savings]

Save_tab = df_to_orgtbl(Save_results, sedf=Sav_SE)
Zero_tab = df_to_orgtbl(Zero_results, sedf=Zer_SE)
Amnt_tab = df_to_orgtbl(Amnt_results, sedf=Amt_SE)

Table = Zero_tab +"\n"+ Save_tab + "\n"+ Amnt_tab

#+end_src

** Income

#+name: income_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
from pandas.io import stata
import statsmodels.api as sm
from matplotlib import pyplot as plt
from TUP import full_data, regressions, reg_table, df_to_orgtbl
"""
Note that topcoding has a large effect on the distribution here, and we see only a small (presumably non-random) portion of actual income for each household.
"""

# Top-Code or censor outliers?
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

D = full_data(balance=[])
keep = D.index

I_file = '../../data/Endline/sections_8_17.csv'
I = pd.read_csv(I_file).rename(columns={"id":"HH"}).set_index("HH", drop=True).ix[keep]

#~Getting non-agriculture income data is easy
I = I.filter(regex="^s16")
Imonths    = I.filter(regex="s16_\dc").rename(columns=lambda x: x[:-1])
Ipermonth  = I.filter(regex="s16_\dd").rename(columns=lambda x: x[:-1])
Income_12m = Imonths.mul(Ipermonth).sum(axis=1)
Iyear      = I.filter(regex="s16_\de").rename(columns=lambda x: x[:-1]).sum(axis=1)

A_file = "../../data/Endline/Agriculture_cleaned.csv"
A = pd.read_csv(A_file).rename(columns={"id":"HH"}).set_index("HH",drop=False).ix[keep]
unit_prices = A.groupby(["harvest_type", "harvest_price_unit"])["harvest_price"].median()
prices = unit_prices.loc[zip(A["harvest_type"],A["harvest_price_unit"])]
A["price"]=list(prices)

A["harvest_unit_match"] = A["harvest_price_unit"] == A["harvest_unit"]
A["price"] = A["harvest_unit_match"]*A["harvest_price"] + (1-A["harvest_unit_match"])*A["price"]

A["income_farm_year"] = A["harvest_size"]*A["price"]
Ayear = A.groupby("HH")["income_farm_year"].sum()

unit_prices = A.groupby(["livestock_type", "livestock_price_unit"])["livestock_price"].median()
prices = unit_prices.loc[zip(A["livestock_type"],A["livestock_price_unit"])]
A["price"]=list(prices)
A["livestock_unit_match"] = A["livestock_price_unit"] == A["livestock_unit"]
A["price"] = A["livestock_unit_match"]*A["livestock_price"] + (1-A["livestock_unit_match"])*A["price"]

A["income_livestock_year"] = A["livestock_size"]*A["price"]
Lyear = A.groupby("HH")["income_livestock_year"].sum()

Outcomes = ["Total", "Non-Farm", "Farm",  "Livestock"]
Controls = ["cons", "TUP","CSH"]
Vals = pd.DataFrame({"Non-Farm": Income_12m, "Farm":Ayear, "Livestock":Lyear})
Vals = Vals.apply(topcode)

Vals["Total"] = Vals.sum(axis=1)
Vals["cons"] = 1.

Vals = Vals.join(D[["TUP","CSH"]],how="left")
Vals["CTL"] = (Vals["TUP"]+Vals["CSH"] ==0).apply(int)

#~ Make graph of distribution
stringify = lambda var: Vals[var].apply(lambda x: var if x else "")
Vals["Group"] = stringify("TUP")+stringify("CSH")+stringify("CTL")
Vals.dropna(subset=[["Total","TUP","CSH","CTL"]]).groupby("Group")["Total"].plot(kind="kde")
plt.title("Total Income Distribution by Group")
plt.savefig("../figures/IncomeDistribution.png")
plt.clf()
#~ Make bar graphs
Imean  = Vals.groupby("Group").mean()[Outcomes]
Icount = Vals.groupby("Group").count()[Outcomes]
Istd = Vals.groupby("Group").std()[Outcomes]
Ise=Istd/np.sqrt(Icount)
Imean.T.plot(kind="bar",yerr=Ise.T)
plt.tight_layout()
plt.xticks(rotation=45)
plt.savefig('../figures/Income_group.png')
plt.clf()

regs = {var: sm.OLS(Vals[var], Vals[Controls], missing="drop").fit() for var in Outcomes}
results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

CTL = Vals["CTL"] 
CTLmean = {var: Vals.query("CTL==1")[var].mean() for var in Outcomes}
CTLsd = {var: Vals.query("CTL==1").std() for var in Outcomes}
diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

for var in Outcomes:
    ttest1= regs[var].t_test("TUP - CSH = 0").summary_frame()

    diff.loc[   r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest1["coef"][0]
    diff_se.loc[r"$\beta^{TUP}-\beta^{CSH}$", var] = ttest1["std err"][0]

results = results.append(diff)
SE = SE.append(diff_se)

tab = df_to_orgtbl(results, sedf=SE)

#+end_src

#+RESULTS: income_analysis
: None


** Conflict Exposure
#+name: conflict_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl

# Top-Code or censor outliers?
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

#~ Read in data
D = full_data("../../data/TUP_full.dta", balance=[])
D = D[D.merge_midline != 1]
C = D.filter(like="conflict").rename(columns = lambda x: x[:-2]) #~ Set up empty DataFrame to fill

#~ Make Outcome variables
#~ NOTE: Not looking at whether they protected assets as only 50 #~ said they did, and all said "Migrated to stay with other family"
C["Bsln_NaN"] = D["merge_midline"] == 2
C["Worried"]  = C["conflict_worried"]
C["Affected"] = C["conflict_affected"]
protect_lives_codes = lambda x: {"Nothing": 0, "Migrate to stay with friend/family": 1, "Migrated and found new accommodation": 1,
                                 "Looked for protection with Govt. Military": 2, "Looked for protection with NGO": 2}.get(x)
C["ProtectLives"] = C["conflict_protectlives"].apply(protect_lives_codes)
C["Migrated"] = (C["ProtectLives"]==1) + \
                (C["conflict_affected1"]=="Needed to elocate or migrate")
C["NoMeans"] = C["conflict_whynotprotect"]=="Didn't have the means"
C["NoInvest"]= C.filter(like="affected").applymap(lambda x: x=="Could not plant crop or invest in business").sum(axis=1)
C = C.drop([var for var in C if var.startswith("conflict_")], 1)
Outcomes = ["ProtectLives", "Worried", "Affected", "Migrated", "NoMeans", "NoInvest"]

#~ Bring in Treatment variables
C["TUP"] = D["TUP"]
C["cons"] = 1.
C = C.applymap(lambda x: int(x) if not np.isnan(x) else x)
Controls = ["cons", "TUP"] #~, "Bsln_NaN"]

#~ Plot
byT=C.groupby("TUP")
Cbar=byT.mean().drop(["Bsln_NaN",'cons'],1)
Cstd=byT.std().drop(["Bsln_NaN",'cons'],1)
Cn=byT.count().drop(["Bsln_NaN",'cons'],1)
for df in (Cbar,Cstd,Cn): 
        df.index=["CTL","TUP"]
        df.index.name="Group"
Cse=Cstd/np.sqrt(Cn)
Cbar.T.plot(kind="bar",yerr=Cse.T)
plt.tight_layout()
plt.xticks(rotation=45)
plt.savefig("../figures/conflict_exposure.png")
plt.clf()


C_regs = regressions(C, outcomes=Outcomes, controls=Controls, Baseline=False, baseline_na=False)
C_results, C_SE  = reg_table(C_regs,  resultdf=True,table_info=["N","F-stat"])

#~ Get control group means and standard deviations and add to regression table
CTLmean = {var: C.query("TUP==0")[var].mean() for var in Outcomes}
CTLsdv  = {var: C.query("TUP==0")[var].std()  for var in Outcomes}
CTLmean, CTLsdv = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsdv,index=["CTL mean"])
C_results = C_results.append(CTLmean)
C_SE      = C_SE.append(CTLsdv)
C_results.drop('cons',inplace=True)
C_SE.drop('cons',inplace=True)

Table = df_to_orgtbl(C_results, sedf=C_SE)
return Table

#+end_src


** Confidence & Autonomy

#+name: autonomy_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
from TUP import full_data, regressions, asset_vars, reg_table , df_to_orgtbl

D = full_data(balance=[])
E = D.filter(regex="^(access|decide|conf|gone)")
years = {"_b":2013, "_m":2014, "_e": 2015}
E = E.rename(columns = lambda x: x[:-2]+years[x[-2:]])

print("THIS IS UNFINISHED...")

by_year = []
for yr in ("2013","2014","2015"): #~ Provide Baseline Values & put in long format
    E_Year = E.filter(like=yr).rename(columns=lambda x:x[:-4])
    for var in Outcomes: 
        if var+"2013" in Sav: S_Year[var+"2013"] = Sav[var+"2013"]
    #~ Note that adding "Year" has to come after topcode, which switches strings to Nan...
    S_Year["Year"] = yr
    by_year.append(S_Year)

Sav = pd.DataFrame(index=D.index) #~ Set up empty DataFrame to fill

for suff,year in years: #~ Make Aggregate savings and land holding variables
    E["Savings{}".format(year)]  = D.filter(regex="^savings_(home|bank|BRAC|NGOs|other){}".format(suff)).sum(1)
    Sav["Food Sav{}".format(year)] = D.filter(regex="^savings_(maize|sorghum|otherfood)_val{}".format(suff)).sum(1)

    Sav["LandCult{}".format(year)] = D.filter(regex="^land_(owncult|rentcult|communitycult){}".format(suff)).sum(1)
    Sav["LandOwn{}".format(year)]  = D.filter(regex="^land_own.*{}".format(suff)).sum(1)

    Sav["Get Trans{}".format(year)]  = D.filter(regex="^transfers_get.*{}".format(suff)).sum(1)
    Sav["Give Trans{}".format(year)] = D.filter(regex="^transfers_give.*{}".format(suff)).sum(1)



#~ In long format with Year,HH index
Sav = pd.concat(by_year).reset_index().set_index(["Year", "HH"], drop=False)
#~ Make Year Dummies for fixed effects


Sav = Sav.join(pd.get_dummies(Sav["Year"]).rename(columns=lambda col: str(int(col))), how="left")
Sav = Sav.drop(["HH","Year"],1)
Sav = Sav.join(D[["TUP","CSH"]])

for group in ("TUP", "CSH"): #~ Make Treatment-by-year interactions
    for year in ("2013", "2014", "2015"):
        Sav["{}*{}".format(group,year)] = Sav[group]*Sav[year]

def isPositive(i):
    #~ Returns 1 if number is positive, 0 if number<=0, nan if already nan, and self if string.
    #~ Note that it's safe to run dummy variables through.
    try:
        if np.isnan(i): return i
        else: return float(i>0)
    except TypeError: return i

too_many_null = 30
Nonzero = Sav.applymap(isPositive)

#~ Naturally, only do this after Creating Nonzero dataframe.
for var in Outcomes: #~ Set zeros to missing And topcode among non-zero values
    for outcome in (var,var+"2013"):
       if outcome in Sav:
           Sav[outcome] = Sav[outcome].replace(0,np.nan)
           Sav[outcome] = Sav.groupby(level="Year")[outcome].apply(topcode) #~ (Untested)

many_null = [item for item in Sav if Sav[item].notnull().sum()<too_many_null]
Sav = Sav.drop(many_null,1).copy()

Controls = ['2014', '2015', 'TUP*2014', 'TUP*2015', 'CSH*2014', 'CSH*2015']

Sav = Sav.loc["2014":"2015"]
Nonzero = Nonzero.loc["2014":"2015"]
Sav_regs = regressions(Sav,     outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)
Zer_regs = regressions(Nonzero, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

Sav_results, Sav_SE  = reg_table(Sav_regs,  resultdf=True,table_info=["N","F-stat"])
Zer_results, Zer_SE  = reg_table(Zer_regs,  resultdf=True,table_info=["N","F-stat"])

CTL = Sav["TUP"]+Sav["CSH"] ==0
#~ Get control group means and standard deviations
Sav_CTLmean = {var: Sav[CTL].loc["2015",var].mean() for var in Outcomes}
Zer_CTLmean = {var: Nonzero[CTL].loc["2015",var].mean() for var in Outcomes}
Sav_CTLsd = {var: Sav[CTL].loc["2015",var].std() for var in Outcomes}
Zer_CTLsd = {var: Nonzero[CTL].loc["2015",var].std() for var in Outcomes}
Sav_diff, Sav_diff_se = pd.DataFrame(Sav_CTLmean,index=["CTL mean"]), pd.DataFrame(Sav_CTLsd,index=["CTL mean"])
Zer_diff, Zer_diff_se = pd.DataFrame(Zer_CTLmean,index=["CTL mean"]), pd.DataFrame(Zer_CTLsd,index=["CTL mean"])

for var in Outcomes:
    #~ Savings regressions first
    ttest1= Sav_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Sav_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Sav_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Sav_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Sav_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Sav_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

    #~ Nonzero regressions second
    ttest1= Zer_regs[var].t_test("TUP*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Zer_regs[var].t_test("TUP*2015 - CSH*2015 = 0").summary_frame()

    Zer_diff.loc[   r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Zer_diff_se.loc[r"$\beta^{TUP}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Zer_diff.loc[   r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Zer_diff_se.loc[r"$\beta^{TUP}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


Land = ["LandCult","LandOwn"] 
Savings = ["Savings","Food Sav", "Get Trans", "Give Trans"] 

Save_results = Sav_results.append(Sav_diff)
Zero_results = Zer_results.append(Zer_diff)
Save_SE = Sav_SE.append(Sav_diff_se)
Zero_SE = Zer_SE.append(Zer_diff_se)

#~ Land_results = Sav_results[Land]
#~ zLan_results = Zer_results[Land]
#~ Land_SE = Sav_SE[Land]
#~ zLan_SE = Zer_SE[Land]
#~ 
#~ Sav_results = Sav_results[Savings]
#~ Zer_results = Zer_results[Savings]
#~ Sav_SE =           Sav_SE[Savings]
#~ Zer_SE =           Zer_SE[Savings]

Save_tab = df_to_orgtbl(Save_results, sedf=Sav_SE)
Zero_tab = df_to_orgtbl(Zero_results, sedf=Zer_SE)

Table = Zero_tab +"\n"+ Save_tab
#+end_src





** Extras
#+name: get_loglambdas
#+BEGIN_SRC python :noweb no-export :results silent

"""
This code does not yet work in this file. Come back to it...
"""
  df = TUP.process_data(C, HH, T, year = year) #~ Process_data() builds consumption data if not given as an argument
  df['Constant']=1
  df["CTL"] = 1-df["TUP"] #~ Code the cash group as controls since they're not in the midline analysis

  explist=[s[2:-2] for s in df.columns[[s.startswith('c_') and s.endswith(year[1]) for s in df.columns]]]
  df = df.rename(columns= lambda x: x[:-2] if x.endswith(year[1]) else x)

  bothdf=[]
  xvars=['hh_size_b','child_total_b','Loc']
  for x in explist:
      if 'c_'+x+r'_b' not in df:
          #~ When you take out the baseline controls in favor of repeated cross-sections, this is where to start...
          print(x+" has no baseline data or had too few non-zero responses at baseline. Skipping.")
          continue
      ydf=pd.DataFrame(df[['c_'+x]].rename(columns={'c_'+x:x.capitalize()}).stack())
      rdict=dict(zip(xvars+['c_'+x+r'_b'],["%s_%s" % (s,x.capitalize()) for s in xvars]+['Baseline_%s' % x.capitalize()]))
      xdf=pd.DataFrame(df[xvars+['c_'+x+r'_b']])
      xdf.index=pd.MultiIndex.from_tuples([(i,x.capitalize()) for i in xdf.index])
      locations=pd.get_dummies(xdf['Loc'],prefix='Loc_%s' % x.capitalize())
      del xdf['Loc']
      xdf.rename(columns=rdict,inplace=True)
      xdf=xdf.join(locations)
      xdf.replace(to_replace=np.NaN,value=0,inplace=True)

      # Add row to restrict location dummies to sum to one
      ydf=pd.concat([ydf,pd.DataFrame([0],index=[(0,x.capitalize())])])
      xdf=pd.concat([xdf,pd.DataFrame([s.startswith('Loc_')+0. for s in xdf.columns],index=xdf.columns,columns=[(0,x.capitalize())]).T]) 

      xdf[0]=ydf
      xdf.dropna(how='any',inplace=True)
      bothdf.append(xdf)

  #~ Are this fillna() call and the xdf.replace call above a problem? It seems necessary for the block-diagonal ols function
  #~ we're using, but aren't we coding zeros as missing and calculating residuals for only those positive consumption? Wouldn't
  #~ replacing them to zero insert some non-zero residual for households that never consume a given good?
  #~ And isn't this the motivation behind svd_missing?
  mydf=pd.concat(bothdf).fillna(value=0)

  X=mydf.iloc[:,1:]

  y=mydf[[0]]

  x=np.exp(y.unstack().iloc[1:,:]) # Expenditures (in levels)
  xshares=x.divide(x.sum(axis=1),axis=0).fillna(value=0).mean() # Expenditure shares (taking missing as zero)
  xshares.index=xshares.index.droplevel(0)

  b,se=ols(X,y)

  ## betahat=b[['Constant_%s' % s.capitalize() for s in explist]]
  ## betahat.rename(columns=dict(zip(betahat.columns,[s.capitalize() for s in explist])),inplace=True)

  e=y-X.dot(b.T)

  e.rename(columns={0:'Resid'},inplace=True)
  e.index.names=['HH','Good']

  testdf=pd.merge(df[['TUP','CTL']].reset_index(),e.reset_index(),how='outer',on=['HH'])
  testdf.set_index(['HH','Good'],inplace=True)

  TUP=testdf['TUP'].mul(testdf['Resid']).dropna().unstack()
  CTL=testdf['CTL'].mul(testdf['Resid']).dropna().unstack()

  e=(e-e.mean()).unstack()

  # Test of significant differences between treatment and control:
  # Weighting matrix:
  A=np.matrix((TUP-CTL).cov().as_matrix()).I
  g=np.matrix((TUP-CTL).mean())
  J=e.shape[0]*g*A*g.T # Chi2 statistic

  p=1-chi2.cdf(J,e.shape[1])

  chi2test="Chi2 test: %f (%f)" % (J,p)

  N=pd.Series([d.shape[0]-1 for d in bothdf],index=[d.index.levels[1][0] for d in bothdf])

  resultdf=pd.DataFrame({'TUP':TUP.mean(),'CTL':CTL.mean(),'$N$':N})
  sedf=pd.DataFrame({'TUP':TUP.std()/np.sqrt(resultdf['$N$']),'CTL':CTL.std()/np.sqrt(resultdf['$N$'])})
  resultdf['Diff.']=resultdf['TUP']-resultdf['CTL']
  sedf['Diff.']=np.sqrt((sedf['TUP']**2) + (sedf['CTL']**2))

  # Use svd (with missing data) to construct beta & log lambda

  myb,myl = get_loglambdas(e,TEST=True)

  myb.index=myb.index.droplevel(0)

  # Normalize log lambdas
  l=myl/myl.std()
#+END_SRC

#+name: residuals_by_group
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
def residuals_by_group(models, groups, outcomes=[], kind="kde", figure_dir="../figures/", seriesname="Treat", blanks_to = "Control"):
    """
     Takes a set of statsmodels regression results and,
     for each outcome, produces a plot comparing the
     distribution of residuals by group.

     models:
         A dictionary of the form {variable name: sm.RegressionResults}. Empty defaults to all available.
     groups:
         A list, pd.Series, or pd.DataFrame with variables
         (A later version could contain an arbitrary set of categorical and give groups for every combination.)
     outcomes:
         A list specifying which variables in models to make plots for.
     kind:
         kde (or density) or histogram (or hist)
         Density plots are on a single axis. Histograms are stacked by group.
     figure_dir:
         The directory into which the figures get saved. If doesn't exist, throws error (future version might make that directory on the fly.)
     Seriesname:
         If a series or list is passed without a name, defaults to `seriesname'
     blanks_to:
         Observations with no treatment status from "groups" gets renamed to `blanks_to'
    """

    #~ Make outcomes a list. If empty, defaults to all variables in models
    if type(outcomes)==str: outcomes=[outcomes]
    if not outcomes: outcomes = sorted(models.keys())

    #~ Make data frame and make "Group" categorical
    df = pd.DataFrame(groups).rename(columns={0:seriesname})
    for var in df: df[var] = df[var].applymap(lambda x: var if x else "")
    df["Group"] = df.sum(axis=1).replace("",blanks_to)

    #~ Make residuals
    for var in outcomes:
        #~ Make column of residual values
        resid_var = "resid_{}".format(var)
        df[resid_var] = models[var].resid
        #~ Groupby object
        groups = df.dropna(subset=[resid_var]).groupby("Group")[resid_var]

        #~ Plot density by group
        if kind in ("kde", "density"):
            fig, ax = plt.subplots()
            groups.plot(kind=kind, ax=ax, legend=True)
            fig.savefig(figure_dir+resid_var+".png")

        #~ Plot histograms by group
        elif kind in ("hist", "histogram"):
            i=0
            fig, ax = plt.subplots(len(set(df["Group"])),1,sharex=True)
            for group, data in grps[var]:
                ax[i].hist(data.values, bins=20)
                ax[i].set_title(group)
                i+=1
            i=0
            fig.savefig(figure_dir+resid_var+".png")
        print(resid_var+".png created.")


#+end_src
   
