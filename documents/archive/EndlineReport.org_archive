#    -*- mode: org -*-


Archived entries from file /home/ligon/Research/SS-TUP/TUP-report/documents/EndlineReport.org


* Output from Elliott's code (older version)                :ignore:noexport:
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-02-23 Tue 10:46
  :ARCHIVE_FILE: ~/Research/SS-TUP/TUP-report/documents/EndlineReport.org
  :ARCHIVE_OLPATH: Data Collection & Timeline/Balance on Obervables
  :ARCHIVE_CATEGORY: EndlineReport
  :END:
#+name: balance_check
#+BEGIN_SRC python :dir ../analysis :results value table :exports none
import check_balance
return check_balance.tables
#+END_SRC


#+name: tab:balance_check
#+caption: Means of some analysis variables at baseline.  Asterisks indicate p<.1, .05, and .01 respectively
#+attr_latex: :environment longtable :align lrrrrr
|-----------------+---------+--------------+--------------+-----|
| Consumption     |     CTL | $\Delta$ TUP | $\Delta$ CSH | $N$ |
|-----------------+---------+--------------+--------------+-----|
| Meat            |    4.21 |       -0.568 |       -0.052 | 378 |
| Fuel            |    0.76 |       -0.039 |       -0.072 | 456 |
| Clothesfootwear |    0.67 |       -0.026 |        0.033 | 595 |
| Soap            |    0.48 |       -0.008 |       -0.026 | 536 |
| Fish            |    2.50 |       -0.154 |       -0.156 | 474 |
| Charities       |    0.03 |       -0.006 |          0.0 | 134 |
| Cereals         |    9.19 |       -0.947 |         0.27 | 605 |
| Transport       |    0.18 |       -0.033 |        0.002 | 193 |
| Cosmetics       |    0.68 |        0.027 |       -0.125 | 468 |
| Sugar           |    1.71 |       -0.078 |       -0.189 | 604 |
| Egg             |    1.10 |       -0.091 |        0.038 | 276 |
| Oil             |    1.36 |        -0.13 |       -0.141 | 613 |
| Ceremonies      |    0.13 |        0.006 |        0.026 | 152 |
| Beans           |    0.70 |        0.232 |        0.226 | 192 |
| Fruit           |    0.69 |       -0.089 |        0.001 | 272 |
| Textiles        |    0.16 |       -0.004 |  $0.056^{*}$ | 376 |
| Utensils        |    0.25 |       -0.009 |        0.008 | 442 |
| Dowry           |    1.27 |       -0.041 |        0.028 | 126 |
| Furniture       |    0.20 |       -0.014 |        0.045 | 368 |
| Salt            |    0.45 |       -0.026 |        0.007 | 617 |
| Vegetables      |    1.54 |       -0.165 |        -0.18 | 471 |
|-----------------+---------+--------------+--------------+-----|
| Assets          |     CTL | $\Delta$ TUP | $\Delta$ CSH | $N$ |
|-----------------+---------+--------------+--------------+-----|
| Smallanimals    |  236.60 |      -86.068 |     -123.133 | 123 |
| Bicycle         |  109.08 |      -12.555 |      -11.414 | 171 |
| Radio           |   58.45 |       -5.968 |      -16.529 | 260 |
| Motorcycle      |  341.74 |      192.956 | 353.836^{**} |  93 |
| Net             |   19.16 |        0.668 |        0.247 | 423 |
| Poultry         |   42.40 |       -3.365 |       -8.894 | 161 |
| Bed             |  241.27 |        7.992 |       32.762 | 521 |
| Chairtables     |  206.79 |      -29.368 |        3.617 | 531 |
| Mobile          |   97.54 |       12.627 |       -4.198 | 414 |
| Netitn          |    7.82 |        1.215 |        1.178 | 181 |
| Cosmetics       |    0.68 |        0.027 |       -0.125 | 468 |
|-----------------+---------+--------------+--------------+-----|
| Household       |     CTL | $\Delta$ TUP | $\Delta$ CSH | $N$ |
|-----------------+---------+--------------+--------------+-----|
| Daily Food      |   25.18 |       -2.215 |       -0.261 | 643 |
| Daily Exp       |   29.90 |       -2.167 |       -0.288 | 646 |
| No. Houses      |    2.83 |        0.031 |        0.118 | 543 |
| In Business     |    0.40 |        0.038 |        0.017 | 265 |
| Cereals         |    9.19 |       -0.947 |         0.27 | 605 |
| # Child         |    3.26 |        0.118 |        0.108 | 594 |
| Asset Tot.      | 1757.05 |      -44.791 |       98.654 | 603 |
| Cash Savings    |  236.90 |        28.52 |      -66.812 | 431 |
| HH size         |    7.23 |       -0.175 |          0.3 | 648 |
|-----------------+---------+--------------+--------------+-----|

This is simply suggestive evidence that the treatment and control groups were similar
in observables at baseline, with the exception that the cash group has atypically
more motorcycles and clothing. But it does suggests that our stratified randomization
was not too far from creating comparable group

* Output from Ethan's code (newer version)                  :ignore:noexport:
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-02-23 Tue 10:56
  :ARCHIVE_FILE: ~/Research/SS-TUP/TUP-report/documents/EndlineReport.org
  :ARCHIVE_OLPATH: Data Collection & Timeline/Balance on Obervables
  :ARCHIVE_CATEGORY: EndlineReport
  :END:
#+name: balance_check_2
#+BEGIN_SRC python :dir ../analysis :results output raw
import check_balance
#+END_SRC

#+RESULTS: balance_check_2
| Consumption               |   CTL |       TUP |        CSH |
|---------------------------+-------+-----------+------------|
| Cereals                   |   9.0 |       8.2 |        9.5 |
| Beans                     |   0.7 |       0.9 |        0.9 |
| Oil                       |   1.3 |       1.2 |        1.2 |
| Salt                      |   0.4 |       0.4 |        0.5 |
| Sugar                     |   1.7 |       1.6 |        1.5 |
| Meat                      |   4.1 |       3.6 |        4.2 |
| Fish                      |   2.4 |       2.4 |        2.3 |
| Egg                       |   1.0 |       1.0 |        1.1 |
| Milk                      |   1.0 |       1.0 |        1.1 |
| Vegetables                |   1.5 |       1.4 |        1.4 |
| Fruit                     |   0.6 |       0.6 |        0.7 |
| Spices                    |   0.2 |       0.3 |        0.2 |
| Alcohol                   |   0.0 |       0.0 |        0.0 |
| Other foods               |   0.0 |       0.0 |        0.0 |
| Fuel                      |   0.8 |       0.7 |        0.7 |
| Cosmetics                 |   0.7 |       0.7 |        0.6 |
| Soap                      |   0.5 |       0.5 |        0.5 |
| Transport                 |   0.2 |       0.1 |        0.2 |
| Entertainment             |   0.1 |       0.1 |        0.1 |
| Clothes & footwear        |   0.7 |       0.6 |        0.7 |
| Utensils                  |   0.2 |       0.2 |        0.3 |
| Furniture                 |   0.2 |       0.2 |        0.2 |
| Textiles                  |   0.1 |       0.1 |    0.2^{*} |
| Ceremonies                |   0.1 |       0.1 |        0.2 |
| Charities                 |   0.0 |       0.0 |        0.0 |
| Dowry                     |   1.3 |       1.2 |        1.3 |
| Other                     |   0.0 |       0.0 |        0.0 |
|---------------------------+-------+-----------+------------|
| Asset                     |   CTL |       TUP |        CSH |
|---------------------------+-------+-----------+------------|
| Cows                      | 261.8 |     112.7 |      153.6 |
| Small animals             | 252.1 |     150.5 | 113.5^{**} |
| Poultry                   |  43.8 |      39.0 |       33.5 |
| Plough                    |   0.0 |       0.0 |        0.0 |
| Shed                      |   1.8 |  0.0^{**} |        1.6 |
| Shop                      |  95.5 |      79.4 |       69.8 |
| Radio                     |  57.6 |      52.5 |       41.9 |
| Tv                        |  29.0 |      45.9 |       36.1 |
| Fan                       |   1.6 |       1.8 |    6.0^{*} |
| Mobile                    |  92.1 | 110.2^{*} |       93.3 |
| Chairs & Tables           | 205.6 |     177.4 |      210.4 |
| Bed                       | 232.3 |     249.3 |      274.0 |
| Bicycle                   | 112.4 |      96.5 |       97.7 |
| Carts                     |   2.6 |       3.5 |        1.6 |
| Sewing                    |  12.5 |       5.0 |   1.2^{**} |
| Net                       |  19.3 |      19.8 |       19.4 |
| Motorcycle                | 342.7 |     534.7 | 695.6^{**} |
|---------------------------+-------+-----------+------------|
| Household characteristics |   CTL |       TUP |        CSH |
|---------------------------+-------+-----------+------------|
| HH size                   |   7.3 |       7.1 |        7.5 |
| # Children                |   3.3 |       3.4 |        3.4 |
| # Houses                  |   2.8 |       2.9 |        2.9 |
| In Business               |   0.4 |       0.4 |        0.4 |
|---------------------------+-------+-----------+------------|
| $N$                       |   125 |       262 |        249 |




* Box and whisker plots for Elliott's balance table                :noexport:
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-02-23 Tue 10:57
  :ARCHIVE_FILE: ~/Research/SS-TUP/TUP-report/documents/EndlineReport.org
  :ARCHIVE_OLPATH: Data Collection & Timeline/Balance on Obervables
  :ARCHIVE_CATEGORY: EndlineReport
  :END:

#+Caption: Box and whisker plot of textiles means
#+NAME: fig:box_whisker_textiles_Elliott
[[../documents/textiles_Elliott.png]]

#+Caption: Box and whisker plot of motorcycles means
#+NAME: fig:box_whisker_motorcycles_Elliott
[[../documents/motorcycle_Elliott.png]]



* Original attrition tables                                 :ignore:noexport:
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-02-23 Tue 11:16
  :ARCHIVE_FILE: ~/Research/SS-TUP/TUP-report/documents/EndlineReport.org
  :ARCHIVE_OLPATH: Data Collection & Timeline/Sample Selection
  :ARCHIVE_CATEGORY: EndlineReport
  :END:
#+name: attrition_check
#+BEGIN_SRC python :dir ../analysis :results value table :exports none
import check_attrition
return check_attrition.TAB
#+END_SRC

#+caption: Total number of households in sample by group and round.  Numbers in parentheses indicate the size of of the subset also interviewed at baseline.
#+attr_latex: :environment longtable :align lrrrrr
#+name: tab:attrition_count
|-------------+------+-----------+-----------|
| Full Sample | 2013 | 2014      | 2015      |
|-------------+------+-----------+-----------|
| Control     |  281 | 265 (219) | 347 (262) |
| UCT         |  124 | 113 (112) | 111 (110) |
| TUP         |  244 | 228 (223) | 236 (231) |
|-------------+------+-----------+-----------|
| All         |  649 | 606 (554) | 694 (603) |
|-------------+------+-----------+-----------|



* Surveyed households also in baseline                      :ignore:noexport:
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-02-23 Tue 11:16
  :ARCHIVE_FILE: ~/Research/SS-TUP/TUP-report/documents/EndlineReport.org
  :ARCHIVE_OLPATH: Data Collection & Timeline/Sample Selection
  :ARCHIVE_CATEGORY: EndlineReport
  :END:
#+caption: Number of households in sample with baseline survey by group and round
|-----------------+---------+---------+---------|
| Balanced Sample |    2013 |    2014 |    2015 |
|-----------------+---------+---------+---------|
| UCT            | 124.000 | 112.000 | 110.000 |
| Control         | 281.000 | 219.000 | 262.000 |
| TUP             | 244.000 | 223.000 | 231.000 |
|-----------------+---------+---------+---------|
| All             | 649.000 | 554.000 | 603.000 |
|-----------------+---------+---------+---------|

*************** Further checks on assignment and samples
#+begin_src python :results output raw table
import pandas as pd
from cfe.df_utils import df_to_orgtbl
import sys
sys.path.append('../analysis')
from check_attrition import D

R=pd.read_stata('../../TUP-data/random_assignment.dta')
R=R.set_index('respid')      
R.index.name = 'idno'

A=pd.DataFrame({'D':D['group'],'R':R['group'],'B':D['Base'],'M':D['Mid'],'E':D['End']})

# Fix issue with variant capitalizations
A.D=A.D.str.title()
A.R=A.R.str.title()

# Examine observations where assignments differ:
print(df_to_orgtbl(A[A['D']!=A['R']]))
#+end_src

#+results:
|        | D       | R          | B         | M         | E         |
|--------+---------+------------+-----------+-----------+-----------|
| 1042.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1106.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1154.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1157.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1186.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1190.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1207.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1217.0 | Control | Cash       | \(1.000\) | \(1.000\) | \(1.000\) |
| 1222.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1248.0 | ---     | High Asset | ---       | ---       | ---       |
| 1252.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 1264.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1285.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1293.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 1295.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1300.0 | Control | Cash       | \(0.000\) | \(0.000\) | \(1.000\) |
| 1305.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1311.0 | Control | ---        | \(0.000\) | \(1.000\) | \(0.000\) |
| 1314.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1315.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1316.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1320.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1338.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1348.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1350.0 | Control | ---        | \(0.000\) | \(1.000\) | \(0.000\) |
| 1352.0 | Control | High Asset | \(1.000\) | \(1.000\) | \(1.000\) |
| 1368.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1403.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 1421.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1436.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1453.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1476.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1480.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1486.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1496.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1502.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1505.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1507.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1548.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1561.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1591.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1619.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1642.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 1667.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1679.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1690.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1702.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1703.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1730.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1731.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1744.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1747.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1748.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1749.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1750.0 | Control | ---        | \(1.000\) | \(0.000\) | \(0.000\) |
| 1755.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1773.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1780.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1792.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1797.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1802.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1813.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1814.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1818.0 | Control | Low Asset  | \(1.000\) | \(1.000\) | \(1.000\) |
| 1826.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1878.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 1892.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1894.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1910.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1915.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1922.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1927.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1950.0 | Control | ---        | \(0.000\) | \(1.000\) | \(0.000\) |
| 1956.0 | Control | Low Asset  | \(1.000\) | \(1.000\) | \(1.000\) |
| 1967.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1988.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 1991.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 1994.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 1995.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2002.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2012.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2024.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2030.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2032.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 2039.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2047.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2051.0 | Control | High Asset | \(1.000\) | \(1.000\) | \(1.000\) |
| 2061.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2066.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2071.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2077.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2079.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2082.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2094.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 2103.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2106.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2107.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2114.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2115.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2139.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2160.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2205.0 | Control | ---        | \(1.000\) | \(0.000\) | \(0.000\) |
| 2214.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2225.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2226.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 2232.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2234.0 | Control | ---        | \(0.000\) | \(0.000\) | \(1.000\) |
| 2238.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2239.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2250.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |
| 2260.0 | Control | ---        | \(0.000\) | \(1.000\) | \(1.000\) |
| 2261.0 | Control | ---        | \(1.000\) | \(0.000\) | \(1.000\) |
| 2267.0 | Control | Low Asset  | \(1.000\) | \(1.000\) | \(1.000\) |
| 2268.0 | Control | ---        | \(1.000\) | \(0.000\) | \(0.000\) |
| 2269.0 | Control | ---        | \(1.000\) | \(0.000\) | \(0.000\) |
| 2270.0 | Control | ---        | \(1.000\) | \(1.000\) | \(1.000\) |

We see seven cases in which there's some discrepancy in the assignment, and 107 cases in which the 
dataframe =D= has a household assigned to "Control" but where
=random_assignment= has no assignment recorded. 

# same as above but with locations.csv
#+begin_src python :results output raw table
import pandas as pd
from cfe.df_utils import df_to_orgtbl
import sys
sys.path.append('../analysis')
from check_attrition import D

R=pd.read_csv('../../TUP-data/csv/locations.csv')
R=R.set_index('RespID')      
R.index.name = 'idno'
R.replace(to_replace='Gift', value='Control', inplace=True)

D.replace(to_replace = 'Low Asset', value = 'Tup', inplace=True)
D.replace(to_replace = 'High Asset', value = 'Tup', inplace=True)


A=pd.DataFrame({'D':D['group'],'R':R['Group'],'B':D['Base'],'M':D['Mid'],'E':D['End']})

# Fix issue with variant capitalizations
A.D=A.D.str.title()
A.R=A.R.str.title()

# Examine observations where assignments differ:
print(df_to_orgtbl(A[A['D']!=A['R']]))
#+end_src

#+RESULTS:
|      | D       | R       | B         | M         | E         |
|------+---------+---------+-----------+-----------+-----------|
| 1157 | Control | Tup     | \(1.000\) | \(0.000\) | \(1.000\) |
| 1248 | ---     | Tup     | ---       | ---       | ---       |
| 1462 | ---     | Control | ---       | ---       | ---       |
| 1647 | ---     | Control | ---       | ---       | ---       |
| 1763 | ---     | Control | ---       | ---       | ---       |
| 2131 | Tup     | Control | \(0.000\) | \(1.000\) | \(1.000\) |
| 2204 | ---     | Control | ---       | ---       | ---       |

# same as above but with checklist_20150602.csv
#+begin_src python :results output raw table
import pandas as pd
from cfe.df_utils import df_to_orgtbl
import sys
sys.path.append('../analysis')
from check_attrition import D

R=pd.read_csv( "../../ElliottTUP/endline/checklists/checklist_20150602.csv")
R=R.set_index('RespID')      
R.index.name = 'idno'
R.replace(to_replace='Gift', value='Control', inplace=True)
R.replace(to_replace='First', value='Cash', inplace=True)
R.replace(to_replace='Second', value='Cash', inplace=True)

D.replace(to_replace = 'Low Asset', value = 'Tup', inplace=True)
D.replace(to_replace = 'High Asset', value = 'Tup', inplace=True)


A=pd.DataFrame({'D':D['group'],'R':R['Give'],'B':D['Base'],'M':D['Mid'],'E':D['End']})

# Fix issue with variant capitalizations
A.D=A.D.str.title()
A.R=A.R.str.title()

# Examine observations where assignments differ:
print(df_to_orgtbl(A[A['D']!=A['R']]))
#+end_src

#+RESULTS:
|      | D       | R       | B         | M         | E         |
|------+---------+---------+-----------+-----------+-----------|
| 1157 | Control | Tup     | \(1.000\) | \(0.000\) | \(1.000\) |
| 1248 | ---     | Tup     | ---       | ---       | ---       |
| 1462 | ---     | Control | ---       | ---       | ---       |
| 1647 | ---     | Control | ---       | ---       | ---       |
| 1763 | ---     | Control | ---       | ---       | ---       |
| 2131 | Tup     | Control | \(0.000\) | \(1.000\) | \(1.000\) |
| 2204 | ---     | Control | ---       | ---       | ---       |

# same as above but with locations.csv as R and asset_assign.csv as D
#+begin_src python :results output raw table
import pandas as pd
from cfe.df_utils import df_to_orgtbl

R=pd.read_csv('../../TUP-data/csv/locations.csv')
R=R.set_index('RespID')      
R.index.name = 'idno'
R.replace(to_replace='Gift', value='Control', inplace=True)

D = pd.read_csv('../../TUP-data/csv/asset_assign.csv')
D=D.set_index('respid')      
D.index.name = 'idno'
D.replace(to_replace = 'TUP-high asset', value = 'Tup', inplace=True)


A=pd.DataFrame({'D':D['group'],'R':R['Group']})

# Fix issue with variant capitalizations
A.D=A.D.str.title()
A.R=A.R.str.title()

# Examine observations where assignments differ:
print(df_to_orgtbl(A[A['D']!=A['R']]))
#+end_src

#+RESULTS:
| idno | D   | R       |
|------+-----+---------|
| 1042 | --- | Control |
| 1106 | --- | Control |
| 1154 | --- | Control |
| 1157 | --- | Tup     |
| 1186 | --- | Control |
| 1190 | --- | Control |
| 1207 | --- | Control |
| 1222 | --- | Control |
| 1252 | --- | Control |
| 1264 | --- | Control |
| 1285 | --- | Control |
| 1293 | --- | Control |
| 1295 | --- | Control |
| 1305 | --- | Control |
| 1311 | --- | Control |
| 1314 | --- | Control |
| 1315 | --- | Control |
| 1316 | --- | Control |
| 1320 | --- | Control |
| 1338 | --- | Control |
| 1348 | --- | Control |
| 1350 | --- | Control |
| 1368 | --- | Control |
| 1403 | --- | Control |
| 1421 | --- | Control |
| 1436 | --- | Control |
| 1453 | --- | Control |
| 1462 | --- | Control |
| 1476 | --- | Control |
| 1480 | --- | Control |
| 1486 | --- | Control |
| 1496 | --- | Control |
| 1502 | --- | Control |
| 1505 | --- | Control |
| 1507 | --- | Control |
| 1548 | --- | Control |
| 1561 | --- | Control |
| 1591 | --- | Control |
| 1619 | --- | Control |
| 1642 | --- | Control |
| 1647 | --- | Control |
| 1667 | --- | Control |
| 1679 | --- | Control |
| 1690 | --- | Control |
| 1702 | --- | Control |
| 1703 | --- | Control |
| 1730 | --- | Control |
| 1731 | --- | Control |
| 1744 | --- | Control |
| 1747 | --- | Control |
| 1748 | --- | Control |
| 1749 | --- | Control |
| 1750 | --- | Control |
| 1755 | --- | Control |
| 1763 | --- | Control |
| 1773 | --- | Control |
| 1780 | --- | Control |
| 1792 | --- | Control |
| 1797 | --- | Control |
| 1802 | --- | Control |
| 1813 | --- | Control |
| 1814 | --- | Control |
| 1826 | --- | Control |
| 1878 | --- | Control |
| 1892 | --- | Control |
| 1894 | --- | Control |
| 1910 | --- | Control |
| 1915 | --- | Control |
| 1922 | --- | Control |
| 1927 | --- | Control |
| 1950 | --- | Control |
| 1967 | --- | Control |
| 1988 | --- | Control |
| 1991 | --- | Control |
| 1994 | --- | Control |
| 1995 | --- | Control |
| 2002 | --- | Control |
| 2012 | --- | Control |
| 2024 | --- | Control |
| 2030 | --- | Control |
| 2032 | --- | Control |
| 2039 | --- | Control |
| 2047 | --- | Control |
| 2061 | --- | Control |
| 2066 | --- | Control |
| 2071 | --- | Control |
| 2077 | --- | Control |
| 2079 | --- | Control |
| 2082 | --- | Control |
| 2094 | --- | Control |
| 2103 | --- | Control |
| 2106 | --- | Control |
| 2107 | --- | Control |
| 2114 | --- | Control |
| 2115 | --- | Control |
| 2131 | Tup | Control |
| 2139 | --- | Control |
| 2160 | --- | Control |
| 2204 | --- | Control |
| 2205 | --- | Control |
| 2214 | --- | Control |
| 2225 | --- | Control |
| 2226 | --- | Control |
| 2232 | --- | Control |
| 2234 | --- | Control |
| 2238 | --- | Control |
| 2239 | --- | Control |
| 2250 | --- | Control |
| 2260 | --- | Control |
| 2261 | --- | Control |
| 2268 | --- | Control |
| 2269 | --- | Control |
| 2270 | --- | Control |

# same as above but with master_assignment.csv as R and TUP_full.dta (group) as D
#+begin_src python :results output raw table
import pandas as pd
from cfe.df_utils import df_to_orgtbl

R=pd.read_csv('../../TUP-report/documents/master_assignment.csv')
R=R.set_index('RespID')      
R.index.name = 'idno'

D = pd.read_stata('../../TUP-data/TUP_full.dta')
D=D.set_index('idno')      
D.replace(to_replace = 'Low Asset', value = 'Tup', inplace=True)
D.replace(to_replace = 'High Asset', value = 'Tup', inplace=True)


A=pd.DataFrame({'D':D['group'],'R':R['Group']})

# Fix issue with variant capitalizations
A.D=A.D.str.title()
A.R=A.R.str.title()

# Examine observations where assignments differ:
print(df_to_orgtbl(A[A['D']!=A['R']]))
#+end_src

#+RESULTS:
|   idno | D       | R       |
|--------+---------+---------|
| 1248.0 | ---     | Tup     |
| 1359.0 | Control | ---     |
| 1462.0 | ---     | Control |
| 1484.0 | Control | ---     |
| 1553.0 | Control | ---     |
| 1647.0 | ---     | Control |
| 1763.0 | ---     | Control |
| 1960.0 | Control | ---     |
| 2142.0 | Control | ---     |
| 2174.0 | Control | ---     |
| 2204.0 | ---     | Control |

# same as above but with master_assignment.csv as R and TUP_full.dta (group_b) as D
#+begin_src python :results output raw table
import pandas as pd
from cfe.df_utils import df_to_orgtbl

R=pd.read_csv('../../TUP-report/documents/master_assignment.csv')
R=R.set_index('RespID')      
R.index.name = 'idno'

D = pd.read_stata('../../TUP-data/TUP_full.dta')
D=D.set_index('idno')      
D.replace(to_replace = 'Low Asset', value = 'Tup', inplace=True)
D.replace(to_replace = 'High Asset', value = 'Tup', inplace=True)


A=pd.DataFrame({'D':D['group_b'],'R':R['Group']})

# Fix issue with variant capitalizations
A.D=A.D.str.title()
A.R=A.R.str.title()

# Examine observations where assignments differ:
print(df_to_orgtbl(A[A['D']!=A['R']]))
#+end_src

#+RESULTS:
|   idno | D   | R       |
|--------+-----+---------|
| 1042.0 |     | Control |
| 1106.0 |     | Control |
| 1151.0 |     | Control |
| 1154.0 |     | Control |
| 1157.0 |     | Control |
| 1186.0 |     | Control |
| 1187.0 |     | Tup     |
| 1190.0 |     | Control |
| 1207.0 |     | Control |
| 1222.0 |     | Control |
| 1248.0 | --- | Tup     |
| 1252.0 |     | Control |
| 1264.0 |     | Control |
| 1285.0 |     | Control |
| 1293.0 |     | Control |
| 1295.0 |     | Control |
| 1300.0 |     | Control |
| 1305.0 |     | Control |
| 1311.0 |     | Control |
| 1314.0 |     | Control |
| 1315.0 |     | Control |
| 1316.0 |     | Control |
| 1318.0 |     | Control |
| 1320.0 |     | Control |
| 1338.0 |     | Control |
| 1344.0 |     | Control |
| 1348.0 |     | Control |
| 1350.0 |     | Control |
| 1359.0 |     | ---     |
| 1368.0 |     | Control |
| 1403.0 |     | Control |
| 1421.0 |     | Control |
| 1436.0 |     | Control |
| 1453.0 |     | Control |
| 1462.0 | --- | Control |
| 1472.0 |     | Control |
| 1476.0 |     | Control |
| 1480.0 |     | Control |
| 1484.0 |     | ---     |
| 1486.0 |     | Control |
| 1496.0 |     | Control |
| 1502.0 |     | Control |
| 1505.0 |     | Control |
| 1507.0 |     | Control |
| 1548.0 |     | Control |
| 1553.0 |     | ---     |
| 1561.0 |     | Control |
| 1591.0 |     | Control |
| 1619.0 |     | Control |
| 1642.0 |     | Control |
| 1647.0 | --- | Control |
| 1667.0 |     | Control |
| 1679.0 |     | Control |
| 1690.0 |     | Control |
| 1702.0 |     | Control |
| 1703.0 |     | Control |
| 1730.0 |     | Control |
| 1731.0 |     | Control |
| 1744.0 |     | Control |
| 1747.0 |     | Control |
| 1748.0 |     | Control |
| 1749.0 |     | Control |
| 1750.0 |     | Control |
| 1755.0 |     | Control |
| 1758.0 |     | Tup     |
| 1760.0 |     | Control |
| 1763.0 | --- | Control |
| 1773.0 |     | Control |
| 1780.0 |     | Control |
| 1792.0 |     | Control |
| 1797.0 |     | Control |
| 1802.0 |     | Control |
| 1807.0 |     | Cash    |
| 1813.0 |     | Control |
| 1814.0 |     | Control |
| 1825.0 |     | Control |
| 1826.0 |     | Control |
| 1878.0 |     | Control |
| 1892.0 |     | Control |
| 1894.0 |     | Control |
| 1910.0 |     | Control |
| 1915.0 |     | Control |
| 1922.0 |     | Control |
| 1927.0 |     | Control |
| 1950.0 |     | Control |
| 1960.0 |     | ---     |
| 1966.0 |     | Control |
| 1967.0 |     | Control |
| 1985.0 |     | Control |
| 1988.0 |     | Control |
| 1991.0 |     | Control |
| 1994.0 |     | Control |
| 1995.0 |     | Control |
| 2002.0 |     | Control |
| 2012.0 |     | Control |
| 2024.0 |     | Control |
| 2030.0 |     | Control |
| 2032.0 |     | Control |
| 2039.0 |     | Control |
| 2047.0 |     | Control |
| 2061.0 |     | Control |
| 2066.0 |     | Control |
| 2071.0 |     | Control |
| 2077.0 |     | Control |
| 2079.0 |     | Control |
| 2082.0 |     | Control |
| 2094.0 |     | Control |
| 2103.0 |     | Control |
| 2106.0 |     | Control |
| 2107.0 |     | Control |
| 2114.0 |     | Control |
| 2115.0 |     | Control |
| 2119.0 |     | Tup     |
| 2131.0 |     | Tup     |
| 2134.0 |     | Control |
| 2139.0 |     | Control |
| 2142.0 |     | ---     |
| 2160.0 |     | Control |
| 2174.0 |     | ---     |
| 2204.0 | --- | Control |
| 2205.0 |     | Control |
| 2212.0 |     | Control |
| 2214.0 |     | Control |
| 2225.0 |     | Control |
| 2226.0 |     | Control |
| 2232.0 |     | Control |
| 2234.0 |     | Control |
| 2238.0 |     | Control |
| 2239.0 |     | Control |
| 2250.0 |     | Control |
| 2252.0 |     | Tup     |
| 2260.0 |     | Control |
| 2261.0 |     | Control |
| 2268.0 |     | Control |
| 2269.0 |     | Control |
| 2270.0 |     | Control |


*************** END



* Old Elliott Results                                              :noexport:
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-02-23 Tue 11:41
  :ARCHIVE_FILE: ~/Research/SS-TUP/TUP-report/documents/EndlineReport.org
  :ARCHIVE_OLPATH: Results/Assets
  :ARCHIVE_CATEGORY: EndlineReport
  :END:
#+name: asset_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<asset_analysis>>
return tab
#+end_src

#+RESULTS: asset_results WITH Tgroup Fixed Effects
|----------------------------------+--------------------+-------------------+-------------------|
|                                  | Total              | Productive        | Livestock         |
|----------------------------------+--------------------+-------------------+-------------------|
| CTL mean                         | \(1149.392\)       | \(303.427\)       | \(191.478\)       |
|                                  | \((1494.992)\)     | \((586.417)\)     | \((529.489)\)     |
|----------------------------------+--------------------+-------------------+-------------------|
| TUP*2014                         | \(301.119^{***}\)  | \(185.870^{***}\) | \(145.139^{***}\) |
|                                  | \((100.921)\)      | \((45.928)\)      | \((39.745)\)      |
| TUP*2015                         | \(152.392\)        | \(76.187^{*}\)    | \(87.150^{**}\)   |
|                                  | \((100.921)\)      | \((45.928)\)      | \((39.745)\)      |
| CSH*2014                         | \(124.143\)        | \(50.705\)        | \(10.077\)        |
|                                  | \((127.384)\)      | \((57.967)\)      | \((50.161)\)      |
| CSH*2015                         | \(-112.701\)       | \(-16.201\)       | \(2.840\)         |
|                                  | \((127.384)\)      | \((57.967)\)      | \((50.161)\)      |
| TUP                              | \(453.511^{***}\)  | \(262.057^{***}\) | \(232.289^{***}\) |
|                                  | \((63.925)\)       | \((29.053)\)      | \((25.152)\)      |
| CSH                              | \(11.442\)         | \(34.504\)        | \(12.917\)        |
|                                  | \((80.739)\)       | \((36.668)\)      | \((31.734)\)      |
| 2014                             | \(895.043^{***}\)  | \(268.534^{***}\) | \(136.818^{***}\) |
|                                  | \((88.645)\)       | \((39.674)\)      | \((34.189)\)      |
| 2015                             | \(1056.150^{***}\) | \(295.315^{***}\) | \(183.214^{***}\) |
|                                  | \((88.645)\)       | \((39.674)\)      | \((34.189)\)      |
| Bsln2013                         | \(0.072^{***}\)    | \(0.015\)         | \(0.021\)         |
|                                  | \((0.017)\)        | \((0.012)\)       | \((0.013)\)       |
| Bsln_NAN                         | \(0.000\)          | \(0.000\)         | \(0.000\)         |
|                                  | \((0.000)\)        | \((0.000)\)       | \((0.000)\)       |
|----------------------------------+--------------------+-------------------+-------------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | \(413.820^{**}\)   | \(202.071^{**}\)  | \(142.298^{**}\)  |
|                                  | \((181.534)\)      | \((82.625)\)      | \((71.498)\)      |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | \(265.093^{**}\)   | \(92.388\)        | \(84.310\)        |
|                                  | \((134.945)\)      | \((61.419)\)      | \((53.148)\)      |
|----------------------------------+--------------------+-------------------+-------------------|
| F-stat                           | \(12.984\)         | \(14.753\)        | \(15.917\)        |
| N                                | \(1488.000\)       | \(1488.000\)      | \(1488.000\)      |
|----------------------------------+--------------------+-------------------+-------------------|

#+RESULTS: asset_results WITHOUT Tgroup Fixed Effects
|----------------------------------+--------------------+-------------------+-------------------|
|                                  | Total              | Productive        | Livestock         |
|----------------------------------+--------------------+-------------------+-------------------|
| CTL mean                         | \(1149.392\)       | \(303.427\)       | \(191.478\)       |
|                                  | \((1494.992)\)     | \((586.417)\)     | \((529.489)\)     |
|----------------------------------+--------------------+-------------------+-------------------|
| TUP*2014                         | \(754.630^{***}\)  | \(447.927^{***}\) | \(377.428^{***}\) |
|                                  | \((135.492)\)      | \((61.624)\)      | \((53.338)\)      |
| TUP*2015                         | \(605.902^{***}\)  | \(338.244^{***}\) | \(319.439^{***}\) |
|                                  | \((135.492)\)      | \((61.624)\)      | \((53.338)\)      |
| CSH*2014                         | \(135.584\)        | \(85.208\)        | \(22.994\)        |
|                                  | \((171.069)\)      | \((77.777)\)      | \((67.307)\)      |
| CSH*2015                         | \(-101.259\)       | \(18.303\)        | \(15.757\)        |
|                                  | \((171.069)\)      | \((77.777)\)      | \((67.307)\)      |
| 2014                             | \(895.043^{***}\)  | \(268.534^{***}\) | \(136.818^{***}\) |
|                                  | \((88.645)\)       | \((39.674)\)      | \((34.189)\)      |
| 2015                             | \(1056.150^{***}\) | \(295.315^{***}\) | \(183.214^{***}\) |
|                                  | \((88.645)\)       | \((39.674)\)      | \((34.189)\)      |
| Bsln2013                         | \(0.072^{***}\)    | \(0.015\)         | \(0.021\)         |
|                                  | \((0.017)\)        | \((0.012)\)       | \((0.013)\)       |
| Bsln_NAN                         | \(0.000\)          | \(0.000\)         | \(0.000\)         |
|                                  | \((0.000)\)        | \((0.000)\)       | \((0.000)\)       |
|----------------------------------+--------------------+-------------------+-------------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | \(855.889^{***}\)  | \(429.624^{***}\) | \(361.670^{***}\) |
|                                  | \((218.012)\)      | \((99.221)\)      | \((85.859)\)      |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | \(707.162^{***}\)  | \(319.941^{***}\) | \(303.682^{***}\) |
|                                  | \((181.065)\)      | \((82.403)\)      | \((71.306)\)      |
|----------------------------------+--------------------+-------------------+-------------------|
| F-stat                           | \(12.984\)         | \(14.753\)        | \(15.917\)        |
| N                                | \(1488.000\)       | \(1488.000\)      | \(1488.000\)      |
|----------------------------------+--------------------+-------------------+-------------------|
#+end_example


* Elliott savings results
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-02-23 Tue 12:08
  :ARCHIVE_FILE: ~/Research/SS-TUP/TUP-report/documents/EndlineReport.org
  :ARCHIVE_OLPATH: Results/Assets
  :ARCHIVE_CATEGORY: EndlineReport
  :END:
#+name: savings_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<savings_analysis>>
return Table
#+end_src

|----------------------------------+--------------+--------------+--------------+--------------|
| % > 0                            | Savings      | Food Sav     | LandCult     | LandOwn      |
|----------------------------------+--------------+--------------+--------------+--------------|
| CTL mean                         | $0.45$       | $0.82$       | $0.82$       | $0.90$       |
|----------------------------------+--------------+--------------+--------------+--------------|
| CSH*2014                         | $-0.06$      | $0.00$       | $-0.04$      | $-0.01$      |
|                                  | $( 0.06)$    | $( 0.04)$    | $( 0.04)$    | $( 0.04)$    |
| CSH*2015                         | $0.03$       | $0.02$       | $0.05$       | $0.02$       |
|                                  | $( 0.05)$    | $( 0.04)$    | $( 0.04)$    | $( 0.04)$    |
| TUP*2014                         | $0.22^{***}$ | $-0.02$      | $-0.03$      | $-0.00$      |
|                                  | $( 0.04)$    | $( 0.03)$    | $( 0.03)$    | $( 0.03)$    |
| TUP*2015                         | $0.21^{***}$ | $-0.03$      | $0.01$       | $-0.01$      |
|                                  | $( 0.04)$    | $( 0.03)$    | $( 0.03)$    | $( 0.03)$    |
| 2014                             | $0.43^{***}$ | $1.00^{***}$ | $0.83^{***}$ | $0.82^{***}$ |
|                                  | $( 0.04)$    | $( 0.02)$    | $( 0.06)$    | $( 0.05)$    |
| 2015                             | $0.39^{***}$ | $0.82^{***}$ | $0.77^{***}$ | $0.84^{***}$ |
|                                  | $( 0.04)$    | $( 0.02)$    | $( 0.05)$    | $( 0.05)$    |
| Bsln2013                         | $0.05$       |              | $0.05$       | $0.07$       |
|                                  | $( 0.04)$    |              | $( 0.05)$    | $( 0.04)$    |
| Bsln NA                          | $0.08^{*}$   |              | $0.05$       | $0.05$       |
|                                  | $( 0.04)$    |              | $( 0.06)$    | $( 0.05)$    |
|----------------------------------+--------------+--------------+--------------+--------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $0.19$       | $-0.04$      | $-0.07$      | $-0.02$      |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $0.18$       | $-0.05$      | $-0.03$      | $-0.03$      |
|----------------------------------+--------------+--------------+--------------+--------------|
| F-stat                           | $8.83$       | $15.60$      | $0.79$       | $0.76$       |
| N                                | $1259.00$    | $870.00$     | $1231.00$    | $1251.00$    |
|----------------------------------+--------------+--------------+--------------+--------------|

#+name: tab:Savings
#+caption: Average treatment effects by group-year on total value (in SSP) of all cash and food savings and area (in fedan) of land being cultiviated by the household (including rented or temporary-use) and owned by the household.
#+attr_latex: :environment longtable :align lrrrrrrr
|----------------------------------+----------------+----------------+----------------+----------------|
| Amt.                             | Savings        | Food Sav       | LandCult       | LandOwn        |
|----------------------------------+----------------+----------------+----------------+----------------|
| CTL mean                         | $191.19$       | $114.78$       | $61.88$        | $46.00$        |
|----------------------------------+----------------+----------------+----------------+----------------|
| CSH*2014                         | $28.74$        | $0.22$         | $10.18$        | $10.50$        |
|                                  | $(42.93)$      | $(15.38)$      | $(15.07)$      | $(12.57)$      |
| CSH*2015                         | $91.40^{**}$   | $-14.34$       | $-39.18^{***}$ | $-32.37^{***}$ |
|                                  | $(40.89)$      | $(14.98)$      | $(14.90)$      | $(11.95)$      |
| TUP*2014                         | $-27.09$       | $17.16$        | $-4.76$        | $-3.02$        |
|                                  | $(29.76)$      | $(12.33)$      | $(11.94)$      | $(10.04)$      |
| TUP*2015                         | $81.33^{***}$  | $1.13$         | $-17.38$       | $-12.56$       |
|                                  | $(29.32)$      | $(12.26)$      | $(11.65)$      | $( 9.41)$      |
| 2014                             | $106.72^{***}$ | $62.03^{***}$  | $11.37$        | $17.31^{**}$   |
|                                  | $(24.85)$      | $( 8.36)$      | $( 9.94)$      | $( 8.56)$      |
| 2015                             | $163.04^{***}$ | $114.78^{***}$ | $61.52^{***}$  | $51.89^{***}$  |
|                                  | $(24.13)$      | $( 7.60)$      | $( 9.54)$      | $( 7.88)$      |
| Bsln2013                         | $0.05^{**}$    |                | $0.94$         | $-2.43$        |
|                                  | $( 0.02)$      |                | $( 3.07)$      | $( 1.95)$      |
| Bsln NA                          | $40.07^{*}$    |                | $-1.60$        | $-6.02$        |
|                                  | $(21.24)$      |                | $( 9.92)$      | $( 8.29)$      |
|----------------------------------+----------------+----------------+----------------+----------------|
| $\beta^{TUP}_{2014}-\beta^{CSH}$ | $-118.49$      | $31.50$        | $34.42$        | $29.35$        |
| $\beta^{TUP}_{2015}-\beta^{CSH}$ | $-10.07$       | $15.47$        | $21.79$        | $19.80$        |
|----------------------------------+----------------+----------------+----------------+----------------|
| F-stat                           | $7.41$         | $7.14$         | $4.91$         | $3.72$         |
| N                                | $671.00$       | $777.00$       | $1042.00$      | $1114.00$      |
|----------------------------------+----------------+----------------+----------------+----------------|


* Old income results
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-03-01 Mon 08:14
  :ARCHIVE_FILE: ~/Research/SS-TUP/TUP-report/documents/EndlineReport.org
  :ARCHIVE_OLPATH: Results/Income
  :ARCHIVE_CATEGORY: EndlineReport
  :END:
#+name: income_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<income_analysis>>
return tab
#+end_src

#+Caption: Distribution of total observed income by group
#+NAME: fig:Income_group
[[../figures/Income_group.png]] 

#+name: tab:Income
#+caption: Average treatment effects by group-year on total value (in SSP) of income reported in 2015 by sector.
#+attr_latex: :environment longtable :align lrrrrrrr
|---------------------------+---------------+---------------+------------+------------|
|                           | Farm          | Livestock     | Non-Farm   | Total      |
|---------------------------+---------------+---------------+------------+------------|
| CTL mean                  | $773.05$      | $640.33$      | $3774.49$  | $4325.54$  |
|---------------------------+---------------+---------------+------------+------------|
| TUP                       | $-142.20^{*}$ | $281.12^{**}$ | $86.24$    | $327.83$   |
|                           | $(77.21)$     | $(126.30)$    | $(469.48)$ | $(455.95)$ |
| CSH                       | $-26.15$      | $-83.81$      | $61.80$    | $7.92$     |
|                           | $(100.82)$    | $(177.25)$    | $(620.53)$ | $(600.43)$ |
|---------------------------+---------------+---------------+------------+------------|
| N                         | $531.00$      | $380.00$      | $606.00$   | $671.00$   |
| F-stat                    | $1.75$        | $3.48$        | $0.02$     | $0.28$     |
|---------------------------+---------------+---------------+------------+------------|
| $\beta^{TUP}-\beta^{CSH}$ | $-116.05$     | $364.94^{**}$ | $24.44$    | $319.91$   |
|                           | $(105.79)$    | $(174.74)$    | $(651.27)$ | $(629.93)$ |
|---------------------------+---------------+---------------+------------+------------|


* Old Conflict Tables                                       :noexport:ignore:
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-03-01 Mon 08:32
  :ARCHIVE_FILE: ~/Research/SS-TUP/TUP-report/documents/EndlineReport.org
  :ARCHIVE_OLPATH: Results/Exposure to Conflict
  :ARCHIVE_CATEGORY: EndlineReport
  :END:

#+name: conflict_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<conflict_analysis>>
return Table
#+end_src

#+name: tab:conflict_exposure
#+caption: Average treatment effects by group-year on the probability of having been affected in a significant way by the outbreak of violence in late 2013
#+attr_latex: :environment longtable :align lrrrrrrr
|----------+---------------+--------------+--------------+--------------+--------------+--------------|
|          | Affected      | Migrated     | NoInvest     | NoMeans      | ProtectLives | Worried      |
|----------+---------------+--------------+--------------+--------------+--------------+--------------|
| CTL mean | $0.53^{***}$  | $0.33^{***}$ | $0.16^{***}$ | $0.33^{***}$ | $0.38^{***}$ | $0.93^{***}$ |
|          | $( 0.03)$     | $( 0.02)$    | $( 0.02)$    | $( 0.02)$    | $( 0.03)$    | $( 0.01)$    |
| TUP      | $-0.13^{***}$ | $0.04$       | $-0.06^{**}$ | $-0.06$      | $0.02$       | $-0.02$      |
|          | $( 0.04)$     | $( 0.04)$    | $( 0.03)$    | $( 0.04)$    | $( 0.05)$    | $( 0.02)$    |
|----------+---------------+--------------+--------------+--------------+--------------+--------------|
| F-stat   | $9.20$        | $0.96$       | $3.95$       | $2.55$       | $0.19$       | $0.49$       |
| N        | $601.00$      | $655.00$     | $655.00$     | $655.00$     | $585.00$     | $603.00$     |
|----------+---------------+--------------+--------------+--------------+--------------+--------------|

#+Caption: % of Sample reporting exposure to conflict by group.
#+NAME: fig:conflict_exposure
[[../figures/conflict_exposure.png]] 


* Elliott attrition balance table                                    :ignore:
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-03-04 Thu 17:39
  :ARCHIVE_FILE: ~/Research/SS-TUP/TUP-report/documents/EndlineReport.org
  :ARCHIVE_OLPATH: Data Collection & Timeline/Sample Selection/Attrition balance table
  :ARCHIVE_CATEGORY: EndlineReport
  :END:
#+caption: Means of household baseline characteristics and regression coefficients for whether they were ultimately found at baseline or endline. (Note that this does not consider households found only in 2014 or 2015).
#+attr_latex: :environment longtable :align lrrrrr
#+name: tab:attrition_balance
|-----------------+---------------+----------------+----------------|
| HH Features     | $\mbox{Mean}_{2013}$ | $\beta_{2014}$ | $\beta_{2015}$ |
|-----------------+---------------+----------------+----------------|
| HH size         |         7.223 |        0.595** |          0.428 |
| # Child         |         3.328 |       0.656*** |          0.423 |
| Asset Prod.     |       512.822 |        126.360 |       -369.190 |
| Asset Tot.      |      1494.324 |        361.889 |      -689.174* |
| Daily Exp       |        25.212 |          1.257 |         -4.150 |
| Daily Food      |        24.300 |          0.299 |        -4.790* |
| In Business     |         0.415 |          0.038 |          0.007 |
| Land Access     |         2.324 |          0.014 |          0.305 |
| No. Houses      |         2.863 |          0.305 |          0.367 |
| Cash Savings    |       178.662 |         46.322 |         54.295 |
|-----------------+---------------+----------------+----------------|
| Assets          |               |                |                |
|-----------------+---------------+----------------+----------------|
| Bed             |       250.534 |         12.649 |        -51.133 |
| Bicycle         |       102.174 |         11.179 |          4.212 |
| Mobile          |       101.482 |          6.336 |        -13.028 |
| Motorcycle      |       481.885 |        213.002 |       -241.819 |
| Carts           |         2.751 |          1.929 |          2.962 |
| Cows            |       181.402 |         67.862 |        -89.273 |
| Smallanimals    |       180.716 |         18.966 |        -79.014 |
|-----------------+---------------+----------------+----------------|
| Consumption     |               |                |                |
|-----------------+---------------+----------------+----------------|
| Cereals         |         8.882 |         -0.084 |       -3.714** |
| Beans           |         0.826 |          0.269 |         -0.382 |
| Ceremonies      |         0.141 |         -0.020 |         -0.038 |
| Charities       |         0.027 |          0.007 |         -0.001 |
| Clothesfootwear |         0.663 |         0.180* |         -0.206 |
| Cosmetics       |         0.668 |          0.005 |          0.229 |
| Dowry           |         1.263 |          0.755 |         -0.399 |
| Egg             |         1.069 |         -0.005 |          0.106 |
| Fish            |         2.417 |         -0.132 |          0.036 |
| Fruit           |         0.656 |          0.009 |         -0.151 |
| Fuel            |         0.733 |          0.105 |         -0.049 |
| Meat            |         3.981 |          0.254 |          0.300 |
| Other           |           0.0 |          0.000 |          0.000 |
| Poultry         |        39.437 |        23.634* |         -2.243 |
| Salt            |         0.438 |      -0.140*** |         -0.043 |
| Soap            |         0.475 |        -0.181* |          0.047 |
| Sugar           |         1.647 |         -0.285 |         -0.020 |
| Textiles        |         0.165 |          0.010 |          0.011 |
| Transport       |         0.163 |          0.004 |          0.018 |
| Tv              |        39.915 |        -16.377 |          0.845 |
| Utensils        |         0.247 |          0.062 |         -0.023 |
| Vegetables      |         1.446 |          0.096 |         -0.151 |
|-----------------+---------------+----------------+----------------|


* Enterprise selection
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-05-17 Mon 18:31
  :ARCHIVE_FILE: ~/Research/SS-TUP/Report/documents/EndlineReport.org
  :ARCHIVE_OLPATH: The Program/The Ultra-Poor Graduation Program
  :ARCHIVE_CATEGORY: EndlineReport
  :END:
#+begin_src python 
import pandas as pd
from cfe.df_utils import df_to_orgtbl

df = pd.read_stata('../../TUP-data/Baseline/Tup programme sudan single file.dta',convert_categoricals=False)

df = df.set_index('idno').iloc[:,-15:]

# Different enumerators handled this question differently; some list
# the assets alphabetically then giving the numeric preference
# ordering of each asset, while others wrote the asset letters in
# order of preference.  Here we sort this out.

idx = df.rename(columns=lambda s:tuple(s.split('_'))).columns
df.columns = pd.MultiIndex.from_tuples(idx)

order  = df['rank'].stack()

ac = df['ac']
ac.columns = range(5)
myorder = order.astype(int)-1

myidx=myorder.droplevel(1).reset_index().set_index(['idno',0])

maybe = ac.stack()[myidx.index]
maybe.index = maybe.sort_index().index
maybe =  maybe[~maybe.index.duplicated()]
maybe.name = 'Choice'

ranks = maybe.unstack()

freq = pd.concat([ranks[x].value_counts() for x in ranks],axis=1)
freq.rename(index={'A':'Goats','B':'Ducks','C':'Maize','D':'Vegetables','E':'Dry Fish','':'Missing'},inplace=True)
freq.columns = freq.columns + 1

print(df_to_orgtbl(freq,float_fmt='%d'))
#+end_src


#+name: tab:enterprise_choice
#+caption: Frequency of Rankings of Enterprise Choices
|            |   1 |   2 |   3 |   4 |   5 |
|------------+-----+-----+-----+-----+-----|
| Dry Fish   | 219 | 105 | 113 |  74 |  73 |
| Goats      | 149 | 115 | 107 | 107 | 116 |
| Ducks      | 120 | 172 | 137 |  88 |  68 |
| Vegetables |  65 |  85 | 108 | 138 | 164 |
| Maize      |  55 | 127 | 127 | 145 | 122 |
| Missing    |   2 |   1 | --- |   3 |   3 |

Table [[tab:enterprise_choice]] reports on the frequency of elicited
preference ranks over the five different choices.  Dry fish was the
most popular single first choice, with animal husbandry (goats then
ducks) the second and third choices.  Crop cultivation (whether of
vegetables or maize) was less popular. 


* Enterprise Assignment & Training
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-05-17 Mon 18:33
  :ARCHIVE_FILE: ~/Research/SS-TUP/Report/documents/EndlineReport.org
  :ARCHIVE_OLPATH: The Program/The Ultra-Poor Graduation Program
  :ARCHIVE_CATEGORY: EndlineReport
  :END:

The second phase of the program was enterprise assignment and
training. Unlike some other programs of this type (e.g., the UPG
program in Bangladesh described by cite:bandiera-etal17), the number
of households given each kind of asset was set in advance, with 75
enrolled in agricultural activities (either maize or vegetable
cultivation), 85 in duck rearing, 45 in goat rearing, and the rest in
small trade involving dried fish.  Within these limits, assignment to
particular activities was made at the discretion of program staff,
taking into account subjects' preferences and skills.

Households then attended training sessions. The first of these were
for general business skills around literacy, numeracy, and financial
management. The next were sector specific and focused on animal
husbandry or crop cultivation.

After training was completed, asset transfers began in late 2013 and
continued through the first few months of 2014. The productive assets
related to each enterprise were valued at around $240 per household,
with a random subset receiving an additional $60 in assets later
in 2014.  Shortly thereafter, households started to attend weekly or
semi-weekly meetings with other nearby participants to discuss with
each other and a BRAC extension officer the details of their
businesses. These meetings also included food transfers for a while,
which were designed to help get households to the point of receiving
revenue from their assets without having to sell them.  In all, the
market value of these food transfers were valued at $110, bringing the
total value of all transfers to either $350 or $410.


* COMMENT Extra Analysis
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-05-18 Tue 13:33
  :ARCHIVE_FILE: ~/Research/SS-TUP/Report/documents/EndlineReport.org
  :ARCHIVE_CATEGORY: EndlineReport
  :END:

** Good-level analysis

Next, \ref{tab:consumption_full} sets aside these aggregated measures to look more
carefully at potential changes in the composition of consumption in each group. Given
the large number of zeros, we use a linear model to consider first the 
frequency of non-zero consumption of each good among treatment and control
households, then look at levels of consumption among households with non-zero
consumption. \Tab{consumption_full} presents point estimates.

A few changes in the composition of consumption are interesting. UPG households appear to consume 17% less
sorghum (often considered an inferior good in Yei) and more on rice, which is
considered a higher-quality staple. While almost everyone reports some health
spending over the past month, both treatment groups spent more, though only
statistically significant in the cash group, which saw a 50% increase over the
control group. The cash group was also 30% (14 pp) more likely to have spent money
on funerals, though they did not spend more on average.

#+name: consumption_disaggreate_results
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
<<consumption_analysis>>
from matplotlib import pyplot as plt
#~ Only keep disaggregate items
Goods = C.filter(regex="^c_").rename(columns=lambda col: col[2:] if col.startswith("c_") else col)
#~ 1 if coded as >0, 0 if zero, keep NaN's missing, since those are only from missing surveys.
Nonzero = Goods.applymap(lambda i: float(i>0) if not np.isnan(i) else np.nan)
#~ Restrict Goods df to positive responses.
Goods = Goods.replace(0,np.nan)
too_many_zeros = 30
many_zeros = [item for item in Goods if Goods[item].notnull().sum()<too_many_zeros]
Nonzero = Nonzero.drop(many_zeros,1)
Goods = Goods.drop(many_zeros,1)

#~ Merge in Control Vars
controls = ["cons","UPG","CSH"]
Goods = Goods.join(C[controls],how="left")
Nonzero = Nonzero.join(C[controls],how="left")
Items = [item[:-2] for item in Goods if item.endswith("_e")]
CTL = Goods[controls].sum(axis=1)==1 #~ i.e. only constant ==1, UPG & UCT ==0
Goods_ctl_mean =   Goods.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Zeros_ctl_mean = Nonzero.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])

Zero, Zero_se = reg_table(regressions(Nonzero,"_e", outcomes=Items, Baseline="_b"), resultdf=True, Transpose=True)
Good, Good_se = reg_table(regressions(Goods,"_e", outcomes=Items, Baseline="_b"  ), resultdf=True, Transpose=True)
#~ Make full table of Standard errors
SE = Zero_se[["UPG","CSH"]].join(Good_se[["UPG","CSH"]], lsuffix=" (%>0)", rsuffix=" (Amt.)")

#~ Make full table of point estimates and control-group means
FullTable = pd.DataFrame({"Mean (CTL)":Goods_ctl_mean, "% >0 (CTL)":Zeros_ctl_mean}).join(Zero[["UPG","CSH"]])
FullTable = FullTable.join(Good[["UPG","CSH","N"]], lsuffix=" (%>0)", rsuffix=" (Amt.)")

#~ Make % change graph
fig, ax = plt.subplots(2,1, figsize=(6,9))
for i, group in enumerate(("UPG","CSH")):
    pct_change = FullTable[group+" (Amt.)"]/FullTable["Mean (CTL)"]
    se_change  = SE[group+" (Amt.)"]/FullTable["Mean (CTL)"]
    pct_change.sort()
    pct_change.plot(kind="bar", yerr=SE[group+" (Amt.)"], ax=ax[i])
    ax[i].set_title(group, fontsize=6)
fig.savefig("../figures/Consumption.png")

FullTable = df_to_orgtbl(FullTable, sedf=SE)
return FullTable
#+end_src

#+name: tab:consumption_full
#+caption: Control group means and estimated treatment effects for percent consuming any and total amounts consumed.
#+attr_latex: :environment longtable :align lrrrrrrr
 #+RESULTS: consumption_disaggreate_results

** Disaggregate Asset Results 

#+name: asset_disaggreate_results
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none 
<<assets_disaggreate_analysis>>
return tab
#+end_src

#+name: tab:asset_disaggregate
#+caption: Control means and treatment effects for Assets owned by >40 households
#+RESULTS: asset_disaggreate_results
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|
|              | # own  |              |             | Value     |                |                |          |
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|
| Item         | CTL    | UPG          | UCT        | CTL       | UPG            | UCT           | N        |
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|
| Pangas       | $1.06$ | $0.01$       | $0.13^{**}$ | $11.86$   | $1.66^{**}$    | $0.04$         | $410.00$ |
| Chickens     | $3.79$ | $0.70$       | $-0.32$     | $154.35$  | $23.31$        | $0.80$         | $162.00$ |
| Mobile       | $1.88$ | $-0.09$      | $0.08$      | $113.96$  | $2.62$         | $1.70$         | $569.00$ |
| Radio        | $1.62$ | $0.84$       | $-0.40$     | $57.25$   | $4.98$         | $5.10$         | $333.00$ |
| Shed         | $1.22$ | $-0.14$      | $-0.22$     | $48.81$   | $37.57$        | $6.81$         | $53.00$  |
| Stoves       | $1.44$ | $0.34$       | $0.31$      | $20.32$   | $18.19^{**}$   | $8.31$         | $84.00$  |
| Potspans     | $4.46$ | $-0.06$      | $-0.23$     | $102.73$  | $15.90$        | $-15.40$       | $582.00$ |
| Ducks        | $5.72$ | $4.26^{***}$ | $-0.16$     | $230.93$  | $109.99^{***}$ | $-19.34$       | $223.00$ |
| Motorcycle   | $1.51$ | $-0.48$      | $0.12$      | $2288.48$ | $300.46$       | $-196.32$      | $66.00$  |
| Chairtables  | $5.02$ | $0.25$       | $0.39$      | $167.62$  | $19.00$        | $-24.73$       | $638.00$ |
| Net          | $3.07$ | $0.03$       | $-0.08$     | $24.49$   | $0.66$         | $-3.81$        | $382.00$ |
| Axes         | $1.02$ | $0.03$       | $-0.02$     | $17.74$   | $0.02$         | $-3.94^{**}$   | $218.00$ |
| Smallanimals | $3.39$ | $0.29$       | $-0.90$     | $767.26$  | $-151.35$      | $-311.05^{**}$ | $155.00$ |
| Charcoal     | $2.20$ | $-0.26$      | $-0.83$     | $35.81$   | $-1.43$        | $-4.65$        | $176.00$ |
| Bicycle      | $6.34$ | $-5.46$      | $-5.52$     | $272.90$  | $-31.50$       | $-42.67$       | $135.00$ |
| Bed          | $3.17$ | $-0.23$      | $-0.40$     | $300.64$  | $19.32$        | $-57.78^{*}$   | $628.00$ |
| Tv           | $1.48$ | $-0.36$      | $-0.26$     | $380.45$  | $121.95$       | $348.23^{**}$  | $45.00$  |
|--------------+--------+--------------+-------------+-----------+----------------+----------------+----------|



* COMMENT Code appendix
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-05-18 Tue 13:33
  :ARCHIVE_FILE: ~/Research/SS-TUP/Report/documents/EndlineReport.org
  :ARCHIVE_CATEGORY: EndlineReport
  :END:
  
** Food Security

 #+name: foodsecure_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle foodsecurity.py
 #~ DevLunch_analysis.py
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 from UPG import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
 D = full_data(DIR = DATADIR,balance=[])
 WEEKLY = True

 codes = {"1-2 times a week": 3,
         "3-6 times a week": 2,
         "Everyday": 1,
         "everyday": 1,
         "Less than once a week": 4,
         "less than once a week": 4,
         "Never": 5,
         "never": 5}

 recode = lambda x: codes.setdefault(x,x)

 Aval2013 = D.filter(regex="^fs_.*_b").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Aval2014 = D.filter(regex="^fs_.*_m").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Aval2015 = D.filter(regex="^fs_.*_e").rename(columns=lambda x: x[3:-2]).applymap(recode)

 if WEEKLY:
     weekly = lambda i: float(i<3) if pd.notnull(i) else np.nan
     Aval2013 = Aval2013.applymap(weekly)
     Aval2014 = Aval2014.applymap(weekly)      
     Aval2015 = Aval2015.applymap(weekly)
 
 index_vars = "worried,portions,fewmeals,nofood,hungry,wholeday".split(",")
 Outcomes = index_vars+["z-score"]
 #~ Creates Year dummies, z-scores and baseline values as `var'2013
 for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
     Aval["Year"]=Year
     if not weekly:
        for var in index_vars:
            Aval[index_vars] = (Aval[index_vars]-Aval[index_vars].mean())/Aval[index_vars].std()
     FS_sum = Aval[index_vars].sum(axis=1)
     Aval["z-score"] = (FS_sum-FS_sum.mean())/FS_sum.std()
     for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
    
 Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
 Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
 Vals = Vals.join(D[["UPG","CSH"]])

 for group in ("UPG", "CSH"):
     for year in ("2013", "2014", "2015"):
         Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

 Controls = ['2014', '2015', 'UPG*2014', 'UPG*2015', 'CSH*2014', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both UPG*2014 and UPG*2015
 Vals=Vals.loc[2014:2015]
 regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = Vals["UPG"]+Vals["CSH"] ==0
 CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("UPG*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("UPG*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+end_src

** Consumption

 #+name: lambda_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/SS-lambdas.py

 import numpy as np
 import pandas as pd
 import cfe.estimation as nd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append("../../data")
 from UPG import full_data, consumption_data, regressions, reg_table, df_to_orgtbl, mobile_data
 food =  ['cereals', 'maize', 'sorghum', 'millet', 'potato', 'sweetpotato', 'rice', 'bread', 'beans', 'oil', 'salt', 'sugar', 'meat', 'livestock', 'poultry', 'fish', 'egg', 'nuts', 'milk', 'vegetables', 'fruit', 'tea', 'spices', 'alcohol', 'otherfood']
 month = ['fuel', 'medicine', 'airtime', 'cosmetics', 'soap', 'transport', 'entertainment', 'childcare', 'tobacco', 'batteries', 'church', 'othermonth']    
 ConsumptionItems = food+['airtime','fuel']
 mobile=True

 D = full_data(DIR=DATADIR)
 C, HH, T = consumption_data(D,WRITE=False,include2016=False)
 HH['log HHSIZE'] = HH["hh_size"].apply(np.log)
 HH = HH.drop("hh_size",1)
 y,z = C.replace(0,np.nan).apply(np.log).sort_index(level=[0,1,2])[ConsumptionItems].copy(),HH.sort_index(level=[0,1,2]).copy()
 y.index.names, z.index.names = ['j','t','mkt'], ['j','t','mkt']
 keep = pd.notnull(y.index.get_level_values("mkt"))
 y,z = y.loc[keep,:].align(z,join="left",axis=0)
 b,ce,d,sed= nd.estimate_reduced_form(y,z,return_se=True,VERBOSE=True)
 ce = ce.dropna(how='all')
 print("Getting Loglambdas")
 bphi,logL=nd.get_loglambdas(ce,TEST="warn")
 try:
    xrange
    logL.to_pickle(DATADIR + "ss-lambdas.df")
 except NameError: logL.to_pickle(DATADIR + "ss-lambdas3.df")
 
 if mobile:
     M,Mc,Mhh = mobile_data(use_dates=True,DIR = DATADIR+"Mobile/")
     y = Mc.replace(0,np.nan).apply(np.log).sort_index(level=[0,1,2]).filter(items=ConsumptionItems).copy()
     z = Mhh.sort_index(level=[0,1,2]).copy()
     y.index.names, z.index.names = ['j','t','mkt'], ['j','t','mkt']
     keep = pd.notnull(y.index.get_level_values("mkt"))
     y,z = y.loc[keep,:].align(z,join="left",axis=0)
     b,ce,d,sed= nd.estimate_reduced_form(y,z,return_se=True,VERBOSE=True)
     ce = ce.dropna(how='all')
     print("Getting Loglambdas")
     Mbphi,MlogL=nd.get_loglambdas(ce,TEST="warn")
     MlogL -= MlogL.mean()
     MlogL /= MlogL.std()
     MlogL = MlogL.unstack('t').drop('4February',1).stack()
     try:
       xrange
       MlogL.to_pickle(DATADIR + "ss-lambdas_mobile.df")
     except NameError: MlogL.to_pickle(DATADIR + "ss-lambdas_mobile3.df")

 #+end_src 

 #+name: consumption_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/SS-consumption.py
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 from UPG import full_data, consumption_data, regressions, reg_table, df_to_orgtbl, mobile_data
 
 USE_LAMBDAS = False #~ Whether to use the output of SS-lambdas.py as an outcome (You may have to run it first.)
 TGROUP_FE   = True  #~ Whether to use UPG & CSH dummy variables as controls (I'm pretty sure we should be)

 food = ['cereals', 'maize', 'sorghum', 'millet', 'potato', 'sweetpotato', 'rice', 'bread', 'beans', 'oil', 'salt', 'sugar', 'meat', 'livestock', 'poultry', 'fish', 'egg', 'nuts', 'milk', 'vegetables', 'fruit', 'tea', 'spices', 'alcohol', 'otherfood']
 month = ['fuel', 'medicine', 'airtime', 'cosmetics', 'soap', 'transport', 'entertainment', 'childcare', 'tobacco', 'batteries', 'church', 'othermonth']    
 year = ['clothesfootwear', 'womensclothes', 'childrensclothes', 'shoes', 'homeimprovement', 'utensils', 'furniture', 'textiles', 'ceremonies', 'funerals', 'charities', 'dowry', 'other']    


 D = full_data(DIR=DATADIR)
 C, HH, T = consumption_data(D,WRITE=True) #"csv")
 if USE_LAMBDAS:
    logL = pd.read_pickle(DATADIR + "ss-lambdas.df")
    logL.index.names=["HH","Year","Location"]
    C = C.join(logL,how="left").rename(columns={"loglambda":"$\log\lambda_{it}$"})
 C = C.reorder_levels([1,2,0])#.sort_index(level=0)
 keep = pd.notnull(C.index.get_level_values("Location"))
 C = C.loc[keep,:]

 # Make aggregate variables
 C["Food"]   = C.filter(items=food).sum(axis=1).replace(0,np.nan)
 C["Month"]   = C.filter(items=food).sum(axis=1)
 C["Year"]   = C.filter(items=food).sum(axis=1)
 C["Tot"]    = C[["Food","Month","Year"]].sum(axis=1).replace(0,np.nan)

 def align_indices(df1,df2):
    """
    Reorder levels of df2 to match that of df1
    Must have same index.names
    """
    I1, I2 = df1.index, df2.index
    try: assert(not set(I1.names).difference(I2.names))
    except AssertionError: raise ValueError("Index names must be the same")
    new_order = []
    for lvl in I1.names: new_order.append(I2.names.index(lvl))
    df2 = df2.reorder_levels(new_order)
    return df1, df2

 def winsorize(Series, **kwargs):
    """
    Need to implement two-sided censoring as well.
    WARNING: if Top<0, all zeros will be changed to Top
    """
    percent    = kwargs.setdefault("percent",99)
    stdev      = kwargs.setdefault("stdev",False)
    drop       = kwargs.setdefault("drop",False)
    drop_zeros = kwargs.setdefault("drop_zeros",True)
    twoway     = kwargs.setdefault("twoway",False)

    if drop_zeros: S = Series.replace(0,np.nan).dropna()
    else: S = Series.dropna()
    N_OBS = S.notnull().sum()
    if N_OBS<10: return S

    if percent: Top = np.percentile(S, percent)
    if stdev:   
        Top =  S.dropna().mean()
        Top += stdev*S.dropna().std()
    try: assert((not drop_zeros) or Top>0)
    except AssertionError: raise ValueError("Top < 0 but zeros excluded")
    if drop: replace_with = np.nan
    else:    replace_with = Top

    _Series = Series.copy()
    _Series[_Series>Top]=replace_with

    if not twoway: return _Series
    else:
        kwargs['twoway'] = False
        return -1*winsorize(-1*_Series, **kwargs)

 def USD_conversion(Exp,exchange_rate=1.,PPP=1.,inflation=1.,time='Year'):
    """
    Convert nominal local currency into price- and inflation-adjusted USD

    Exp - A numeric or pd.Series object 
    exchange_rate - Taken as LCU/USD. 
    PPP - Taken as $Real/$nominal
    inflation - Taken as % inflation compared to some baseline.
    time - If a list is passed, `time' indicates the name or position of the time level in Exp.index
        NOTE: This has to be a cumulative number, so if inflation is 20% for two straight years, that year should be divided by (1+.2)**2
    Final calculation will basically be Exp_usdppp = Exp*(exchange_rate*PPP)/inflation

    if pd.Series are passed for any kwarg, index name needs to be in the multi-index of Exp.
    """
    if type(inflation)==list: inflation=[1./i for i in inflation]
    else: inflation = 1/inflation
    if type(exchange_rate)==list: exchange_rate=[1./i for i in exchange_rate]
    else: exchange_rate = 1/exchange_rate
    
    _Exp = Exp.copy()
    VARS = (exchange_rate, PPP,inflation)
    if list in map(type,VARS):
        if time in _Exp.index.names: time=_Exp.index.names.index(time)
        time = _Exp.index.levels[time]
    for var in VARS:
        if type(var)==list: var=pd.Series(var,index=time)
        try: _Exp = _Exp.mul(var)
        except ValueError: #~ If Series index doesn't have a name, try this...
            var.index.name = var.name
            _Exp = _Exp.mul(var)
    return _Exp

 def percapita_conversion(Exp,HH,children=["boys","girls"],adult_equivalent=1.,minus_children='hh_size'):
    """
    Returns household per-capita expenditures given:
        `Exp'- Total household expenditures
        `HH' - Total number of individuals in the household
            If HH is a pd.DataFrame, Exp is divided by HH.sum(1)
            if `children' is the name of a column or a list of column names, 
            those first get divided by the factor adult_equivalent
    """
    try: HH.columns #~ If HH is a series, just divide though
    except AttributeError: return Exp.div(HH)
    _HH = HH.copy()
    if type(children)==str: children=[children]
    children = _HH.columns.intersection(children).tolist()
    if minus_children: _HH[minus_children] -= _HH[children].sum(1)
    if children: _HH[children] *= adult_equivalent
    Exp,_HH = align_indices(Exp,_HH)
    return Exp.div(_HH.sum(1).replace(0,1))

 #~ Source: http://data.worldbank.org/indicator/PA.NUS.PRVT.PP?locations=SS&name_desc=false
 xrate = [ 2.161, 2.162, 3.293] #~ Using PPP adjusted xrate and just setting PPP=1.
 PPP = 1.
 inflation= 1. #~ Bank data uses international $, which is inflation adjusted.
 C["Exp_usd"] = winsorize(USD_conversion(C["Tot"],exchange_rate=xrate,PPP=PPP,inflation=inflation))
 C["Tot_pc"] = percapita_conversion(C["Exp_usd"],HH,adult_equivalent=.5)
 #C["Exp_usdpc_tc"] = winsorize(C["Exp_usdpc"])

 #C["z-score"]  = (C["Tot"]-C["Tot"].mean())/C["Tot"].std()
 C["FoodShr"]= C["Food"].div(C["Tot"]) #$\approx$ FoodShare variable
 C["logTot"] = C["Tot"].apply(np.log)
 C = C.join(T, how="left",lsuffix="_")

 if USE_LAMBDAS: Outcomes = ["Tot","logTot","FoodShr", "Food", "$\log\lambda_{it}$"]
 else: Outcomes = ["Tot","logTot","FoodShr", "Food"]

 #$\approx$ Make Baseline variable
 for var in Outcomes: 
     Bl = C.loc[2013,var].reset_index("Location",drop=True)
     #if var in mC: mC = mC.join(Bl,rsuffix="2013", how="left")
     C = C.join(Bl,rsuffix="2013", how="left")


 C["Y"]=np.nan
 for yr in (2013, 2014, 2015): C.loc[yr,"Y"]=str(int(yr))

 C = C.join(pd.get_dummies(C["Y"]), how="left",lsuffix="_")
 for group in ("UPG", "CSH"):
     for year in ("2013", "2014", "2015"):
         interaction = C[group]*C[year]
         if interaction.sum()>0: C["{}*{}".format(group,year)] = interaction

 if TGROUP_FE: Controls = ["2014","2015", 'UPG*2014', 'CSH*2014', 'UPG*2015', 'CSH*2015', 'UPG','CSH']
 else:         Controls = ["2014","2015", 'UPG*2014', 'CSH*2014', 'UPG*2015', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both UPG*2014 and UPG*2015
 C = C.loc[2014:2015]
 regs  = regressions(C, outcomes=Outcomes,  controls=Controls,  Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = C["UPG"]+C["CSH"] ==0
 CTLmean = {var: C[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: C[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("UPG*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("UPG*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+END_SRC
   
 #+name: mobile_analysis
 #+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/SS-mobile.py
 import sys
 DATADIR = "../../data/"
 sys.path.append("../../data")
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 from UPG import full_data, consumption_data, regressions, reg_table, df_to_orgtbl, mobile_data
 ITEMS = ["beans", "sugar", "fish", "nuts", "vegetables", "airtime", "fuel"]

 D = full_data(DIR=DATADIR)
 HH, T = consumption_data(D,WRITE=True)[1:] #"csv")
 M, C,mHH= mobile_data(DIR = DATADIR+"Mobile/")
 try: logL = pd.read_pickle(DATADIR+"ss-lambdas_mobile.df")
 except EnvironmentError: raise IOError("Need to run SS-lambdas.py")
 logL.index.names=["HH","Year","Location"]
 logL.name       =["loglambda"]
 C    = C.join(logL,how="left").rename(columns={"loglambda":"$\log\lambda_{it}$"})
 C    = C.reorder_levels([1,0,2]).sortlevel()
 keep = pd.notnull(C.index.get_level_values("Location"))
 C    = C.loc[keep,:]
 # Make aggregate variables
 C["Tot"]    = C.filter(ITEMS).sum(axis=1).replace(0,np.nan)
 C["logTot"] = C["Tot"].apply(np.log)
 C           = C.join(T, how="left",lsuffix="_")
 C['const']  = 1.

 Outcomes =["Tot",  "logTot", "$\log\lambda_{it}$"]
 Controls= ['const', 'UPG', 'CSH']

 regs = regressions(C,outcomes=Outcomes, controls=Controls, Baseline=2013)
 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])
 CTL = C["UPG"]+C["CSH"] ==0
 CTLmean = {var: C.loc[CTL,var].mean() for var in Outcomes}
 CTLsd = {var: C.loc[CTL,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest= regs[var].t_test("UPG - CSH = 0").summary_frame()
     diff.loc[   r"$\beta^{UPG}-\beta^{CSH}$", var] = ttest["coef"][0]
     diff_se.loc[r"$\beta^{UPG}-\beta^{CSH}$", var] = ttest["std err"][0]

 results = results.append(diff)
 SE = SE.append(diff_se)
 mtab = df_to_orgtbl(results, sedf=SE)

 #+end_src
   
** Assets
   
#+name: asset_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/asset_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
from matplotlib import pyplot as plt
import sys
sys.path.append("../../data")
from UPG import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

TGROUP_FE = True

D = full_data(DIR="../../data/")

Outcomes = ["Total", "Productive","Livestock"]
Aval2013 = asset_vars(D,year=2013)[0][Outcomes]
Aval2014 = asset_vars(D,year=2014)[0][Outcomes]
Aval2015 = asset_vars(D,year=2015)[0][Outcomes]

#$\approx$ Creates Year dummies and baseline values as `var'2013
for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
    Aval["Year"]=Year
    for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
Vals = Vals.join(D[["UPG","CSH"]])
Vals["CTL"] = (Vals["UPG"]+Vals["CSH"] ==0).apply(int)

for group in ("UPG", "CSH"):
    for year in ("2013", "2014", "2015"):
        Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

#~ Make graph of distribution
stringify = lambda var: Vals[var].apply(lambda x: var if x else "")
Vals["Group"] = stringify("UPG")+stringify("CSH")+stringify("CTL")


amean = Vals.groupby(["Year","Group"]).mean()[["Total","Productive","Livestock"]]
aN = Vals.groupby(["Year","Group"]).count()[["Total","Productive","Livestock"]]
astd = Vals.groupby(["Year","Group"]).std()[["Total","Productive","Livestock"]]
ase = astd/np.sqrt(aN)
asset_pctchange = (amean/amean.ix[2013]).unstack("Year") - 1

for var in ("Total","Productive","Livestock"):
   #~ fig,ax = plt.subplots(1,2)
   #~ for i,yr in enumerate((2014,2015)):
   #~     Vals.ix[yr].dropna(subset=[[var,"UPG","CSH","CTL"]]).groupby("Group")[var].plot(kind="kde",ax=ax[i])
   #~     ax[i].set_title("{} Asset Value in {}".format(var,yr))
   #~     ax[i].legend()
   #~     #~ ax[i].set_aspect(1)
   #~     ax[i].set_xlim(left=0)
   #~ plt.savefig("../figures/Asset{}_kde.png".format(var))
   #~ plt.clf()
   amean.unstack("Group")["Total"].plot(kind="bar",yerr=ase.unstack("Group")["Total"].values)
   plt.tight_layout()
   plt.xticks(rotation=45)
   plt.savefig("../figures/Asset{}_groupyear.png".format(var))
   plt.clf()

if TGROUP_FE: Controls = ["2014","2015", 'UPG*2014', 'CSH*2014', 'UPG*2015', 'CSH*2015', 'UPG','CSH']
else:         Controls = ["2014","2015", 'UPG*2014', 'CSH*2014', 'UPG*2015', 'CSH*2015']


#$\approx$ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both UPG*2014 and UPG*2015
Vals=Vals.loc[2014:2015]
regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

CTL = Vals["UPG"]+Vals["CSH"] ==0
CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

for var in Outcomes:
    ttest1= regs[var].t_test("UPG*2014 - CSH*2015 = 0").summary_frame()
    ttest2= regs[var].t_test("UPG*2015 - CSH*2015 = 0").summary_frame()

    diff.loc[   r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    diff_se.loc[r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    diff.loc[   r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    diff_se.loc[r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


results = results.append(diff)
SE = SE.append(diff_se)

tab = df_to_orgtbl(results, sedf=SE)
#+end_src

#+name: assets_disaggreate_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
import numpy as np
import pandas as pd
import sys
sys.path.append("../../data")
from UPG import full_data, regressions, asset_vars, reg_table, df_to_orgtbl

D = full_data(balance=[])
D["cons"] = 1.
Count = D.filter(regex="^asset_n_").rename(columns=lambda col: col[8:])
Vals = D.filter(regex="^asset_val_").rename(columns=lambda col: col[10:])
#~ 1 if coded as >0, 0 if zero, keep NaN's missing, since those are only from missing surveys.
Nonzero = Count.applymap(lambda i: float(i>0) if not np.isnan(i) else np.nan)
#~ Restrict both df's to nonzero responses.
Count = Count.replace(0,np.nan)
Vals  =  Vals.replace(0,np.nan)
too_many_zeros = 30
many_zeros = [item for item in Vals if Vals[item].notnull().sum()<too_many_zeros]
for df in (Nonzero, Count, Vals): df.drop(many_zeros,1, inplace=True)

#~ Merge in Control Vars
controls = ["cons","UPG","UCT"]
Nonzero  = Nonzero.join(D[controls],how="left")
Count    =   Count.join(D[controls],how="left")
Vals     =    Vals.join(D[controls],how="left")

Items = [item[:-2] for item in Vals if item.endswith("_e")]
CTL = Vals[controls].sum(axis=1)==1 #~ i.e. only constant ==1, UPG & UCT ==0
Zeros_ctl_mean = Nonzero.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Count_ctl_mean =   Count.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])
Vals_ctl_mean  =    Vals.ix[CTL].filter(regex="_e$").mean(axis=0).rename(lambda col:col[:-2])

ZeroTable       = reg_table(regressions(Nonzero,"_e", outcomes=Items, controls = ["cons",'UCT','UPG'], Baseline="_b"), orgtbl=True, Transpose=True)
Count, Count_se = reg_table(regressions(Count,"_e",   outcomes=Items, controls = ["cons",'UCT','UPG'], Baseline="_b"), resultdf=True, Transpose=True)
Vals, Vals_se   = reg_table(regressions(Vals,"_e",    outcomes=Items, controls = ["cons",'UCT','UPG'], Baseline="_b"), resultdf=True, Transpose=True)

#~ Make full table of Standard errors-- MAKE SURE YOU HAVE THE SUFFIXES RIGHT.
SE = Count_se[["UPG","UCT"]].join(Vals_se[["UPG","UCT"]], rsuffix=" (SSP)", lsuffix=" (# own)")

#~ Make full table of point estimates and control-group means
FullTable = pd.DataFrame({"# own (CTL)":Count_ctl_mean, "Value (CTL)":Vals_ctl_mean}).join(Count[["UPG","UCT"]])
FullTable = FullTable.join(Vals[["UPG","UCT","N"]], rsuffix=" (SSP)", lsuffix=" (# own)")
FullTable = df_to_orgtbl(FullTable, sedf=SE)
AllTables = FullTable+"\n\n"+ZeroTable
return AllTables
#+end_src

** Savings

#+name: savings_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle ../analysis/savings_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
import sys
sys.path.append("../../data")
from UPG import full_data, regressions, asset_vars, reg_table , df_to_orgtbl

def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

D = full_data(balance=[])

Sav = pd.DataFrame(index=D.index) #~ Set up empty DataFrame to fill

years = [("_b",2013), ("_m",2014), ("_e", 2015)]
for suff,year in years: #~ Make Aggregate savings and land holding variables
    Sav["Savings{}".format(year)]  = D.filter(regex="^savings_(home|bank|BRAC|NGOs|other){}".format(suff)).sum(1)
    Sav["Food Sav{}".format(year)] = D.filter(regex="^savings_(maize|sorghum|otherfood)_val{}".format(suff)).sum(1)

    Sav["LandCult{}".format(year)] = D.filter(regex="^land_(owncult|rentcult|communitycult){}".format(suff)).sum(1)
    Sav["LandOwn{}".format(year)]  = D.filter(regex="^land_own.*{}".format(suff)).sum(1)

    Sav["Get Trans{}".format(year)]  = D.filter(regex="^transfers_get.*{}".format(suff)).sum(1)
    Sav["Give Trans{}".format(year)] = D.filter(regex="^transfers_give.*{}".format(suff)).sum(1)

Outcomes = ["Savings","Food Sav","LandCult","LandOwn", "Get Trans", "Give Trans"] #~ Loans give/received omitted

by_year = []
for yr in ("2013","2014","2015"): #~ Provide Baseline Values & put in long format
    S_Year = Sav.filter(like=yr).rename(columns=lambda x:x[:-4])
    for var in Outcomes: 
        if var+"2013" in Sav: S_Year[var+"2013"] = Sav[var+"2013"]
    #~ Note that adding "Year" has to come after topcode, which switches strings to Nan...
    S_Year["Year"] = yr
    by_year.append(S_Year)

#~ In long format with Year,HH index
Sav = pd.concat(by_year).reset_index().set_index(["Year", "HH"], drop=False)
#~ Make Year Dummies for fixed effects


Sav = Sav.join(pd.get_dummies(Sav["Year"]).rename(columns=lambda col: str(int(col))), how="left")
Sav = Sav.drop(["HH","Year"],1)
Sav = Sav.join(D[["UPG","CSH"]])

for group in ("UPG", "CSH"): #~ Make Treatment-by-year interactions
    for year in ("2013", "2014", "2015"):
        Sav["{}*{}".format(group,year)] = Sav[group]*Sav[year]

def isPositive(i):
    #~ Returns 1 if number is positive, 0 if number<=0, nan if already nan, and self if string.
    #~ Note that it's safe to run dummy variables through..
    try:
        if np.isnan(i): return i
        else: return float(i>0)
    except TypeError: return i

too_many_null = 30
Nonzero = Sav.applymap(isPositive)

#~ Save DataFrame with zeros
Savings = Sav.copy()
#~ Naturally, only do this after Creating Nonzero dataframe.
for var in Outcomes: #~ Set zeros to missing And topcode among non-zero values
    for outcome in (var,var+"2013"):
       if outcome in Sav:
           Sav[outcome] = Sav[outcome].replace(0,np.nan)
           Sav[outcome] = Sav.groupby(level="Year")[outcome].apply(topcode) #~ (Untested)

many_null = [item for item in Sav if Sav[item].count()<too_many_null]
many_null2 =[item for item in Savings if Savings[item].count()<too_many_null]
Sav = Sav.drop(many_null,1).copy()
Savings = Savings.drop(many_null,1).copy()

Controls = ['2014', '2015', 'UPG*2014', 'UPG*2015', 'CSH*2014', 'CSH*2015']

Sav = Sav.loc["2014":"2015"]
Savings = Savings.loc["2014":"2015"]
Nonzero = Nonzero.loc["2014":"2015"]
Sav_regs = regressions(Sav,     outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)
Zer_regs = regressions(Nonzero, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)
Amt_regs = regressions(Savings, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

Sav_results, Sav_SE  = reg_table(Sav_regs,  resultdf=True,table_info=["N","F-stat"])
Zer_results, Zer_SE  = reg_table(Zer_regs,  resultdf=True,table_info=["N","F-stat"])
Amt_results, Amt_SE  = reg_table(Amt_regs,  resultdf=True,table_info=["N","F-stat"])

CTL  = Sav["UPG"]+Sav["CSH"] ==0
CTL2 = Savings["UPG"]+Savings["CSH"] ==0
#~ Get control group means and standard deviations
Sav_CTLmean = {var: Sav[CTL].loc["2015",var].mean() for var in Outcomes}
Zer_CTLmean = {var: Nonzero[CTL].loc["2015",var].mean() for var in Outcomes}
Amt_CTLmean = {var: Savings[CTL2].loc["2015",var].mean() for var in Outcomes}

Sav_CTLsd = {var: Sav[CTL].loc["2015",var].std() for var in Outcomes}
Zer_CTLsd = {var: Nonzero[CTL].loc["2015",var].std() for var in Outcomes}
Amt_CTLsd = {var: Savings[CTL2].loc["2015",var].std() for var in Outcomes}

Sav_diff, Sav_diff_se = pd.DataFrame(Sav_CTLmean,index=["CTL mean"]), pd.DataFrame(Sav_CTLsd,index=["CTL mean"])
Zer_diff, Zer_diff_se = pd.DataFrame(Zer_CTLmean,index=["CTL mean"]), pd.DataFrame(Zer_CTLsd,index=["CTL mean"])
Amt_diff, Amt_diff_se = pd.DataFrame(Amt_CTLmean,index=["CTL mean"]), pd.DataFrame(Amt_CTLsd,index=["CTL mean"])

for var in Outcomes:
    #~ Savings regressions first
    ttest1= Sav_regs[var].t_test("UPG*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Sav_regs[var].t_test("UPG*2015 - CSH*2015 = 0").summary_frame()

    Sav_diff.loc[   r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Sav_diff_se.loc[r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Sav_diff.loc[   r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Sav_diff_se.loc[r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

    #~ Nonzero regressions second
    ttest1= Zer_regs[var].t_test("UPG*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Zer_regs[var].t_test("UPG*2015 - CSH*2015 = 0").summary_frame()

    Zer_diff.loc[   r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Zer_diff_se.loc[r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Zer_diff.loc[   r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Zer_diff_se.loc[r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

    #~ Savings regressions third
    ttest1= Amt_regs[var].t_test("UPG*2014 - CSH*2015 = 0").summary_frame()
    ttest2= Amt_regs[var].t_test("UPG*2015 - CSH*2015 = 0").summary_frame()

    Amt_diff.loc[   r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
    Amt_diff_se.loc[r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

    Amt_diff.loc[   r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
    Amt_diff_se.loc[r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]

Save_results = Sav_results.append(Sav_diff)
Zero_results = Zer_results.append(Zer_diff)
Amnt_results = Amt_results.append(Amt_diff)
Save_SE = Sav_SE.append(Sav_diff_se)
Zero_SE = Zer_SE.append(Zer_diff_se)
Amnt_SE = Amt_SE.append(Amt_diff_se)

#~ Land = ["LandCult","LandOwn"] 
#~ Savings = ["Savings","Food Sav", "Get Trans", "Give Trans"] 
#~ Land_results = Sav_results[Land]
#~ zLan_results = Zer_results[Land]
#~ Land_SE = Sav_SE[Land]
#~ zLan_SE = Zer_SE[Land]
#~ 
#~ Sav_results = Sav_results[Savings]
#~ Zer_results = Zer_results[Savings]
#~ Sav_SE =           Sav_SE[Savings]
#~ Zer_SE =           Zer_SE[Savings]

Save_tab = df_to_orgtbl(Save_results, sedf=Sav_SE)
Zero_tab = df_to_orgtbl(Zero_results, sedf=Zer_SE)
Amnt_tab = df_to_orgtbl(Amnt_results, sedf=Amt_SE)

Table = Zero_tab +"\n"+ Save_tab + "\n"+ Amnt_tab

#+end_src

** Income

#+name: income_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
from pandas.io import stata
import statsmodels.api as sm
from matplotlib import pyplot as plt
import sys
sys.path.append("../../data")
from UPG import full_data, regressions, reg_table, df_to_orgtbl
"""
Note that topcoding has a large effect on the distribution here, and we see only a small (presumably non-random) portion of actual income for each household.
"""

# Top-Code or censor outliers?
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

D = full_data(balance=[])
keep = D.index

I_file = '../../data/Endline/sections_8_17.csv'
I = pd.read_csv(I_file).rename(columns={"id":"HH"}).set_index("HH", drop=True).ix[keep]

#~Getting non-agriculture income data is easy
I = I.filter(regex="^s16")
Imonths    = I.filter(regex="s16_\dc").rename(columns=lambda x: x[:-1])
Ipermonth  = I.filter(regex="s16_\dd").rename(columns=lambda x: x[:-1])
Income_12m = Imonths.mul(Ipermonth).sum(axis=1)
Iyear      = I.filter(regex="s16_\de").rename(columns=lambda x: x[:-1]).sum(axis=1)

A_file = "../../data/Endline/Agriculture_cleaned.csv"
A = pd.read_csv(A_file).rename(columns={"id":"HH"}).set_index("HH",drop=False).ix[keep]
unit_prices = A.groupby(["harvest_type", "harvest_price_unit"])["harvest_price"].median()
prices = unit_prices.loc[zip(A["harvest_type"],A["harvest_price_unit"])]
A["price"]=list(prices)

A["harvest_unit_match"] = A["harvest_price_unit"] == A["harvest_unit"]
A["price"] = A["harvest_unit_match"]*A["harvest_price"] + (1-A["harvest_unit_match"])*A["price"]

A["income_farm_year"] = A["harvest_size"]*A["price"]
Ayear = A.groupby("HH")["income_farm_year"].sum()

unit_prices = A.groupby(["livestock_type", "livestock_price_unit"])["livestock_price"].median()
prices = unit_prices.loc[zip(A["livestock_type"],A["livestock_price_unit"])]
A["price"]=list(prices)
A["livestock_unit_match"] = A["livestock_price_unit"] == A["livestock_unit"]
A["price"] = A["livestock_unit_match"]*A["livestock_price"] + (1-A["livestock_unit_match"])*A["price"]

A["income_livestock_year"] = A["livestock_size"]*A["price"]
Lyear = A.groupby("HH")["income_livestock_year"].sum()

Outcomes = ["Total", "Non-Farm", "Farm",  "Livestock"]
Controls = ["cons", "UPG","CSH"]
Vals = pd.DataFrame({"Non-Farm": Income_12m, "Farm":Ayear, "Livestock":Lyear})
Vals = Vals.apply(topcode)

Vals["Total"] = Vals.sum(axis=1)
Vals["cons"] = 1.

Vals = Vals.join(D[["UPG","CSH"]],how="left")
Vals["CTL"] = (Vals["UPG"]+Vals["CSH"] ==0).apply(int)

#~ Make graph of distribution
stringify = lambda var: Vals[var].apply(lambda x: var if x else "")
Vals["Group"] = stringify("UPG")+stringify("CSH")+stringify("CTL")
Vals.dropna(subset=[["Total","UPG","CSH","CTL"]]).groupby("Group")["Total"].plot(kind="kde")
plt.title("Total Income Distribution by Group")
plt.savefig("../figures/IncomeDistribution.png")
plt.clf()
#~ Make bar graphs
Imean  = Vals.groupby("Group").mean()[Outcomes]
Icount = Vals.groupby("Group").count()[Outcomes]
Istd = Vals.groupby("Group").std()[Outcomes]
Ise=Istd/np.sqrt(Icount)
Imean.T.plot(kind="bar",yerr=Ise.T)
plt.tight_layout()
plt.xticks(rotation=45)
plt.savefig('../figures/Income_group.png')
plt.clf()

regs = {var: sm.OLS(Vals[var], Vals[Controls], missing="drop").fit() for var in Outcomes}
results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

CTL = Vals["CTL"] 
CTLmean = {var: Vals.query("CTL==1")[var].mean() for var in Outcomes}
CTLsd = {var: Vals.query("CTL==1").std() for var in Outcomes}
diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

for var in Outcomes:
    ttest1= regs[var].t_test("UPG - CSH = 0").summary_frame()

    diff.loc[   r"$\beta^{UPG}-\beta^{CSH}$", var] = ttest1["coef"][0]
    diff_se.loc[r"$\beta^{UPG}-\beta^{CSH}$", var] = ttest1["std err"][0]

results = results.append(diff)
SE = SE.append(diff_se)

tab = df_to_orgtbl(results, sedf=SE)

#+end_src

#+RESULTS: income_analysis
: None

** Conflict Exposure
#+name: conflict_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
import sys
sys.path.append("../../data")
from UPG import full_data, regressions, asset_vars, reg_table , df_to_orgtbl

# Top-Code or censor outliers?
def topcode(var, Nstd=3, drop=False):
    if drop: var[var>var.mean()+Nstd*var.std()] = np.nan
    else: var[var>var.mean()+Nstd*var.std()] = var.mean()+Nstd*var.std() 
    return var

#~ Read in data
D = full_data("../../data/UPG_full.dta", balance=[])
D = D[D.merge_midline != 1]
C = D.filter(like="conflict").rename(columns = lambda x: x[:-2]) #~ Set up empty DataFrame to fill

#~ Make Outcome variables
#~ NOTE: Not looking at whether they protected assets as only 50 #~ said they did, and all said "Migrated to stay with other family"
C["Bsln_NaN"] = D["merge_midline"] == 2
C["Worried"]  = C["conflict_worried"]
C["Affected"] = C["conflict_affected"]
protect_lives_codes = lambda x: {"Nothing": 0, "Migrate to stay with friend/family": 1, "Migrated and found new accommodation": 1,
                                 "Looked for protection with Govt. Military": 2, "Looked for protection with NGO": 2}.get(x)
C["ProtectLives"] = C["conflict_protectlives"].apply(protect_lives_codes)
C["Migrated"] = (C["ProtectLives"]==1) + \
                (C["conflict_affected1"]=="Needed to elocate or migrate")
C["NoMeans"] = C["conflict_whynotprotect"]=="Didn't have the means"
C["NoInvest"]= C.filter(like="affected").applymap(lambda x: x=="Could not plant crop or invest in business").sum(axis=1)
C = C.drop([var for var in C if var.startswith("conflict_")], 1)
Outcomes = ["ProtectLives", "Worried", "Affected", "Migrated", "NoMeans", "NoInvest"]

#~ Bring in Treatment variables
C["UPG"] = D["UPG"]
C["cons"] = 1.
C = C.applymap(lambda x: int(x) if not np.isnan(x) else x)
Controls = ["cons", "UPG"] #~, "Bsln_NaN"]

#~ Plot
byT=C.groupby("UPG")
Cbar=byT.mean().drop(["Bsln_NaN",'cons'],1)
Cstd=byT.std().drop(["Bsln_NaN",'cons'],1)
Cn=byT.count().drop(["Bsln_NaN",'cons'],1)
for df in (Cbar,Cstd,Cn): 
        df.index=["CTL","UPG"]
        df.index.name="Group"
Cse=Cstd/np.sqrt(Cn)
Cbar.T.plot(kind="bar",yerr=Cse.T)
plt.tight_layout()
plt.xticks(rotation=45)
plt.savefig("../figures/conflict_exposure.png")
plt.clf()


C_regs = regressions(C, outcomes=Outcomes, controls=Controls, Baseline=False, baseline_na=False)
C_results, C_SE  = reg_table(C_regs,  resultdf=True,table_info=["N","F-stat"])

#~ Get control group means and standard deviations and add to regression table
CTLmean = {var: C.query("UPG==0")[var].mean() for var in Outcomes}
CTLsdv  = {var: C.query("UPG==0")[var].std()  for var in Outcomes}
CTLmean, CTLsdv = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsdv,index=["CTL mean"])
C_results = C_results.append(CTLmean)
C_SE      = C_SE.append(CTLsdv)
C_results.drop('cons',inplace=True)
C_SE.drop('cons',inplace=True)

Table = df_to_orgtbl(C_results, sedf=C_SE)
return Table

#+end_src

** Confidence & Autonomy

#+name: decision_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 from UPG import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
 D = full_data(DIR = DATADIR,balance=[])

 Decide14 = D.filter(regex="^decide_.*_m").rename(columns=lambda x: x[:-2].split("_")[-1])#~.applymap(recode)
 Decide15 = D.filter(regex="^decide_.*_e").rename(columns=lambda x: x[:-2].split("_")[-1])#~.applymap(recode)

{'decide_arguments',
 'decide_fearful',

 'decide_pressuretospend'}

 who_decides = ['dailypurchase', 'familyvisits', 'healthchild', 'healthown', 'majorpurchase', 'moneyown']
 who_recode  = lambda x: float("Herself" in x) if pd.notnull(x) else x

 if WEEKLY:
     weekly = lambda i: float(i<3) if pd.notnull(i) else np.nan
     Aval2013 = Aval2013.applymap(weekly)
     Aval2014 = Aval2014.applymap(weekly)      
     Aval2015 = Aval2015.applymap(weekly)
 
 index_vars = "worried,portions,fewmeals,nofood,hungry,wholeday".split(",")
 Outcomes = index_vars+["z-score"]
 #~ Creates Year dummies, z-scores and baseline values as `var'2013
 for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
     Aval["Year"]=Year
     if not weekly:
        for var in index_vars:
            Aval[index_vars] = (Aval[index_vars]-Aval[index_vars].mean())/Aval[index_vars].std()
     FS_sum = Aval[index_vars].sum(axis=1)
     Aval["z-score"] = (FS_sum-FS_sum.mean())/FS_sum.std()
     for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
    
 Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
 Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
 Vals = Vals.join(D[["UPG","CSH"]])

 for group in ("UPG", "CSH"):
     for year in ("2013", "2014", "2015"):
         Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

 Controls = ['2014', '2015', 'UPG*2014', 'UPG*2015', 'CSH*2014', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both UPG*2014 and UPG*2015
 Vals=Vals.loc[2014:2015]
 regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = Vals["UPG"]+Vals["CSH"] ==0
 CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("UPG*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("UPG*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+end_src

#+name: confidence_analysis
#+begin_src python :dir ../analysis :noweb no-export :results values :exports none :tangle DevLunch_analysis.py
 import numpy as np
 import pandas as pd
 import statsmodels.api as sm
 import sys
 DATADIR = "../../data/"
 sys.path.append(DATADIR)
 from UPG import full_data, regressions, asset_vars, reg_table , df_to_orgtbl
 D = full_data(DIR = DATADIR,balance=[])

 autonomy_codes = {'go_NGO',
                   'go_church',
                   'go_healthcenter',
                   'go_housenonrelative',
                   'go_houserelative',
                   'go_market',
                   'go_school',
                   'go_water'}

 codes = {"1-2 times a week": 3,
         "3-6 times a week": 2,
         "Everyday": 1,
         "everyday": 1,
         "Less than once a week": 4,
         "less than once a week": 4,
         "Never": 5,
         "never": 5}

 recode = lambda x: codes.setdefault(x,x)

 Autnmy14 = D.filter(regex="^cango_.*_m").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Autnmy15 = D.filter(regex="^cango_.*_e").rename(columns=lambda x: x[3:-2]).applymap(recode)

{'decide_arguments',
 'decide_fearful',

 'decide_dailypurchase',
 'decide_familyvisits',
 'decide_healthchild',
 'decide_healthown',
 'decide_majorpurchase',
 'decide_moneyown',
 'decide_pressuretospend'}

 Decide14 = D.filter(regex="^decide_.*_m").rename(columns=lambda x: x[:-2])#~.applymap(recode)
 Decide15 = D.filter(regex="^decide_.*_e").rename(columns=lambda x: x[:-2])#~.applymap(recode)

 Confid14 = D.filter(regex="^conf_.*_m").rename(columns=lambda x: x[3:-2]).applymap(recode)
 Confid15 = D.filter(regex="^conf_.*_e").rename(columns=lambda x: x[3:-2]).applymap(recode)

 if WEEKLY:
     weekly = lambda i: float(i<3) if pd.notnull(i) else np.nan
     Aval2013 = Aval2013.applymap(weekly)
     Aval2014 = Aval2014.applymap(weekly)      
     Aval2015 = Aval2015.applymap(weekly)
 
 index_vars = "worried,portions,fewmeals,nofood,hungry,wholeday".split(",")
 Outcomes = index_vars+["z-score"]
 #~ Creates Year dummies, z-scores and baseline values as `var'2013
 for Year, Aval in zip((2013, 2014, 2015), (Aval2013, Aval2014, Aval2015)):
     Aval["Year"]=Year
     if not weekly:
        for var in index_vars:
            Aval[index_vars] = (Aval[index_vars]-Aval[index_vars].mean())/Aval[index_vars].std()
     FS_sum = Aval[index_vars].sum(axis=1)
     Aval["z-score"] = (FS_sum-FS_sum.mean())/FS_sum.std()
     for var in Outcomes: Aval[var+"2013"] = Aval2013[var]
    
 Vals = pd.concat((Aval2013, Aval2014, Aval2015)).reset_index().set_index(["Year", "HH"], drop=False)
 Vals = Vals.join(pd.get_dummies(Vals["Year"]).rename(columns=lambda col: str(int(col))), how="left")
 Vals = Vals.join(D[["UPG","CSH"]])

 for group in ("UPG", "CSH"):
     for year in ("2013", "2014", "2015"):
         Vals["{}*{}".format(group,year)] = Vals[group]*Vals[year]

 Controls = ['2014', '2015', 'UPG*2014', 'UPG*2015', 'CSH*2014', 'CSH*2015']

 #~ This is the main specification. Given the mismatch in timing, we compare CSH*2015 to both UPG*2014 and UPG*2015
 Vals=Vals.loc[2014:2015]
 regs = regressions(Vals, outcomes=Outcomes, controls=Controls, Baseline=2013, baseline_na=True)

 results, SE  = reg_table(regs,  resultdf=True,table_info=["N","F-stat"])

 CTL = Vals["UPG"]+Vals["CSH"] ==0
 CTLmean = {var: Vals[CTL].loc[2015,var].mean() for var in Outcomes}
 CTLsd = {var: Vals[CTL].loc[2015,var].std() for var in Outcomes}
 diff, diff_se = pd.DataFrame(CTLmean,index=["CTL mean"]), pd.DataFrame(CTLsd,index=["CTL mean"])

 for var in Outcomes:
     ttest1= regs[var].t_test("UPG*2014 - CSH*2015 = 0").summary_frame()
     ttest2= regs[var].t_test("UPG*2015 - CSH*2015 = 0").summary_frame()

     diff.loc[   r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["coef"][0]
     diff_se.loc[r"$\beta^{UPG}_{2014}-\beta^{CSH}$", var] = ttest1["std err"][0]

     diff.loc[   r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["coef"][0]
     diff_se.loc[r"$\beta^{UPG}_{2015}-\beta^{CSH}$", var] = ttest2["std err"][0]


 results = results.append(diff)
 SE = SE.append(diff_se)

 tab = df_to_orgtbl(results, sedf=SE)
 #+end_src

** Extras
#+name: get_loglambdas
#+BEGIN_SRC python :noweb no-export :results silent

"""
This code does not yet work in this file. Come back to it...
"""
  df = UPG.process_data(C, HH, T, year = year) #~ Process_data() builds consumption data if not given as an argument
  df['Constant']=1
  df["CTL"] = 1-df["UPG"] #~ Code the cash group as controls since they're not in the midline analysis

  explist=[s[2:-2] for s in df.columns[[s.startswith('c_') and s.endswith(year[1]) for s in df.columns]]]
  df = df.rename(columns= lambda x: x[:-2] if x.endswith(year[1]) else x)

  bothdf=[]
  xvars=['hh_size_b','child_total_b','Loc']
  for x in explist:
      if 'c_'+x+r'_b' not in df:
          #~ When you take out the baseline controls in favor of repeated cross-sections, this is where to start...
          print(x+" has no baseline data or had too few non-zero responses at baseline. Skipping.")
          continue
      ydf=pd.DataFrame(df[['c_'+x]].rename(columns={'c_'+x:x.capitalize()}).stack())
      rdict=dict(zip(xvars+['c_'+x+r'_b'],["%s_%s" % (s,x.capitalize()) for s in xvars]+['Baseline_%s' % x.capitalize()]))
      xdf=pd.DataFrame(df[xvars+['c_'+x+r'_b']])
      xdf.index=pd.MultiIndex.from_tuples([(i,x.capitalize()) for i in xdf.index])
      locations=pd.get_dummies(xdf['Loc'],prefix='Loc_%s' % x.capitalize())
      del xdf['Loc']
      xdf.rename(columns=rdict,inplace=True)
      xdf=xdf.join(locations)
      xdf.replace(to_replace=np.NaN,value=0,inplace=True)

      # Add row to restrict location dummies to sum to one
      ydf=pd.concat([ydf,pd.DataFrame([0],index=[(0,x.capitalize())])])
      xdf=pd.concat([xdf,pd.DataFrame([s.startswith('Loc_')+0. for s in xdf.columns],index=xdf.columns,columns=[(0,x.capitalize())]).T]) 

      xdf[0]=ydf
      xdf.dropna(how='any',inplace=True)
      bothdf.append(xdf)

  #~ Are this fillna() call and the xdf.replace call above a problem? It seems necessary for the block-diagonal ols function
  #~ we're using, but aren't we coding zeros as missing and calculating residuals for only those positive consumption? Wouldn't
  #~ replacing them to zero insert some non-zero residual for households that never consume a given good?
  #~ And isn't this the motivation behind svd_missing?
  mydf=pd.concat(bothdf).fillna(value=0)

  X=mydf.iloc[:,1:]

  y=mydf[[0]]

  x=np.exp(y.unstack().iloc[1:,:]) # Expenditures (in levels)
  xshares=x.divide(x.sum(axis=1),axis=0).fillna(value=0).mean() # Expenditure shares (taking missing as zero)
  xshares.index=xshares.index.droplevel(0)

  b,se=ols(X,y)

  ## betahat=b[['Constant_%s' % s.capitalize() for s in explist]]
  ## betahat.rename(columns=dict(zip(betahat.columns,[s.capitalize() for s in explist])),inplace=True)

  e=y-X.dot(b.T)

  e.rename(columns={0:'Resid'},inplace=True)
  e.index.names=['HH','Good']

  testdf=pd.merge(df[['UPG','CTL']].reset_index(),e.reset_index(),how='outer',on=['HH'])
  testdf.set_index(['HH','Good'],inplace=True)

  UPG=testdf['UPG'].mul(testdf['Resid']).dropna().unstack()
  CTL=testdf['CTL'].mul(testdf['Resid']).dropna().unstack()

  e=(e-e.mean()).unstack()

  # Test of significant differences between treatment and control:
  # Weighting matrix:
  A=np.matrix((UPG-CTL).cov().as_matrix()).I
  g=np.matrix((UPG-CTL).mean())
  J=e.shape[0]*g*A*g.T # Chi2 statistic

  p=1-chi2.cdf(J,e.shape[1])

  chi2test="Chi2 test: %f (%f)" % (J,p)

  N=pd.Series([d.shape[0]-1 for d in bothdf],index=[d.index.levels[1][0] for d in bothdf])

  resultdf=pd.DataFrame({'UPG':UPG.mean(),'CTL':CTL.mean(),'$N$':N})
  sedf=pd.DataFrame({'UPG':UPG.std()/np.sqrt(resultdf['$N$']),'CTL':CTL.std()/np.sqrt(resultdf['$N$'])})
  resultdf['Diff.']=resultdf['UPG']-resultdf['CTL']
  sedf['Diff.']=np.sqrt((sedf['UPG']**2) + (sedf['CTL']**2))

  # Use svd (with missing data) to construct beta & log lambda

  myb,myl = get_loglambdas(e,TEST=True)

  myb.index=myb.index.droplevel(0)

  # Normalize log lambdas
  l=myl/myl.std()
#+END_SRC

#+name: residuals_by_group
#+begin_src python :dir ../analysis :noweb no-export :results values  :exports none
def residuals_by_group(models, groups, outcomes=[], kind="kde", figure_dir="../figures/", seriesname="Treat", blanks_to = "Control"):
    """
     Takes a set of statsmodels regression results and,
     for each outcome, produces a plot comparing the
     distribution of residuals by group.

     models:
         A dictionary of the form {variable name: sm.RegressionResults}. Empty defaults to all available.
     groups:
         A list, pd.Series, or pd.DataFrame with variables
         (A later version could contain an arbitrary set of categorical and give groups for every combination.)
     outcomes:
         A list specifying which variables in models to make plots for.
     kind:
         kde (or density) or histogram (or hist)
         Density plots are on a single axis. Histograms are stacked by group.
     figure_dir:
         The directory into which the figures get saved. If doesn't exist, throws error (future version might make that directory on the fly.)
     Seriesname:
         If a series or list is passed without a name, defaults to `seriesname'
     blanks_to:
         Observations with no treatment status from "groups" gets renamed to `blanks_to'
    """

    #~ Make outcomes a list. If empty, defaults to all variables in models
    if type(outcomes)==str: outcomes=[outcomes]
    if not outcomes: outcomes = sorted(models.keys())

    #~ Make data frame and make "Group" categorical
    df = pd.DataFrame(groups).rename(columns={0:seriesname})
    for var in df: df[var] = df[var].applymap(lambda x: var if x else "")
    df["Group"] = df.sum(axis=1).replace("",blanks_to)

    #~ Make residuals
    for var in outcomes:
        #~ Make column of residual values
        resid_var = "resid_{}".format(var)
        df[resid_var] = models[var].resid
        #~ Groupby object
        groups = df.dropna(subset=[resid_var]).groupby("Group")[resid_var]

        #~ Plot density by group
        if kind in ("kde", "density"):
            fig, ax = plt.subplots()
            groups.plot(kind=kind, ax=ax, legend=True)
            fig.savefig(figure_dir+resid_var+".png")

        #~ Plot histograms by group
        elif kind in ("hist", "histogram"):
            i=0
            fig, ax = plt.subplots(len(set(df["Group"])),1,sharex=True)
            for group, data in grps[var]:
                ax[i].hist(data.values, bins=20)
                ax[i].set_title(group)
                i+=1
            i=0
            fig.savefig(figure_dir+resid_var+".png")
        print(resid_var+".png created.")


#+end_src
   



 



* Omnibus Analysis                                                 :noexport:
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-06-15 Tue 17:04
  :ARCHIVE_FILE: ~/Research/SS-TUP/Report/documents/EndlineReport.org
  :ARCHIVE_CATEGORY: EndlineReport
  :END:

Here we consider the most straight-forward possible comparison of
programs.  Instead of the aggregation which characterizes many of our
main results, we collect a list of all the primitive variables
collected in our surveys for which we'd expect simply estimate conduct a POST analysis on /all/ of the
outcomes which inform those aggregations, without any transformation
or modification.
#+begin_src python :results output raw table :tangle /tmp/post.py
import ss_standard
import pandas as pd
from cfe.df_utils import drop_missing, df_to_orgtbl
import statsmodels.api as sm
import numpy as np
from  scipy import stats

prettylabels = {'meat': ' Meat (last three days)', 'fish': 'Fish (last three days)', 
                'cereals': 'Cereals (last three days)', 'sugar': 'Sugar (last three days)', 
                'egg': 'Egg (last three days)', 'oil': 'Oil (last three days)', 
                'beans': 'Beans (last three days)', 'fruit': 'Fruit (last three days)', 
                'salt': 'Salt (last three days)', 'vegetables': 'Vegetables (last three days)', 
                'milk': 'Milk (last three days)', 'spices': 'Spices (last three days)', 
                'alcohol': 'Alcohol (last three days)', 'otherfood': 'Other food (last three days)', 
                'fuel': 'Cooking fuel (last month)', 'soap': 'Toiletries (last month)', 
                'transport': 'Transportation (last month)', 'cosmetics': 'Cosmetics (last month)', 
                'entertainment': 'Entertainment (last month)', 'clothesfootwear': 'Clothing, footwear (last year)', 
                'charities': 'Charities (last year)', 'ceremonies': 'Rituals/ceremonies (last year)', 
                'textiles': 'Textiles (last year)', 'utensils': 'Utensils (last year)',
                'dowry': 'Dowry (last year)', 'furniture': 'Furniture (last year)', 
                'other': 'Other non-food (last year)', 'cows': 'Large livestock (cows)', 
                'smallanimals': 'Small livestock', 'bicycle': 'Bicycle', 
                'radio': 'Radio', 'motorcycle': 'Motorcycle', 
                'net': 'Mosquito net', 'poultry': 'Poultry', 
                'bed': 'Bed', 'chairtables': 'Chair/table', 
                'mobile': 'Mobile phone', 'netITN': 'Mosquito net ITN', 
                'in_business': 'In business', 
                'child_total': '# Child', 'hh_size':'Household size'}

assets = ss_standard.basic_data(outcomes=None,filter='^asset_val_')
assets = assets.rename(columns={'poultry':'Poultry (birds)'})

savings = ss_standard.basic_data(outcomes=['savings_home', 'savings_bank', 'savings_BRAC', 'savings_NGOs', 
                                           'savings_other',  'transfers_give1', 'transfers_give2', 'transfers_give3',
                                           'savings_maize_val', 'savings_sorghum_val', 's8b_2c', 's8b_2d',
                                           's4b_oth_1b', 's4b_oth_2b','savings_otherfood1_val','savings_otherfood2_val'])


savings.rename({'s8b_2c': 'savings_otherfood1_val',
                's8b_2d': 'savings_otherfood2_val',
                's4b_oth_1b': 'savings_otherfood1_val',
                's4b_oth_2b': 'savings_otherfood2_val'}, axis=1, inplace=True)

savings = savings.groupby(level=0,axis=1).sum()

land = ss_standard.basic_data(outcomes=['land_owncult', 'land_rentcult', 'land_communitycult',
                                        'land_ownnocult', 'land_ownrent'])


# Issue that units are supposed to be fedan (4200 m^2), but are sometimes instead expressed in square meters (20mx20m plots)
for v in ['land_owncult', 'land_rentcult', 'land_communitycult', 'land_ownnocult', 'land_ownrent']:
        land[v].loc[land[v]>20] = land[v].loc[land[v]>20]/4200

consumption = ss_standard.basic_data(filter = r'^c_')

fs = ss_standard.basic_data(filter = r'^fs_')

# replace words with estimate of days per month not hungry
fs.replace({"Everyday": 0,
            "everyday": 0,
            "3-6 times a week": 28-4.5*4,
            "1-2 times a week": 28-1.5*4,
            "Less than once a week": 28-4,
            "less than once a week": 28-4,
            "Never": 28,
            "never": 28}, inplace=True)


outcomes = {'worried':'Worried', 
            'notpreferred':'No preferred',
            'fewkinds':'Few kinds',
            'preferrednot':'Ate unpreferred',
            'portions':'Limited portions',
            'fewmeals':'Fewer Meals', 
            'nofood':'No food in house', 
            'hungry':'Went hungry', 
            'wholeday':'Whole day without eating'}

fs = fs.rename(columns=outcomes)

from income import farm_yearly, nonfarm_yearly

nonfarm_yearly['Year'] = '2015'
nonfarm_yearly = nonfarm_yearly.groupby(['idno','Year']).sum()


farm_yearly['Year'] = '2015'
farm_yearly = farm_yearly.groupby(['idno','Year']).sum()


df = pd.concat({'Farm Income':farm_yearly,
                'Nonfarm Income':nonfarm_yearly,
                'Food Security':fs,
                'Consumption':consumption,
                'Land':land,
                'Savings':savings,
                'Physical Assets':assets},axis=1)

df = pd.concat([farm_yearly,
                nonfarm_yearly,
                fs,
                consumption,
                land,
                savings,
                assets],axis=1).rename(columns=prettylabels)

df = df.loc[:,~df.columns.duplicated(keep='first')]

group = df['Group']
groups = pd.get_dummies(group)
# Drop Group vars
df = df.loc[:,[s for s in df.columns if 'Group' not in s]]

df = df.loc[:,df.std()>0]
outcomes = df.columns.tolist()

#df = pd.concat([df,groups],axis=1)
df['Group'] = group  # Ancova
df = ss_standard.ancova_form(df,outcomes)


Residuals = ss_standard.residuals(df,outcomes)
Residuals = Residuals.join(group).reset_index().set_index(['idno','Year','Group'])
Residuals.to_pickle('/tmp/residuals.pickle')

Results = {}

Results['Values (SSP)'] = ss_standard.results(df,outcomes,nonzero=False,logs=False,return_stats=True)
Results['Nonzero'] = ss_standard.results(df,outcomes,nonzero=True,logs=False,return_stats=True)
Results['Log Values'] = ss_standard.results(df,outcomes,nonzero=False,logs=True,return_stats=True)

other = pd.DataFrame({k:r[2].stack() for k,r in Results.items()})
other.index.names = ['Variable','Outcome']
other.columns.name = 'Class'
other = other.stack().unstack('Variable').reorder_levels(['Class','Outcome']).sort_index()
other = other.reindex(['Values (SSP)','Nonzero','Log Values'],level=0)

# binomial test
ptab = {}
countsplus = {}
countsminus = {}
for sort in Results.keys():
    plus = (Results[sort][0].T>0).sum()
    minus = (Results[sort][0].T<0).sum()
    countsplus[sort] = plus
    countsminus[sort] = minus
    plus = plus.to_dict()
    minus = minus.to_dict()
    ptab[sort] = {k:stats.binom_test((plus[k],minus[k]),alternative='greater') for k in plus.keys() if r'$\beta' in k}

countsplus = pd.DataFrame(countsplus).T.filter(like='beta')
countsminus = pd.DataFrame(countsminus).T.filter(like='beta')

ptab = pd.DataFrame(ptab).T

print(df_to_orgtbl(np.round(ptab,decimals=3).astype(str),
                   conf_ints=(countsplus.astype(str),countsminus.astype(str)),float_fmt=r'%s'))

print(ss_standard.table_stacked_by_class(Results,elide=True))

#print(ss_standard.table_stacked_by_class(Results,elide=False))
#+end_src 

#+name: tab:post
#+caption: POST analysis of all variables
#+attr_latex: :environment longtable :align rrrrr




* Summary Index Tests                                              :noexport:
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-06-15 Tue 17:04
  :ARCHIVE_FILE: ~/Research/SS-TUP/Report/documents/EndlineReport.org
  :ARCHIVE_CATEGORY: EndlineReport
  :END:

This section explores an idea proposed by Anderson (2008), which
involves normalizing all outcomes and weighting them in groups by
their inverse covariance matrix, then using means of these weighted normalized
observations to estimate treatment effects, testing significance with
a standard $t$ test.  This seems very close to the obvious GMM test of
$\E( y| T) = E(y)$.  

In the spirit of the last, we should also be able to estimate the
treatment effects in a system, then use the estimated covariance
matrix of the coefficients to construct a classical $F$ test of the
hypothesis that all treatment effects are zero, which we implement below.

#+begin_src python :results output raw table :tangle /tmp/summary_index.py
import ss_standard
import pandas as pd
from cfe.df_utils import drop_missing, df_to_orgtbl
import statsmodels.api as sm
import numpy as np
from  scipy import stats

prettylabels = {'meat': ' Meat (last three days)', 'fish': 'Fish (last three days)', 
                'cereals': 'Cereals (last three days)', 'sugar': 'Sugar (last three days)', 
                'egg': 'Egg (last three days)', 'oil': 'Oil (last three days)', 
                'beans': 'Beans (last three days)', 'fruit': 'Fruit (last three days)', 
                'salt': 'Salt (last three days)', 'vegetables': 'Vegetables (last three days)', 
                'milk': 'Milk (last three days)', 'spices': 'Spices (last three days)', 
                'alcohol': 'Alcohol (last three days)', 'otherfood': 'Other food (last three days)', 
                'fuel': 'Cooking fuel (last month)', 'soap': 'Toiletries (last month)', 
                'transport': 'Transportation (last month)', 'cosmetics': 'Cosmetics (last month)', 
                'entertainment': 'Entertainment (last month)', 'clothesfootwear': 'Clothing, footwear (last year)', 
                'charities': 'Charities (last year)', 'ceremonies': 'Rituals/ceremonies (last year)', 
                'textiles': 'Textiles (last year)', 'utensils': 'Utensils (last year)',
                'dowry': 'Dowry (last year)', 'furniture': 'Furniture (last year)', 
                'other': 'Other non-food (last year)', 'cows': 'Large livestock (cows)', 
                'smallanimals': 'Small livestock', 'bicycle': 'Bicycle', 
                'radio': 'Radio', 'motorcycle': 'Motorcycle', 
                'net': 'Mosquito net', 'poultry': 'Poultry', 
                'bed': 'Bed', 'chairtables': 'Chair/table', 
                'mobile': 'Mobile phone', 'netITN': 'Mosquito net ITN', 
                'in_business': 'In business', 
                'child_total': '# Child', 'hh_size':'Household size'}

assets = ss_standard.basic_data(outcomes=None,filter='^asset_val_')
assets = assets.rename(columns={'poultry':'Poultry (birds)'})

savings = ss_standard.basic_data(outcomes=['savings_home', 'savings_bank', 'savings_BRAC', 'savings_NGOs', 
                                           'savings_other',  'transfers_give1', 'transfers_give2', 'transfers_give3',
                                           'savings_maize_val', 'savings_sorghum_val', 's8b_2c', 's8b_2d',
                                           's4b_oth_1b', 's4b_oth_2b','savings_otherfood1_val','savings_otherfood2_val'])


savings.rename({'s8b_2c': 'savings_otherfood1_val',
                's8b_2d': 'savings_otherfood2_val',
                's4b_oth_1b': 'savings_otherfood1_val',
                's4b_oth_2b': 'savings_otherfood2_val'}, axis=1, inplace=True)

savings = savings.groupby(level=0,axis=1).sum()

land = ss_standard.basic_data(outcomes=['land_owncult', 'land_rentcult', 'land_communitycult',
                                        'land_ownnocult', 'land_ownrent'])


# Issue that units are supposed to be fedan (4200 m^2), but are sometimes instead expressed in square meters (20mx20m plots)
for v in ['land_owncult', 'land_rentcult', 'land_communitycult', 'land_ownnocult', 'land_ownrent']:
        land[v].loc[land[v]>20] = land[v].loc[land[v]>20]/4200

consumption = ss_standard.basic_data(filter = r'^c_')

fs = ss_standard.basic_data(filter = r'^fs_')

# replace words with estimate of days per month not hungry
fs.replace({"Everyday": 0,
            "everyday": 0,
            "3-6 times a week": 28-4.5*4,
            "1-2 times a week": 28-1.5*4,
            "Less than once a week": 28-4,
            "less than once a week": 28-4,
            "Never": 28,
            "never": 28}, inplace=True)


outcomes = {'worried':'Worried', 
            'notpreferred':'No preferred',
            'fewkinds':'Few kinds',
            'preferrednot':'Ate unpreferred',
            'portions':'Limited portions',
            'fewmeals':'Fewer Meals', 
            'nofood':'No food in house', 
            'hungry':'Went hungry', 
            'wholeday':'Whole day without eating'}

fs = fs.rename(columns=outcomes)

from income import farm_yearly, nonfarm_yearly

nonfarm_yearly['Year'] = '2015'
nonfarm_yearly = nonfarm_yearly.groupby(['idno','Year']).sum()


farm_yearly['Year'] = '2015'
farm_yearly = farm_yearly.groupby(['idno','Year']).sum()


df = pd.concat({'Farm Income':farm_yearly,
                'Nonfarm Income':nonfarm_yearly,
                'Food Security':fs,
                'Consumption':consumption,
                'Land':land,
                'Savings':savings,
                'Physical Assets':assets},axis=1)

df = pd.concat([farm_yearly,
                nonfarm_yearly,
                fs,
                consumption,
                land,
                savings,
                assets],axis=1).rename(columns=prettylabels)

df = df.loc[:,~df.columns.duplicated(keep='first')]

group = df['Group']
groups = pd.get_dummies(group)
# Drop Group vars
df = df.loc[:,[s for s in df.columns if 'Group' not in s]]

df = df.loc[:,df.std()>0]
outcomes = df.columns.tolist()

#df = pd.concat([df,groups],axis=1)
df['Group'] = group  # Ancova

df.to_pickle('/tmp/mydf.pickle')

df = ss_standard.ancova_form(df,outcomes)

b,Vb = ss_standard.system_estimator(df,outcomes)

#+end_src
#+begin_src python :results output raw table :tangle /tmp/summary_index.py
import ss_standard
import pandas as pd
from cfe.df_utils import drop_missing, df_to_orgtbl
import statsmodels.api as sm
import numpy as np
from  scipy import stats
from metrics_miscellany import tests, utils, estimators

df = pd.read_pickle('/tmp/mydf.pickle')
outcomes = df.columns.tolist()
outcomes.remove('Group')

df = ss_standard.ancova_form(df,outcomes)

Xbar,Ybar = ss_standard.system_data(df,outcomes)

b,Vb = estimators.ols(Xbar,Ybar,cov_type='HC3',PSD_COV=1e-2)

#b0,Vb0 = ss_standard.system_estimation(Xbar,Ybar)


# Test!

p = []
chi2s = []
ortho_t = []
for i in range(1000):
    t = tests.chi2_test("Variable in ['UCT*2015']",b,Vb,TEST=True)
    p.append(t[1])
    chi2s.append(t[0])
    ortho_t.append(t[-1])

#+end_src


* Rank tests                                                       :noexport:
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-06-15 Tue 17:04
  :ARCHIVE_FILE: ~/Research/SS-TUP/Report/documents/EndlineReport.org
  :ARCHIVE_CATEGORY: EndlineReport
  :END:
#+begin_src python :results output raw table :tangle /tmp/rank_tests.py
import pandas as pd
from scipy import stats
import numpy as np
from collections import defaultdict
from cfe.df_utils import df_to_orgtbl

e = pd.read_pickle('/tmp/residuals.pickle')

e = e.reset_index()
e['Group'] = e['Group'].fillna('Control')

# Toss UCT into control group for 2014
e['Group'] = e.apply(lambda x: 'Control' if ((x['Group']=='UCT') & (x['Year']=='2014')) else x['Group'], axis=1)
e = e.set_index(['idno','Year','Group'])

Comparisons = {(('TUP','2014'),('Control','2014')):r'$U^{2014}_{UGP}$',
               (('TUP','2015'),('Control','2015')):r'$U^{2015}_{UGP}$',
               (('UCT','2015'),('Control','2015')):r'$U^{2015}_{UCT}$',
               (('TUP','2014'),('UCT','2015')):r'$U^{2014,2015}_{UGP,UCT}$',
               (('TUP','2015'),('UCT','2015')):r'$U^{2015,2015}_{UGP,UCT}$'}

Tests = defaultdict(dict)
for outcome in e.columns.tolist():
    x = e[[outcome]]
    S = {}
    for group in set(e.index.get_level_values('Group')):
        for year in set(e.index.get_level_values('Year')):
            group_year = x.query("Year=='%s' and Group=='%s'" % (year,group)).squeeze()
            if len(group_year):
                S[(group,year)] = group_year
    
    for test in Comparisons:
        try:
            Tests[Comparisons[test]][outcome] = stats.mannwhitneyu(S[test[0]],S[test[1]]).pvalue
        except (KeyError,ValueError):
            Tests[Comparisons[test]][outcome] = np.nan

Tests = pd.DataFrame(Tests)

print(df_to_orgtbl(Tests,float_fmt='%5.3f'))
    
#+end_src

#+results:
|                                | $U^{2015}_{UGP}$ | $U^{2015}_{UCT}$ | $U^{2014,2015}_{UGP,UCT}$ | $U^{2015,2015}_{UGP,UCT}$ |
|--------------------------------+------------------+------------------+---------------------------+---------------------------|
| farm_income_yearly_sum         |            0.048 |            0.308 |                     0.000 |                     0.102 |
| livestock_income_yearly_sum    |            0.000 |            0.041 |                     0.000 |                     0.006 |
| source1                        |            0.385 |            0.468 |                     0.000 |                     0.423 |
| source2                        |            0.163 |            0.327 |                     0.000 |                     0.134 |
| source3                        |            0.039 |            0.219 |                     0.000 |                     0.027 |
| source4                        |            0.110 |            0.244 |                     0.000 |                     0.058 |
| source5                        |            0.192 |            0.115 |                     0.000 |                     0.046 |
| Worried                        |            0.226 |            0.165 |                     0.225 |                     0.073 |
| No preferred                   |            0.441 |            0.021 |                     0.031 |                     0.019 |
| Few kinds                      |            0.135 |            0.084 |                     0.003 |                     0.310 |
| Ate unpreferred                |            0.381 |            0.040 |                     0.007 |                     0.073 |
| Limited portions               |            0.395 |            0.401 |                     0.000 |                     0.497 |
| Fewer Meals                    |            0.012 |            0.228 |                     0.000 |                     0.154 |
| No food in house               |            0.037 |            0.033 |                     0.003 |                     0.370 |
| Went hungry                    |            0.035 |            0.100 |                     0.018 |                     0.393 |
| Whole day without eating       |            0.302 |            0.254 |                     0.019 |                     0.385 |
| Cereals (last three days)      |            0.047 |            0.401 |                     0.014 |                     0.063 |
| Beans (last three days)        |            0.218 |            0.429 |                     0.389 |                     0.357 |
| Oil (last three days)          |            0.139 |            0.274 |                     0.348 |                     0.058 |
| Salt (last three days)         |            0.020 |            0.027 |                     0.450 |                     0.415 |
| Sugar (last three days)        |            0.023 |            0.110 |                     0.019 |                     0.388 |
| Meat (last three days)         |            0.014 |            0.126 |                     0.000 |                     0.255 |
| Fish (last three days)         |            0.000 |            0.089 |                     0.002 |                     0.109 |
| Egg (last three days)          |            0.186 |            0.473 |                     0.000 |                     0.142 |
| Milk (last three days)         |            0.032 |            0.113 |                     0.000 |                     0.227 |
| Vegetables (last three days)   |            0.358 |            0.268 |                     0.043 |                     0.224 |
| Fruit (last three days)        |            0.330 |            0.305 |                     0.000 |                     0.165 |
| Spices (last three days)       |            0.453 |            0.078 |                     0.000 |                     0.061 |
| Alcohol (last three days)      |            0.000 |            0.000 |                     0.000 |                     0.096 |
| Other food (last three days)   |            0.036 |            0.137 |                     0.000 |                     0.373 |
| Cooking fuel (last month)      |            0.098 |            0.053 |                     0.381 |                     0.260 |
| Cosmetics (last month)         |            0.000 |            0.007 |                     0.001 |                     0.375 |
| Toiletries (last month)        |            0.220 |            0.008 |                     0.000 |                     0.044 |
| Transportation (last month)    |            0.243 |            0.355 |                     0.000 |                     0.180 |
| Entertainment (last month)     |            0.021 |            0.079 |                     0.000 |                     0.382 |
| Clothing, footwear (last year) |            0.011 |            0.089 |                     0.000 |                     0.373 |
| Utensils (last year)           |            0.000 |            0.004 |                     0.000 |                     0.130 |
| Furniture (last year)          |            0.000 |            0.000 |                     0.000 |                     0.043 |
| Textiles (last year)           |            0.488 |            0.103 |                     0.038 |                     0.084 |
| Rituals/ceremonies (last year) |            0.000 |            0.000 |                     0.000 |                     0.460 |
| Charities (last year)          |            0.000 |            0.000 |                     0.000 |                     0.000 |
| Dowry (last year)              |            0.003 |            0.096 |                     0.000 |                     0.231 |
| Other non-food (last year)     |            0.000 |            0.001 |                     0.000 |                     0.210 |
| maize                          |            0.052 |            0.092 |                     0.000 |                     0.469 |
| sorghum                        |            0.026 |            0.346 |                     0.000 |                     0.123 |
| millet                         |            0.000 |            0.003 |                     0.000 |                     0.415 |
| potato                         |            0.000 |            0.049 |                     0.000 |                     0.089 |
| sweetpotato                    |            0.002 |            0.004 |                     0.000 |                     0.199 |
| rice                           |            0.005 |            0.464 |                     0.080 |                     0.027 |
| bread                          |            0.030 |            0.034 |                     0.000 |                     0.345 |
| livestock                      |            0.020 |            0.326 |                     0.000 |                     0.126 |
| Poultry                        |            0.054 |            0.009 |                     0.000 |                     0.100 |
| nuts                           |            0.000 |            0.265 |                     0.053 |                     0.015 |
| tea                            |            0.014 |            0.223 |                     0.000 |                     0.191 |
| medicine                       |            0.466 |            0.121 |                     0.433 |                     0.104 |
| airtime                        |            0.214 |            0.443 |                     0.152 |                     0.332 |
| childcare                      |            0.014 |            0.295 |                     0.000 |                     0.133 |
| tobacco                        |            0.000 |            0.000 |                     0.000 |                     0.089 |
| batteries                      |            0.006 |            0.225 |                     0.005 |                     0.139 |
| durables                       |            0.000 |            0.000 |                     0.000 |                     0.000 |
| othermonth                     |            0.001 |            0.017 |                     0.000 |                     0.384 |
| womensclothes                  |            0.058 |            0.090 |                     0.000 |                     0.464 |
| childrensclothes               |            0.034 |            0.035 |                     0.408 |                     0.355 |
| shoes                          |            0.011 |            0.004 |                     0.000 |                     0.180 |
| homeimprovement                |            0.002 |            0.006 |                     0.000 |                     0.337 |
| funerals                       |            0.065 |            0.176 |                     0.000 |                     0.009 |
| church                         |            0.051 |            0.010 |                     0.000 |                     0.123 |
| land_owncult                   |            0.027 |            0.470 |                     0.000 |                     0.073 |
| land_ownnocult                 |            0.330 |            0.366 |                     0.000 |                     0.122 |
| land_ownrent                   |            0.046 |            0.393 |                     0.000 |                     0.077 |
| land_rentcult                  |            0.186 |            0.411 |                     0.000 |                     0.044 |
| land_communitycult             |            0.070 |            0.205 |                     0.000 |                     0.066 |
| savings_BRAC                   |            0.003 |            0.000 |                     0.000 |                     0.000 |
| savings_NGOs                   |            0.000 |            0.000 |                     0.000 |                     0.117 |
| savings_bank                   |            0.000 |            0.000 |                     0.000 |                     0.485 |
| savings_home                   |            0.000 |            0.008 |                     0.000 |                     0.143 |
| savings_maize_val              |            0.000 |            0.000 |                     0.000 |                     0.390 |
| savings_other                  |            0.000 |            0.000 |                     0.000 |                     0.382 |
| savings_otherfood1_val         |            0.005 |            0.001 |                     0.000 |                     0.141 |
| savings_otherfood2_val         |            0.000 |            0.000 |                     0.000 |                     0.160 |
| savings_sorghum_val            |            0.000 |            0.002 |                     0.000 |                     0.060 |
| transfers_give1                |            0.000 |            0.000 |                     0.000 |                     0.234 |
| transfers_give2                |            0.000 |            0.000 |                     0.000 |                     0.432 |
| transfers_give3                |            0.000 |            0.000 |                     0.000 |                     0.291 |
| Large livestock (cows)         |            0.000 |            0.000 |                     0.000 |                     0.066 |
| Small livestock                |            0.154 |            0.036 |                     0.000 |                     0.154 |
| plough                         |            0.000 |            0.000 |                     0.000 |                     0.192 |
| shed                           |            0.000 |            0.000 |                     0.000 |                     0.022 |
| shop                           |            0.000 |            0.000 |                     0.000 |                     0.204 |
| Radio                          |            0.000 |            0.048 |                     0.000 |                     0.135 |
| tv                             |            0.002 |            0.066 |                     0.000 |                     0.135 |
| fan                            |            0.000 |            0.000 |                     0.000 |                     0.040 |
| Mobile phone                   |            0.123 |            0.189 |                     0.112 |                     0.047 |
| Chair/table                    |            0.014 |            0.019 |                     0.093 |                     0.380 |
| Bed                            |            0.027 |            0.000 |                     0.145 |                     0.009 |
| Bicycle                        |            0.430 |            0.480 |                     0.000 |                     0.466 |
| carts                          |            0.001 |            0.004 |                     0.000 |                     0.143 |
| sewing                         |            0.000 |            0.000 |                     0.000 |                     0.237 |
| Mosquito net                   |            0.057 |            0.338 |                     0.061 |                     0.215 |
| Motorcycle                     |            0.000 |            0.000 |                     0.000 |                     0.347 |
| chickens                       |            0.329 |            0.416 |                     0.000 |                     0.298 |
| ducks                          |            0.000 |            0.105 |                     0.000 |                     0.000 |
| charcoal                       |            0.001 |            0.003 |                     0.000 |                     0.293 |
| pangas                         |            0.072 |            0.005 |                     0.000 |                     0.079 |
| axes                           |            0.001 |            0.020 |                     0.000 |                     0.339 |
| ploughs_again                  |            0.000 |            0.001 |                     0.000 |                       --- |
| stoves                         |            0.043 |            0.010 |                     0.000 |                     0.128 |
| potspans                       |            0.018 |            0.000 |                     0.001 |                     0.038 |




* Food Insecurity                                                  :noexport:
  :PROPERTIES:
  :ARCHIVE_TIME: 2021-06-15 Tue 17:07
  :ARCHIVE_FILE: ~/Research/SS-TUP/Report/documents/EndlineReport.org
  :ARCHIVE_OLPATH: Results
  :ARCHIVE_CATEGORY: EndlineReport
  :END:

Does the increase in consumption expenditures we measure
for the UPG group in 2014 translate into greater food security?  In
each year, we elicit information on roughly how many days during the
most recent month the respondent has had experiences indicative of
food insecurity.  Included are (from top to bottom of Table
[[tab:food_insecurity]]) eating "less preferred" foods; eating just a "few
kinds" of foods; eating "fewer meals"; eating "limited portions";
having "no food [at all] in the house"; been able to eat "no
preferred" foods; "went to sleep hungry"; going a "whole day without
eating"; and being "worried" about not having enough food.  The
columns corresponding to 2014 and 2015 can be interpreted as the
average number of days per month people who report experiencing each.

Average levels of these variables indicate very high food insecurity;
for instance, the the average respondent is "worried" about 15 days
out of the month, and reports going for 5--7 whole days without eating
during the most recent month.  There is also a definite decrease in 
food security between 2014 and 2015.  

Since our examination of the log value of consumption expenditures (and in particular
food) turned up significant average treatment effects for the UPG (in 2014)
 and UCT groups (in 2015), one wonders whether these increases in food
expenditures help to significantly reduce food insecurity.   And
indeed, there's some very modest evidence that's consistent this; in
2014 UPG households "went to sleep hungry" nearly one day less during
the month than did the control group, and worried nearly two  days
less.  But these effects are not estimated very precisely, and the
claimed significance would not survive any standard multiple inference
correction.  

Though there are certainly multiple outcomes we're estimating here,
they are all attempts to measure some latent "food insecurity".  So 
in this spirit we calculate the first principal components of the
matrix of food insecurity measures, one for each household-year.
But though the signs of the effects associated with this summary
measure are consistent with the UPG program having a small effect on
food security, none are statistically significant.

*************** Notes on Recreating food insecurity table                      
The table looks at whether respondents experienced going a whole
day without eating, limiting portions, etc. in a typical week.  
The z-score is calculated by taking the sum of binary variables 'worried', 
'portions', 'fewmeals', 'nofood', 'hungry', 'wholeday' for each
individual, separately at baseline, midline, and endline. Call that
'fs_sum_m' for an individual's sum at midline. Then their z-score at
midline  would be (fs_sum_m - fs_sum_m.mean())/fs_sum_m.std(). CTL
mean lists the mean and standard deviation of the control group in 2015.

Differences with previous code: 
-correction for what should be considered "weekly" (this code
includes "Everyday", "3-6 times a week", and "1-2 times a week" as
being considered weekly, whereas the previous code left out "1-2 times a week")
-This code adds 'worried' to the table. It uses 'worried' to
calculate the z-score, which  was also used to calculate
the z-score in the previous table.

Next steps: 
-change how the z-score is being calculated because .sum(axis=1)
equates NaN to 0, so some respondents who had NaN for every measure
will still have a z-score, hence the higher N for 'z-score' than for
the other variables
-divide the standard deviation of CTL mean by the sqrt of the number of control
observations to make it comparable with the other point estimates and
standard errors in the table?
-add another column with f statistic for joint hypothesis that all coefficients are equal to 0
*************** END

** food insecurity table                                             :ignore:

#+name: food_insecurity
#+begin_src python :noweb no-export :exports none  :results output table raw :var FE=1 :colnames no :tangle /tmp/food_security.py
import pandas as pd
import numpy as np
from scipy import stats
from cfe.df_utils import df_to_orgtbl
import ss_standard
from cfe.estimation import svd_missing

fs = ss_standard.basic_data(filter = r'^fs_')

# replace words with their corresponding code number from the survey
fs.replace({"Everyday": 28,
            "everyday": 28,
            "3-6 times a week": 4.5*4,
            "1-2 times a week": 1.5*4,
            "Less than once a week": 4,
            "less than once a week": 4,
            "Never": 0,
            "never": 0}, inplace=True)


outcomes = {'worried':'Worried', 
            'notpreferred':'No preferred',
            'fewkinds':'Few kinds',
            'preferrednot':'Ate unpreferred',
            'portions':'Limited portions',
            'fewmeals':'Fewer Meals', 
            'nofood':'No food in house', 
            'hungry':'Went hungry', 
            'wholeday':'Whole day without eating'}

fs = fs.rename(columns=outcomes)

# used to calculate the z-score
index_vars = [outcomes[k] for k in list(outcomes.keys())]
outcomes = index_vars #+ [r'\(z\)-score','Score']

#fs[r'\(z\)-score'] = np.sqrt(((fs[index_vars] - fs[index_vars].mean())**2/fs[index_vars].var()).mean(axis=1))

u,s,vt = svd_missing(fs[index_vars].T,max_rank=1)

score = pd.Series(vt.squeeze(),index=fs.index)
score = (score - score.mean())/score.std()

fs['Score'] = score

df = ss_standard.ancova_form(fs,outcomes)

Results = {}

Results['Averages'] = ss_standard.results(df,outcomes,nonzero=False,logs=False,return_stats=True)

print(ss_standard.table_stacked_by_class(Results,elide=True))
print(ss_standard.table_stacked_by_class(Results,elide=False))
#+end_src

#+latex: \begin{landscape}
#+name: tab:food_insecurity
#+caption: Effects of programs on different measures of food insecurity.  Asterisks: *** $p<0.01$, ** $p<0.05$, * $p<0.10$.
| Outcome                  | $\beta_{UPG}^{2014}$ | $\beta_{UPG}^{2015}$ | $\beta_{UCT}^{2014}$ | $\beta_{UCT}^{2015}$ | 2014             | 2015             | Baseline value  | Baseline missing | $\beta_{UPG}^{2014} - \beta_{UCT}^{2015}$ | $\beta_{UPG}^{2015} - \beta_{UCT}^{2015}$ |
|--------------------------+----------------------+----------------------+----------------------+----------------------+------------------+------------------+-----------------+------------------+-------------------------------------------+-------------------------------------------|
| Ate less preferred       | \(  0.41\)           | \( -0.32\)           | \( -1.61\)^{*}       | \(  1.16\)           | \( 10.40\)^{***} | \( 12.01\)^{***} | \( -0.18\)      | \( -2.21\)^{***} | \( -0.75\)                                | \( -1.49\)                                |
| $N=1297$                 | (\(  0.73\))         | (\(  0.67\))         | (\(  0.82\))         | (\(  0.92\))         | (\(  0.70\))     | (\(  0.66\))     | (\(  0.18\))    | (\(  0.81\))     | (\(  1.18\))                              | (\(  1.14\))                              |
| Few kinds                | \(  1.02\)           | \(  0.71\)           | \( -0.39\)           | \(  1.40\)           | \( 10.78\)^{***} | \( 13.22\)^{***} | \( -0.46\)^{**} | \( -1.96\)^{**}  | \( -0.38\)                                | \( -0.69\)                                |
| $N=1297$                 | (\(  0.73\))         | (\(  0.71\))         | (\(  0.90\))         | (\(  0.94\))         | (\(  0.75\))     | (\(  0.72\))     | (\(  0.21\))    | (\(  0.88\))     | (\(  1.19\))                              | (\(  1.18\))                              |
| Fewer Meals              | \(  0.16\)           | \(  0.91\)           | \( -0.27\)           | \( -0.04\)           | \(  8.94\)^{***} | \( 10.68\)^{***} | \( -0.31\)^{*}  | \( -1.63\)^{**}  | \(  0.20\)                                | \(  0.94\)                                |
| $N=1297$                 | (\(  0.65\))         | (\(  0.65\))         | (\(  0.76\))         | (\(  0.81\))         | (\(  0.60\))     | (\(  0.57\))     | (\(  0.18\))    | (\(  0.72\))     | (\(  1.04\))                              | (\(  1.04\))                              |
| Limited portions         | \(  0.33\)           | \( -0.35\)           | \( -0.21\)           | \( -0.41\)           | \(  8.02\)^{***} | \( 10.49\)^{***} | \(  0.14\)      | \( -0.81\)       | \(  0.75\)                                | \(  0.06\)                                |
| $N=1292$                 | (\(  0.68\))         | (\(  0.65\))         | (\(  0.77\))         | (\(  0.85\))         | (\(  0.63\))     | (\(  0.61\))     | (\(  0.16\))    | (\(  0.73\))     | (\(  1.09\))                              | (\(  1.07\))                              |
| No food in house         | \( -0.54\)           | \( -0.54\)           | \( -0.21\)           | \( -0.03\)           | \(  5.63\)^{***} | \(  8.86\)^{***} | \( -0.10\)      | \(  0.06\)       | \( -0.52\)                                | \( -0.51\)                                |
| $N=1293$                 | (\(  0.48\))         | (\(  0.65\))         | (\(  0.58\))         | (\(  0.85\))         | (\(  0.54\))     | (\(  0.61\))     | (\(  0.16\))    | (\(  0.74\))     | (\(  0.98\))                              | (\(  1.07\))                              |
| No preferred             | \(  0.32\)           | \( -0.35\)           | \( -0.65\)           | \(  1.72\)^{*}       | \( 10.97\)^{***} | \( 13.81\)^{***} | \( -0.40\)^{*}  | \( -2.16\)^{**}  | \( -1.40\)                                | \( -2.07\)                                |
| $N=1297$                 | (\(  0.75\))         | (\(  0.75\))         | (\(  0.89\))         | (\(  1.02\))         | (\(  0.74\))     | (\(  0.74\))     | (\(  0.23\))    | (\(  0.90\))     | (\(  1.26\))                              | (\(  1.26\))                              |
| Went to sleep hungry     | \( -0.94\)^{**}      | \(  0.14\)           | \( -0.46\)           | \(  0.77\)           | \(  5.45\)^{***} | \(  7.86\)^{***} | \( -0.18\)      | \( -1.21\)       | \( -1.71\)^{*}                            | \( -0.63\)                                |
| $N=1297$                 | (\(  0.43\))         | (\(  0.67\))         | (\(  0.58\))         | (\(  0.91\))         | (\(  0.58\))     | (\(  0.67\))     | (\(  0.16\))    | (\(  0.76\))     | (\(  1.01\))                              | (\(  1.12\))                              |
| Whole day without eating | \( -0.54\)           | \(  0.23\)           | \( -0.13\)           | \(  0.92\)           | \(  4.97\)^{***} | \(  7.55\)^{***} | \( -0.08\)      | \( -0.81\)       | \( -1.46\)                                | \( -0.69\)                                |
| $N=1282$                 | (\(  0.46\))         | (\(  0.68\))         | (\(  0.63\))         | (\(  0.96\))         | (\(  0.60\))     | (\(  0.67\))     | (\(  0.15\))    | (\(  0.72\))     | (\(  1.06\))                              | (\(  1.18\))                              |
| Worried                  | \( -1.76\)^{*}       | \( -1.05\)           | \(  0.54\)           | \(  0.40\)           | \( 14.94\)^{***} | \( 15.57\)^{***} | \( -0.33\)      | \( -2.26\)^{**}  | \( -2.16\)                                | \( -1.45\)                                |
| $N=1291$                 | (\(  1.00\))         | (\(  0.83\))         | (\(  1.29\))         | (\(  1.09\))         | (\(  0.89\))     | (\(  0.77\))     | (\(  0.26\))    | (\(  1.02\))     | (\(  1.48\))                              | (\(  1.37\))                              |
|--------------------------+----------------------+----------------------+----------------------+----------------------+------------------+------------------+-----------------+------------------+-------------------------------------------+-------------------------------------------|
| Score (1st PC)           | \(  0.03\)           | \(  0.03\)           | \(  0.06\)           | \( -0.10\)           | \(  0.08\)       | \( -0.29\)       | \( -0.40\)^{*}  | \( -0.14\)       | \(  0.13\)                                | \(  0.14\)                                |
| $N=1299$                 | (\(  0.08\))         | (\(  0.09\))         | (\(  0.10\))         | (\(  0.12\))         | (\(  0.19\))     | (\(  0.20\))     | (\(  0.22\))    | (\(  0.20\))     | (\(  0.14\))                              | (\(  0.15\))                              |
#+latex: \end{landscape}



